<!doctype html><html lang=en><head><meta charset=utf-8><title>How we designed retries in Linkerd 2.2 | Linkerd</title>
<link rel="shortcut icon" href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Retries are a fundamental mechanism for handling partial or transient failures in a distributed system. But retries can also be dangerous—if done poorly, they can rapidly escalate a small error into a system-wide outage. In this post, we describe how we designed retries in Linkerd 2.2 in a way that allows Linkerd to automatically improve system reliability while minimizing risk.
Marking a route as retryable
In Linkerd 2.2 we introduced retries, or the ability for Linkerd to automatically retry failed requests. This gives Linkerd the ability to automatically handle partial or transient failures in a service, without the application having to be aware: if a request fails, Linkerd can just try it again! Combined with Linkerd&rsquo;s request-level load balancing, this also allows Linkerd to handle failures of individual pods. In Linkerd, you specify retries as part of a service profile (introduced in a previous blog post). Marking a route as retryable is as simple as adding `isRetryable: true` to the corresponding service profile entry:"><meta property="og:url" content="https://travisbeckham.github.io/2019/02/22/how-we-designed-retries-in-linkerd-2.2/"><meta property="og:site_name" content="Linkerd"><meta property="og:title" content="How we designed retries in Linkerd 2.2"><meta property="og:description" content="Retries are a fundamental mechanism for handling partial or transient failures in a distributed system. But retries can also be dangerous—if done poorly, they can rapidly escalate a small error into a system-wide outage. In this post, we describe how we designed retries in Linkerd 2.2 in a way that allows Linkerd to automatically improve system reliability while minimizing risk.
Marking a route as retryable In Linkerd 2.2 we introduced retries, or the ability for Linkerd to automatically retry failed requests. This gives Linkerd the ability to automatically handle partial or transient failures in a service, without the application having to be aware: if a request fails, Linkerd can just try it again! Combined with Linkerd’s request-level load balancing, this also allows Linkerd to handle failures of individual pods. In Linkerd, you specify retries as part of a service profile (introduced in a previous blog post). Marking a route as retryable is as simple as adding `isRetryable: true` to the corresponding service profile entry:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2019-02-22T00:00:00+00:00"><meta property="article:modified_time" content="2019-02-22T00:00:00+00:00"><meta property="og:image" content="https://travisbeckham.github.io/2019/02/22/how-we-designed-retries-in-linkerd-2.2/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://travisbeckham.github.io/2019/02/22/how-we-designed-retries-in-linkerd-2.2/cover.png"><meta name=twitter:title content="How we designed retries in Linkerd 2.2"><meta name=twitter:description content="Retries are a fundamental mechanism for handling partial or transient failures in a distributed system. But retries can also be dangerous—if done poorly, they can rapidly escalate a small error into a system-wide outage. In this post, we describe how we designed retries in Linkerd 2.2 in a way that allows Linkerd to automatically improve system reliability while minimizing risk.
Marking a route as retryable In Linkerd 2.2 we introduced retries, or the ability for Linkerd to automatically retry failed requests. This gives Linkerd the ability to automatically handle partial or transient failures in a service, without the application having to be aware: if a request fails, Linkerd can just try it again! Combined with Linkerd’s request-level load balancing, this also allows Linkerd to handle failures of individual pods. In Linkerd, you specify retries as part of a service profile (introduced in a previous blog post). Marking a route as retryable is as simple as adding `isRetryable: true` to the corresponding service profile entry:"><meta name=twitter:site content="@Linkerd"><link rel=canonical href=https://travisbeckham.github.io/2019/02/22/how-we-designed-retries-in-linkerd-2.2/><link rel=stylesheet href=/css/main.css><script src=/js/main.js></script><script async defer src=https://buttons.github.io/buttons.js></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","author":"Alex Leong","datePublished":"2019-02-22T00:00:00Z","dateModified":"2019-02-22T00:00:00Z","headline":"How we designed retries in Linkerd 2.2","image":"https://travisbeckham.github.io/2019/02/22/how-we-designed-retries-in-linkerd-2.2/cover.png","publisher":{"@type":"Organization","name":"linkerd.io","logo":{"@type":"ImageObject","url":"https://travisbeckham.github.io/logos/linkerd.png","width":472,"height":100}}}</script></head><body><header class=main-header><div class=main-header__container><div class=main-header__logo><a href=/><img src=/logos/linkerd.png alt=Linkerd></a></div><input class=main-header__toggle-checkbox type=checkbox id=main-header-toggle>
<label class=main-header__toggle for=main-header-toggle><span class=main-header__toggle-icon><span class=main-header__toggle-icon--open><svg class="icon icon--primary" viewBox="0 0 24 24"><path d="M3 6H21V8H3V6m0 5H21v2H3V11m0 5H21v2H3V16z"/></svg>
</span><span class=main-header__toggle-icon--close><svg class="icon icon--primary" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg></span></span></label><div class=main-header__nav><nav class=main-nav><ul class=main-nav__menu><li><a href=/docs>Docs</a></li><li><a href=#>Community<svg class="icon icon--primary" viewBox="0 0 24 24"><path d="M7.41 8.58 12 13.17l4.59-4.59L18 10l-6 6-6-6L7.41 8.58z"/></svg></a><ul><li><a href=/community/get-involved/>Get Involved</a></li><li><a href=/community/adopters/>Adopters</a></li><li><a href=/community/ambassadors/>Linkerd Ambassadors</a></li><li><a href=https://linkerd.buoyant.io target=_blank rel=noopener>Buoyant's Linkerd Forum</a></li></ul></li><li class=main-nav__menu--selected><a href=/blog/>Blog</a></li><li><a href=/faq/>FAQ</a></li><li><a href=/enterprise/>Enterprise</a></li></ul><div class=main-nav__search><form action=/search method=get><div class="search-input search-input--sm"><svg class="icon" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5.0 0116 9.5c0 1.61-.59 3.09-1.56 4.23L14.71 14H15.5l5 5L19 20.5l-5-5V14.71L13.73 14.44C12.59 15.41 11.11 16 9.5 16A6.5 6.5.0 013 9.5 6.5 6.5.0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
<input type=text name=q placeholder=Search></div></form></div><div class=main-nav__github><a class=github-button href=https://github.com/linkerd/linkerd2 data-icon=octicon-star data-size=large data-show-count=true aria-label="Star linkerd/linkerd2 on GitHub">Star</a></div><div class=main-nav__forum><a href=https://linkerd.buoyant.io class="button button--primary button--sm" target=_blank rel=noopener>Join Forum</a></div></nav></div></div></header><div class=main-announcement><strong>Oct 23, 2024</strong> New blog post: Towards a Sustainable Service Mesh.
<a href=/2024/10/23/making-linkerd-sustainable/ class=main-announcement__link>Read the post</a></div><main class=main-content><div class="blog blog--single"><div class="blog__container container"><div class=blog-post><div class=blog-post__header><h1>How we designed retries in Linkerd 2.2</h1><div class=blog-post-meta><div class=blog-post-meta__media><img src=/authors/alex-leong_hu10086457063558840925.jpg alt="Alex Leong" class="img img--48 img--round"></div><div class=blog-post-meta__body><div class=blog-post-meta__name>Alex Leong</div><div class=blog-post-meta__date>Feb 22, 2019 • 9 min read</div></div></div></div><div class=blog-post__cover></div><div class="blog-post__content prose"><p>Retries are a fundamental mechanism for handling partial or transient failures in a distributed system. But retries can also be dangerous—if done poorly, they can rapidly escalate a small error into a system-wide outage. In this post, we describe how we designed retries in Linkerd 2.2 in a way that allows Linkerd to automatically improve system reliability while minimizing risk.</p><h2 id=marking-a-route-as-retryable>Marking a route as retryable</h2><p>In Linkerd 2.2 we introduced <em>retries</em>, or the ability for Linkerd to automatically retry failed requests. This gives Linkerd the ability to automatically handle partial or transient failures in a service, without the application having to be aware: if a request fails, Linkerd can just try it again! Combined with Linkerd&rsquo;s <a href=https://linkerd.io/2/features/load-balancing/ target=_blank rel=noopener>request-level load balancing</a>, this also allows Linkerd to handle failures of individual pods. In Linkerd, you specify retries as part of a <a href=https://linkerd.io/2/features/service-profiles/ target=_blank rel=noopener>service profile</a> (introduced in a <a href=https://blog.linkerd.io/2018/12/07/service-profiles-for-per-route-metrics/ target=_blank rel=noopener>previous blog post</a>). Marking a route as retryable is as simple as adding `isRetryable: true` to the corresponding service profile entry:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#7ee787>name</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>HEAD /authors/{id}.json</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>    </span><span style=color:#7ee787>condition</span>:<span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>      </span><span style=color:#7ee787>method</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>HEAD</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>      </span><span style=color:#7ee787>pathRegex</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>/authors/[^/]*\\.json</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>    </span><span style=color:#7ee787>isRetryable</span>:<span style=color:#6e7681> </span><span style=color:#79c0ff>true</span><span style=color:#6e7681>
</span></span></span></code></pre></div><p>Of course, before you add retry behavior to a route, you should make sure that the route is <em>idempotent</em>—in other words, that multiple calls to the same route with the same parameters will have no ill effects. This is important because retries (by definition!) may cause multiple copies of the same request to be sent to a service. If the request does something non-idempotent, e.g. subtracting a dollar from your bank account, you probably don&rsquo;t want it to be automatically retried. Once enabled, retries have two important parameters: a <em>budget</em> and a <em>timeout</em>. Let&rsquo;s take both of these in turn.</p><h2 id=using-retry-budgets>Using retry budgets</h2><p>Once you&rsquo;ve marked a route as retryable, Linkerd allows you to configure a <em>retry budget</em> for a service. Linkerd ships with reasonable default values, but if you want to customize the budget, you can set it in the service profile:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#7ee787>retryBudget</span>:<span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># The retryRatio is the maximum ratio of retries requests to original</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># requests.  A retryRatio of 0.2 means that retries may add at most an</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># additional 20% to the request load.</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#7ee787>retryRatio</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>0.2</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># This is an allowance of retries per second in addition to those allowed</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># by the retryRatio.  This allows retries to be performed, when the request</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># rate is very low.</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#7ee787>minRetriesPerSecond</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>10</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># This duration indicates for how long requests should be considered for the</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># purposes of calculating the retryRatio.  A higher value considers a larger</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#8b949e;font-style:italic># window and therefore allows burstier retries.</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>  </span><span style=color:#7ee787>ttl</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>10s</span><span style=color:#6e7681>
</span></span></span></code></pre></div><p>Linkerd&rsquo;s use of retry budgets is a better alternative to the normal practice of configuring retries with the <em>max retries</em>. Let&rsquo;s take a moment to understand why.</p><h3 id=why-budgets-and-not-max-retries>Why budgets and not max retries?</h3><p>First, some background. The most common way of configuring retries is to specify a maximum number of retry attempts to perform before giving up. This is a familiar idea to anyone who&rsquo;s used a web browser: you try to load a webpage, and if it doesn&rsquo;t load, you try again. If it still doesn&rsquo;t load, you try a third time. Finally you give up. Unfortunately, there are at least two problems with configuring retries this way:</p><p><strong>Choosing the maximum number of retry attempts is a guessing game.</strong> You need to pick a number that’s high enough to make a difference when things are <em>somewhat</em> failing, but not so high that it generates extra load on the system when it&rsquo;s <em>really</em> failing. In practice, you usually pick a maximum retry attempts number out of a hat (e.g. 3) and hope for the best.</p><p><strong>Systems configured this way are vulnerable to retry storms.</strong> A retry storm begins when one service starts to experience a larger than normal failure rate. This causes its clients to retry those failed requests. The extra load from the retries causes the service to slow down further and fail more requests, triggering more retries. If each client is configured to retry up to 3 times, this can quadruple the number of requests being sent! To make matters even worse, if any of the clients’ clients are configured with retries, the number of retries compounds multiplicatively and can turn a small number of errors into a self-inflicted denial of service attack.</p><p>To avoid these problems, Linkerd uses retry budgets. Rather than specifying a fixed maximum number of retry attempts per request, Linkerd keeps track of the ratio between regular requests and retries and keeps this number below a limit. For example, you may specify that you want retries to add at most 20% more requests. Linkerd will then retry as much as it can while maintaining that ratio.</p><p>Thus, using retry budgets makes <em>explicit</em> the trade-off between improving success rate and additional load. Your retry budget is exactly how much extra load your system is willing to accept from retries.</p><p>(And finally, a retry budget in Linkerd also includes an allowance for a minimum rate of retries which will always be allowed, independent of the ratio. This allows Linkerd to retry in very low traffic systems.)</p><h2 id=setting-per-request-timeouts>Setting per-request timeouts</h2><p>In addition to a budgets, retries are parameterized by a per-request <em>timeout</em>. A timeout ensures that a request that always fails will eventually return a response, even if that response is a failure. Once the timeout is reached, Linkerd will cancel the request and return a HTTP 504 response. Similar to retry budgets, retry timeouts have a sane default that can be overridden in the service profile:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>- <span style=color:#7ee787>name</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>HEAD /authors/{id}.json</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>    </span><span style=color:#7ee787>condition</span>:<span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>      </span><span style=color:#7ee787>method</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>HEAD</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>      </span><span style=color:#7ee787>pathRegex</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>/authors/[^/]*\\.json</span><span style=color:#6e7681>
</span></span></span><span style=display:flex><span><span style=color:#6e7681>    </span><span style=color:#7ee787>timeout</span>:<span style=color:#6e7681> </span><span style=color:#a5d6ff>50ms</span><span style=color:#6e7681>
</span></span></span></code></pre></div><h2 id=who-owns-retry-behavior-the-client-or-the-server>Who owns retry behavior? The client or the server?</h2><p>You may have noticed something interesting in the configuration snippets above. In &ldquo;traditional&rdquo; retrying systems  (e.g. a web browser), retry behavior is configured on the client—after all, this is where the retries actually take place. But in the service profiles above, we&rsquo;re specifying retry policy on the <em>server</em> side instead.</p><p>Being able to attach policy to the server side, but have it be obeyed by the client side, is one of the fundamental benefits of Linkerd&rsquo;s service profile approach. Retry configuration logically <em>belongs</em> at the level of the service (&ldquo;this is how you should talk to me&rdquo;). Since Linkerd controls both client and server behavior, we can do this the right way: a service profile allows a service to publish exactly &ldquo;here is how I want you to talk to me&rdquo;, and all traffic going through Linkerd, regardless of source, will respect that behavior. Pretty cool!</p><h2 id=putting-it-all-together>Putting it all together</h2><p>We&rsquo;ve shown how you can configure Linkerd&rsquo;s retry behavior by combining timeouts, budgets, and retryability. Now let&rsquo;s put it all together with a brief demo. If you have a terminal window and a Kubernetes cluster, you can follow along at home. We&rsquo;ll start by installing Linkerd and our sample books application:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>linkerd install | kubectl apply -f - <span style=color:#ff7b72;font-weight:700>&amp;&amp;</span> <span style=color:#79c0ff>\
</span></span></span><span style=display:flex><span><span style=color:#79c0ff></span>  curl --proto <span style=color:#a5d6ff>&#39;=https&#39;</span> --tlsv1.2 -sSfL https://run.linkerd.io/booksapp.yml | linkerd inject - | kubectl apply -f - <span style=color:#ff7b72;font-weight:700>&amp;&amp;</span> <span style=color:#79c0ff>\
</span></span></span><span style=display:flex><span><span style=color:#79c0ff></span>  linkerd check
</span></span></code></pre></div><p>One thing that we can notice about this application is that the success rate of requests from the books service to the authors service is very poor:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ linkerd routes deploy/books --to svc/authors
</span></span><span style=display:flex><span>ROUTE       SERVICE   SUCCESS      RPS   LATENCY_P50   LATENCY_P95   LATENCY_P99
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>[</span>DEFAULT<span style=color:#ff7b72;font-weight:700>]</span>   authors    54.24%   3.9rps           5ms          14ms          19ms
</span></span></code></pre></div><p>To get a better picture of what’s going on here, let’s add a service profile for the authors service, generated from a Swagger definition:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ curl --proto <span style=color:#a5d6ff>&#39;=https&#39;</span> --tlsv1.2 -sSfL https://run.linkerd.io/booksapp/authors.swagger | linkerd profile --open-api - authors | kubectl apply -f  -
</span></span><span style=display:flex><span>$ linkerd routes deploy/books --to svc/authors
</span></span><span style=display:flex><span>ROUTE                       SERVICE   SUCCESS      RPS   LATENCY_P50   LATENCY_P95   LATENCY_P99
</span></span><span style=display:flex><span>DELETE /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json   authors     0.00%   0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>GET /authors.json           authors     0.00%   0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>GET /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json      authors     0.00%   0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>HEAD /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json     authors    50.85%   3.9rps           5ms          10ms          17ms
</span></span><span style=display:flex><span>POST /authors.json          authors     0.00%   0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>[</span>DEFAULT<span style=color:#ff7b72;font-weight:700>]</span>                   authors     0.00%   0.0rps           0ms           0ms           0ms
</span></span></code></pre></div><p>One thing that’s clear is that all requests from books to authors are to the <code>HEAD /authors/{id}.json</code> route and those requests are failing about 50% of the time. To correct this, let’s edit the authors service profile and make those requests retryable:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit sp/authors.default.svc.cluster.local
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>[</span>...<span style=color:#ff7b72;font-weight:700>]</span>
</span></span><span style=display:flex><span>  - condition:
</span></span><span style=display:flex><span>      method: HEAD
</span></span><span style=display:flex><span>      pathRegex: /authors/<span style=color:#ff7b72;font-weight:700>[</span>^/<span style=color:#ff7b72;font-weight:700>]</span>*<span style=color:#79c0ff>\\</span>.json
</span></span><span style=display:flex><span>    name: HEAD /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json
</span></span><span style=display:flex><span>    isRetryable: true <span style=color:#8b949e;font-style:italic>### ADD THIS LINE ###</span>
</span></span></code></pre></div><p>After editing the service profile, we see a nearly immediate improvement in success rate:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ linkerd routes deploy/books --to svc/authors -o wide
</span></span><span style=display:flex><span>ROUTE                       SERVICE   EFFECTIVE_SUCCESS   EFFECTIVE_RPS   ACTUAL_SUCCESS   ACTUAL_RPS   LATENCY_P50   LATENCY_P95   LATENCY_P99
</span></span><span style=display:flex><span>DELETE /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json   authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>GET /authors.json           authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>GET /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json      authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>HEAD /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json     authors             100.00%          2.8rps           58.45%       4.7rps           7ms          25ms          37ms
</span></span><span style=display:flex><span>POST /authors.json          authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>[</span>DEFAULT<span style=color:#ff7b72;font-weight:700>]</span>                   authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span></code></pre></div><p>Success rate looks great but the p95 and p99 latencies have increased. This is to be expected because doing retries takes time. However, we can limit this by setting a <a href=https://linkerd.io/2/features/retries-and-timeouts/#timeouts target=_blank rel=noopener>timeouts</a> another new feature of Linkerd 2.x - at the maximum duration that we’re willing to wait. For the purposes of this demo, I’ll set a timeout of 25ms. Your results will vary depending on the characteristics of your system.</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl edit sp/authors.default.svc.cluster.local
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>[</span>...<span style=color:#ff7b72;font-weight:700>]</span>
</span></span><span style=display:flex><span>  - condition:
</span></span><span style=display:flex><span>      method: HEAD
</span></span><span style=display:flex><span>      pathRegex: /authors/<span style=color:#ff7b72;font-weight:700>[</span>^/<span style=color:#ff7b72;font-weight:700>]</span>*<span style=color:#79c0ff>\\</span>.json
</span></span><span style=display:flex><span>    isRetryable: true
</span></span><span style=display:flex><span>    name: HEAD /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json
</span></span><span style=display:flex><span>    timeout: 25ms <span style=color:#8b949e;font-style:italic>### ADD THIS LINE ###</span>
</span></span></code></pre></div><p>We now see that success rate has come down slightly because some requests are timing out, but that the tail latency has been greatly reduced:</p><div class=highlight><pre tabindex=0 style=color:#e6edf3;background-color:#0d1117;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ linkerd routes deploy/books --to svc/authors -o wide
</span></span><span style=display:flex><span>ROUTE                       SERVICE   EFFECTIVE_SUCCESS   EFFECTIVE_RPS   ACTUAL_SUCCESS   ACTUAL_RPS   LATENCY_P50   LATENCY_P95   LATENCY_P99
</span></span><span style=display:flex><span>DELETE /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json   authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>GET /authors.json           authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>GET /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json      authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span>HEAD /authors/<span style=color:#ff7b72;font-weight:700>{</span>id<span style=color:#ff7b72;font-weight:700>}</span>.json     authors              97.73%          2.9rps           49.71%       5.8rps           9ms          25ms          29ms
</span></span><span style=display:flex><span>POST /authors.json          authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span><span style=display:flex><span><span style=color:#ff7b72;font-weight:700>[</span>DEFAULT<span style=color:#ff7b72;font-weight:700>]</span>                   authors               0.00%          0.0rps            0.00%       0.0rps           0ms           0ms           0ms
</span></span></code></pre></div><p>Note that the p99 latency appears to be greater than our 25ms timeout due to histogram bucketing artifacts.</p><h2 id=conclusion>Conclusion</h2><p>In this post, we described how Linkerd can automatically retry requests in a way that minimizes risk to the system. We described why retry behavior is specified at the server rather than client, level, and we walked you through a quick demonstration of how to deploy the retries and timeout features on a service in a demo app.</p><p>Retries are a big step forward in Linkerd&rsquo;s reliability roadmap. The intersection of service profiles, retries, and diagnostics is a particularly exciting area for Linkerd, and you can expect <a href=https://github.com/linkerd/linkerd2/issues/2016 target=_blank rel=noopener>more cool features</a> in future releases—so stay tuned!</p><p>Like this post? Linkerd is a community project and is hosted by the <a href=https://cncf.io/ target=_blank rel=noopener>Cloud Native Computing Foundation</a>. If you have feature requests, questions, or comments, we’d love to have you join our rapidly-growing community! Linkerd is <a href=https://github.com/linkerd/linkerd2 target=_blank rel=noopener>hosted on GitHub</a>, and we have a thriving community on <a href=https://slack.linkerd.io/ target=_blank rel=noopener>Slack</a>, <a href=https://twitter.com/linkerd target=_blank rel=noopener>Twitter</a>, and the <a href=https://lists.cncf.io/g/cncf-linkerd-users target=_blank rel=noopener>mailing lists</a>. Come and join the fun!</p></div></div><div class=blog-post-related><h2>Suggested Blog Posts</h2><div class=blog-post-related__pages><div class="card card--horz card--center"><div class=card__media><img src=/2019/02/12/announcing-linkerd-2.2/cover.png alt=Cover class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2019/02/12/announcing-linkerd-2.2/>Announcing Linkerd 2.2</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Feb 12, 2019 • 5 min read</div></div></div></div></div><div class="card card--horz card--center"><div class=card__media><img src=/2018/12/08/service-profiles-for-per-route-metrics/cover_hu15148671010256722806.jpg alt="Cover hu15148671010256722806" class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2018/12/08/service-profiles-for-per-route-metrics/>Service Profiles for Per-Route Metrics</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Dec 8, 2018 • 7 min read</div></div></div></div></div><div class="card card--horz card--center"><div class=card__media><img src=/2018/12/06/announcing-linkerd-2.1/cover_hu5414770961456091062.jpg alt="Cover hu5414770961456091062" class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2018/12/06/announcing-linkerd-2.1/>Announcing Linkerd 2.1</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Dec 6, 2018 • 3 min read</div></div></div></div></div><div class="card card--horz card--center"><div class=card__media><img src=/2018/09/18/announcing-linkerd-2.0/cover_hu2635522132769039048.jpg alt="Cover hu2635522132769039048" class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2018/09/18/announcing-linkerd-2.0/>Announcing Linkerd 2.0</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Sep 18, 2018 • 3 min read</div></div></div></div></div></div></div></div></div></main><footer class=main-footer><div class=main-footer__top><div class="main-footer__container container"><div class=main-footer__info><p><img src=/logos/linkerd.png alt=Linkerd></p><p>Linkerd was originally created by <a href=https://buoyant.io/ target=_blank rel=noopener>Buoyant</a></p><p>View <a href=https://github.com/linkerd/linkerd/wiki/Linkerd-code-of-conduct target=_blank rel=noopener>Code of Conduct</a></p></div><div class=main-footer__links><div class=main-footer__community><h4>Community</h4><ul><li><a href=https://github.com/linkerd/linkerd2 target=_blank rel=noopener>GitHub</a></li><li><a href=https://slack.linkerd.io target=_blank rel=noopener>Slack</a></li><li><a href=https://linkerd.buoyant.io target=_blank rel=noopener>Linkerd Forum</a></li></ul></div><div class=main-footer__follow><h4>Follow</h4><ul><li><a href=https://www.linkedin.com/company/linkerd/ target=_blank rel=noopener>Linkedin</a></li><li><a href=https://www.youtube.com/buoyantio target=_blank rel=noopener>YouTube</a></li><li><a href=https://twitter.com/linkerd target=_blank rel=noopener>Twitter</a></li></ul></div></div></div></div><div class=main-footer__bottom><div class="main-footer__container container"><p><a href=https://github.com/linkerd/website/tree/main/linkerd.io/content target=_blank rel=noopener>Edit This Site</a></p><p>Copyright © 2024 Linkerd Authors. All rights reserved.</p></div></div></footer></body></html>