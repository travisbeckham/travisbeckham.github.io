<!doctype html><html lang=en><head><meta charset=utf-8><title>gRPC Load Balancing on Kubernetes without Tears | Linkerd</title>
<link rel="shortcut icon" href=/favicon.png><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Many new gRPC users are surprised to find that Kubernetes&rsquo;s default load balancing often doesn&rsquo;t work out of the box with gRPC. For example, here&rsquo;s what happens when you take a simple gRPC Node.js microservices app and deploy it on Kubernetes:

  
    
    Pods
  While the voting service displayed here has several pods, it&rsquo;s clear from Kubernetes&rsquo;s CPU graphs that only one of the pods is actually doing any work—because only one of the pods is receiving any traffic. Why?"><meta property="og:url" content="https://travisbeckham.github.io/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/"><meta property="og:site_name" content="Linkerd"><meta property="og:title" content="gRPC Load Balancing on Kubernetes without Tears"><meta property="og:description" content="Many new gRPC users are surprised to find that Kubernetes’s default load balancing often doesn’t work out of the box with gRPC. For example, here’s what happens when you take a simple gRPC Node.js microservices app and deploy it on Kubernetes:
Pods While the voting service displayed here has several pods, it’s clear from Kubernetes’s CPU graphs that only one of the pods is actually doing any work—because only one of the pods is receiving any traffic. Why?"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2018-11-14T00:00:00+00:00"><meta property="article:modified_time" content="2018-11-14T00:00:00+00:00"><meta property="og:image" content="https://travisbeckham.github.io/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://travisbeckham.github.io/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/cover.png"><meta name=twitter:title content="gRPC Load Balancing on Kubernetes without Tears"><meta name=twitter:description content="Many new gRPC users are surprised to find that Kubernetes’s default load balancing often doesn’t work out of the box with gRPC. For example, here’s what happens when you take a simple gRPC Node.js microservices app and deploy it on Kubernetes:
Pods While the voting service displayed here has several pods, it’s clear from Kubernetes’s CPU graphs that only one of the pods is actually doing any work—because only one of the pods is receiving any traffic. Why?"><meta name=twitter:site content="@Linkerd"><link rel=canonical href=https://travisbeckham.github.io/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/><link rel=stylesheet href=/css/main.css><script src=/js/main.js></script><script async defer src=https://buttons.github.io/buttons.js></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","author":"William Morgan","datePublished":"2018-11-14T00:00:00Z","dateModified":"2018-11-14T00:00:00Z","headline":"gRPC Load Balancing on Kubernetes without Tears","image":"https://travisbeckham.github.io/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/cover.png","publisher":{"@type":"Organization","name":"linkerd.io","logo":{"@type":"ImageObject","url":"https://travisbeckham.github.io/logos/linkerd.png","width":472,"height":100}}}</script></head><body><header class=main-header><div class=main-header__container><div class=main-header__logo><a href=/><img src=/logos/linkerd.png alt=Linkerd></a></div><input class=main-header__toggle-checkbox type=checkbox id=main-header-toggle>
<label class=main-header__toggle for=main-header-toggle><span class=main-header__toggle-icon><span class=main-header__toggle-icon--open><svg class="icon icon--primary" viewBox="0 0 24 24"><path d="M3 6H21V8H3V6m0 5H21v2H3V11m0 5H21v2H3V16z"/></svg>
</span><span class=main-header__toggle-icon--close><svg class="icon icon--primary" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg></span></span></label><div class=main-header__nav><nav class=main-nav><ul class=main-nav__menu><li><a href=/docs>Docs</a></li><li><a href=#>Community<svg class="icon icon--primary" viewBox="0 0 24 24"><path d="M7.41 8.58 12 13.17l4.59-4.59L18 10l-6 6-6-6L7.41 8.58z"/></svg></a><ul><li><a href=/community/get-involved/>Get Involved</a></li><li><a href=/community/adopters/>Adopters</a></li><li><a href=/community/ambassadors/>Linkerd Ambassadors</a></li><li><a href=https://linkerd.buoyant.io target=_blank rel=noopener>Buoyant's Linkerd Forum</a></li></ul></li><li class=main-nav__menu--selected><a href=/blog/>Blog</a></li><li><a href=/faq/>FAQ</a></li><li><a href=/enterprise/>Enterprise</a></li></ul><div class=main-nav__search><form action=/search method=get><div class="search-input search-input--sm"><svg class="icon" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5.0 0116 9.5c0 1.61-.59 3.09-1.56 4.23L14.71 14H15.5l5 5L19 20.5l-5-5V14.71L13.73 14.44C12.59 15.41 11.11 16 9.5 16A6.5 6.5.0 013 9.5 6.5 6.5.0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
<input type=text name=q placeholder=Search></div></form></div><div class=main-nav__github><a class=github-button href=https://github.com/linkerd/linkerd2 data-icon=octicon-star data-size=large data-show-count=true aria-label="Star linkerd/linkerd2 on GitHub">Star</a></div><div class=main-nav__forum><a href=https://linkerd.buoyant.io class="button button--primary button--sm" target=_blank rel=noopener>Join Forum</a></div></nav></div></div></header><div class=main-announcement>Service Mesh 101: Get Service Mesh-Certified
<span class=main-announcement__link><a href=https://buoyant.io/courses/service-mesh-101 target=_blank rel=noopener>Enroll now</a></span></div><main class=main-content><div class="blog blog--single"><div class="blog__container container"><div class=blog-post><div class=blog-post__header><h1>gRPC Load Balancing on Kubernetes without Tears</h1><div class=blog-post-meta><div class=blog-post-meta__media><img src=/authors/william-morgan.jpg alt="William Morgan" class="img img--48 img--round"></div><div class=blog-post-meta__body><div class=blog-post-meta__name>William Morgan</div><div class=blog-post-meta__date>Nov 14, 2018 • 6 min read</div></div></div></div><div class=blog-post__cover></div><div class="blog-post__content prose"><p>Many new gRPC users are surprised to find that Kubernetes&rsquo;s default load balancing often doesn&rsquo;t work out of the box with gRPC. For example, here&rsquo;s what happens when you take a <a href=https://github.com/sourishkrout/nodevoto target=_blank rel=noopener>simple gRPC Node.js microservices app</a> and deploy it on Kubernetes:</p><figure><img alt=pods class="img img--max-fill img--center img--rounded" src=/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/grpc-pods.png><figcaption>Pods</figcaption></figure><p>While the <code>voting</code> service displayed here has several pods, it&rsquo;s clear from Kubernetes&rsquo;s CPU graphs that only one of the pods is actually doing any work—because only one of the pods is receiving any traffic. Why?</p><p>In this blog post, we describe why this happens, and how you can easily fix it by adding gRPC load balancing to any Kubernetes app with <a href=https://linkerd.io target=_blank rel=noopener>Linkerd</a>.</p><h2 id=why-does-grpc-need-special-load-balancing>Why does gRPC need special load balancing?</h2><p>First, let&rsquo;s understand why we need to do something special for gRPC.</p><p>gRPC is an increasingly common choice for application developers. Compared to alternative protocols such as JSON-over-HTTP, gRPC can provide some significant benefits, including dramatically lower (de)serialization costs, automatic type checking, formalized APIs, and less TCP management overhead.</p><p>However, gRPC also breaks the standard connection-level load balancing, including what&rsquo;s provided by Kubernetes. This is because gRPC is built on HTTP/2, and HTTP/2 is designed to have a single long-lived TCP connection, across which all requests are *multiplexed—*meaning multiple requests can be active on the same connection at any point in time. Normally, this is great, as it reduces the overhead of connection management. However, it also means that (as you might imagine) connection-level balancing isn&rsquo;t very useful. Once the connection is established, there&rsquo;s no more balancing to be done. All requests will get pinned to a single destination pod, as shown below:</p><figure><img alt=diagram class="img img--max-fill img--center img--rounded" src=/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/diagram-1.png><figcaption>Diagram</figcaption></figure><h2 id=why-doesnt-this-affect-http11>Why doesn&rsquo;t this affect HTTP/1.1?</h2><p>The reason why this problem doesn&rsquo;t occur in HTTP/1.1, which also has the concept of long-lived connections, is because HTTP/1.1 has several features that naturally result in cycling of TCP connections. Because of this, connection-level balancing is &ldquo;good enough&rdquo;, and for most HTTP/1.1 apps we don&rsquo;t need to do anything more.</p><p>To understand why, let&rsquo;s take a deeper look at HTTP/1.1. In contrast to HTTP/2, HTTP/1.1 cannot multiplex requests—meaning that only one HTTP request can be active at a time per TCP connection. The client makes a request, e.g. <code>GET /foo</code>, and then waits until the server responds. While that request-response cycle is happening, no other requests can be issued on that connection.</p><p>Usually, we want lots of requests happening in parallel. Therefore, to have concurrent HTTP/1.1 requests, we need to make multiple HTTP/1.1 connections, and issue our requests across all of them. Additionally, long-lived HTTP/1.1 connections typically expire after some time, and are torn down by the client (or server). These two factors combined mean that HTTP/1.1 requests typically cycle across multiple TCP connections, and so connection-level balancing works.</p><h2 id=so-how-do-we-load-balance-grpc-requests>So how do we load balance gRPC requests?</h2><p>Now back to gRPC. Since we can&rsquo;t balance at the connection level, in order to do gRPC load balancing, we need to shift from connection balancing to <em>request</em> balancing. In other words, we need to open an HTTP/2 connection to each destination, and balance <em>requests</em> across these connections, as shown below:</p><figure><img alt=diagram class="img img--max-fill img--center img--rounded" src=/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/diagram-2.png><figcaption>Diagram</figcaption></figure><p>In network terms, this means we need to make decisions at L5/L7 rather than L3/L4, i.e. we need to understand the protocol sent over the TCP connections.</p><p>How do we accomplish this? There are a couple options. First, our application code could manually maintain its own load balancing &ldquo;pool&rdquo; of destinations, and we could configure our gRPC client to <a href=https://godoc.org/google.golang.org/grpc/balancer target=_blank rel=noopener>use this load balancing pool</a>. This approach gives us the most control, but it can be very complex in environments like Kubernetes where the pool changes over time as Kubernetes reschedules pods.</p><p>Our application would have to watch the Kubernetes API and keep itself up to date with the pods. Alternatively, in Kubernetes, we could deploy our app as <a href=https://kubernetes.io/docs/concepts/services-networking/service/#headless-services target=_blank rel=noopener>headless services</a>. In this case, Kubernetes <a href=https://kubernetes.io/docs/concepts/services-networking/service/#headless-services target=_blank rel=noopener>will create multiple A records</a> in the DNS entry for the service. If our gRPC client is sufficiently advanced, it can automatically maintain the load balancing pool from those DNS entries. But this approach restricts us to certain gRPC clients, and it&rsquo;s rarely possible to only use headless services.</p><p>Finally, we can take a third approach: use a lightweight proxy.</p><h2 id=latency-based-grpc-load-balancing-on-kubernetes-with-linkerd>Latency-based gRPC load balancing on Kubernetes with Linkerd</h2><p><a href=https://linkerd.io target=_blank rel=noopener>Linkerd</a> is a <a href=https://cncf.io target=_blank rel=noopener>CNCF</a>-hosted <em>service mesh</em> for Kubernetes. Most relevant to our purposes, Linkerd also functions as a <em>service sidecar</em>, where it can be applied to a single service—even without cluster-wide permissions. What this means is that when we add Linkerd to our service, it adds a tiny, ultra-fast proxy to each pod, and these proxies watch the Kubernetes API and do gRPC load balancing automatically. Our deployment then looks like this:</p><figure><img alt=multiplex class="img img--max-fill img--center img--rounded" src=/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/multiplex.png><figcaption>Multiplex</figcaption></figure><p>Using Linkerd has a couple big advantages. First, it works with services written in any language, with any gRPC client, and any deployment model (headless or not). Because Linkerd&rsquo;s proxies are completely transparent, they auto-detect HTTP/2 and HTTP/1.x and do L7 load balancing, and they pass through all other traffic as pure TCP. This means that everything will <em>just work.</em></p><p>Second, Linkerd&rsquo;s load balancing is very sophisticated. Not only does Linkerd maintain a watch on the Kubernetes API and automatically update the load balancing pool as pods get rescheduled, Linkerd uses an <em>exponentially-weighted moving average</em> of response latencies to automatically send requests to the fastest pods. If one pod is slowing down, even momentarily, Linkerd will shift traffic away from it. This can reduce end-to-end tail latencies.</p><p>Finally, Linkerd&rsquo;s Rust-based proxies are incredibly fast and small. They introduce &lt;1ms of p99 latency and require &lt;10mb of RSS per pod, meaning that the impact on system performance will be negligible.</p><h2 id=grpc-load-balancing-in-60-seconds>gRPC Load Balancing in 60 seconds</h2><p>Linkerd is very easy to try. Just follow the steps in the <a href=https://linkerd.io/2.16/getting-started/ target=_blank rel=noopener>Linkerd Getting Started Instructions</a> — install the CLI on your laptop, install the control plane on your cluster, and &ldquo;mesh&rdquo; your service (inject the proxies into each pod). You&rsquo;ll have Linkerd running on your service in no time, and should see proper gRPC balancing immediately.</p><p>Let&rsquo;s take a look at our sample <code>voting</code> service again, this time after installing Linkerd:</p><figure><img alt="Voting service" class="img img--max-fill img--center img--rounded" src=/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/voting-service.png><figcaption>Voting service</figcaption></figure><p>As we can see, the CPU graphs for all pods are active, indicating that all pods are now taking traffic—without having to change a line of code. Voila, gRPC load balancing as if by magic!</p><p>Linkerd also gives us built-in traffic-level dashboards, so we don&rsquo;t even need to guess what&rsquo;s happening from CPU charts any more. Here&rsquo;s a Linkerd graph that&rsquo;s showing the success rate, request volume, and latency percentiles of each pod:</p><figure><img alt="Pod overview" class="img img--max-fill img--center img--rounded" src=/2018/11/14/grpc-load-balancing-on-kubernetes-without-tears/pod-overview.png><figcaption>Pod overview</figcaption></figure><p>We can see that each pod is getting around 5 RPS. We can also see that, while we&rsquo;ve solved our load balancing problem, we still have some work to do on our success rate for this service. (The demo app is built with an intentional failure—as an exercise to the reader, see if you can figure it out by using the Linkerd dashboard!)</p><h2 id=wrapping-it-up>Wrapping it up</h2><p>If you&rsquo;re interested in a dead simple way to add gRPC load balancing to your Kubernetes services, regardless of what language it&rsquo;s written in, what gRPC client you&rsquo;re using, or how it&rsquo;s deployed, you can use Linkerd to add gRPC load balancing in a few commands.</p><p>There&rsquo;s a lot more to Linkerd, including security, reliability, and debugging and diagnostics features, but those are topics for future blog posts.</p><p>Want to learn more? We’d love to have you join our rapidly-growing community! Linkerd is <a href=https://github.com/linkerd/linkerd2 target=_blank rel=noopener>hosted on GitHub</a>, and we have a thriving community on <a href=https://slack.linkerd.io target=_blank rel=noopener>Slack</a>, <a href=https://twitter.com/linkerd target=_blank rel=noopener>Twitter</a>, and the <a href=https://lists.cncf.io/g/cncf-linkerd-users target=_blank rel=noopener>mailing lists</a>. Come and join the fun!</p></div></div><div class=blog-post-related><h2>Suggested Blog Posts</h2><div class=blog-post-related__pages><div class="card card--horz card--center"><div class=card__media><img src=/2019/02/02/debugging-ruby-services-in-kubernetes-with-linkerd/cover.png alt=Cover class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2019/02/02/debugging-ruby-services-in-kubernetes-with-linkerd/>Debugging Ruby Services in Kubernetes With Linkerd</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Feb 2, 2019 • 9 min read</div></div></div></div></div><div class="card card--horz card--center"><div class=card__media><img src=/2018/12/08/service-profiles-for-per-route-metrics/cover.png alt=Cover class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2018/12/08/service-profiles-for-per-route-metrics/>Service Profiles for Per-Route Metrics</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Dec 8, 2018 • 7 min read</div></div></div></div></div><div class="card card--horz card--center"><div class=card__media><img src=/2018/11/13/debugging-node-services-in-kubernetes-with-linkerd-2.0/cover.png alt=Cover class="img img--128 img--rounded"></div><div class=card__body><div class=card__header><h4><a href=/2018/11/13/debugging-node-services-in-kubernetes-with-linkerd-2.0/>Debugging Node Services in Kubernetes With Linkerd 2.0</a></h4><div class=blog-post-meta><div class=blog-post-meta__date>Nov 13, 2018 • 9 min read</div></div></div></div></div></div></div></div></div></main><footer class=main-footer><div class=main-footer__top><div class="main-footer__container container"><div class=main-footer__info><p><img src=/logos/linkerd.png alt=Linkerd></p><p>Linkerd was originally created by <a href=https://buoyant.io/ target=_blank rel=noopener>Buoyant</a></p><p>View <a href=https://github.com/linkerd/linkerd/wiki/Linkerd-code-of-conduct target=_blank rel=noopener>Code of Conduct</a></p></div><div class=main-footer__links><div class=main-footer__community><h4>Community</h4><ul><li><a href=https://github.com/linkerd/linkerd2 target=_blank rel=noopener>GitHub</a></li><li><a href=https://slack.linkerd.io target=_blank rel=noopener>Slack</a></li><li><a href=https://linkerd.buoyant.io target=_blank rel=noopener>Linkerd Forum</a></li></ul></div><div class=main-footer__follow><h4>Follow</h4><ul><li><a href=https://www.linkedin.com/company/linkerd/ target=_blank rel=noopener>Linkedin</a></li><li><a href=https://www.youtube.com/buoyantio target=_blank rel=noopener>YouTube</a></li><li><a href=https://twitter.com/linkerd target=_blank rel=noopener>Twitter</a></li></ul></div></div></div></div><div class=main-footer__bottom><div class="main-footer__container container"><p><a href=https://github.com/linkerd/website/tree/main/linkerd.io/content target=_blank rel=noopener>Edit This Site</a></p><p>Copyright © 2024 Linkerd Authors. All rights reserved.</p></div></div></footer></body></html>