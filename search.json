[{"ancestors":["Blog"],"content":"Open source covers a vast spectrum of projects, but they all face the same existential threat: what ensures this project will be maintained in the future? Today, Linkerd adopters can be confident they’re getting the world’s simplest, fastest, lightest service mesh. Can they be confident they’re getting a project that will be around tomorrow? Another decade?\nThese are not theoretical questions. The past 18 months alone have provided many examples of just how fragile open source can be:\nIn February, Weaveworks, the company behind much of the Gitops movement, shut its doors, temporarily leaving the fate of the widely-used CNCF-graduated Flux project in doubt. In March, Redis, a well-established infrastructure project, changed its licenses to move away from open source, following a similar license change by Terraform a year earlier. Last year, Open Service Mesh, a CNCF service mesh project created, maintained, and marketed heavily by Microsoft, shut down entirely, its maintainers reassigned to other projects. Being a CNCF project—even a graduated one—offers no protection. The CNCF does not step in to rescue dying projects. Nor does the backing of a trillion-dollar company; nor a happy and robust community. Each project must find its own answer to this question.\nTo this end, in February we announced a significant change to Linkerd. We would no longer publish stable release packages as part of the open source project. Instead, we would rely on the vendor ecosystem for that work. The change itself was a small simplification of the project, but the goal behind it was very large: to ensure Linkerd could become a truly sustainable project, without relicensing, without violating CNCF rules, and without changing Linkerd’s fundamental open source nature.\nWe made this change the only way we thought was truly realistic: by building a clear economic path from the companies that build their businesses on top of Linkerd, to the vendors who employ, today, 100% of the Linkerd maintainers.\nDid it work?\nThe Linkerd community is amazing Yes. Today, we can confidently state that the future of Linkerd is very bright. You can read Buoyant’s full announcement here.\nWe first need to start with a giant thank you to the Linkerd community. Since our announcement, we’ve had countless conversations with adopters who understood the criticality of what we were trying to do and why it made sense for them; who told us they had our backs; who took this leap of faith with us.\nSince making this change, the Linkerd team has gained both maintainers and momentum, achieving milestone after milestone:\nWe’ve shipped many features that have been languishing in the backlog, including support for IPv6, authorization policy audit mode, and bringing our Gateway API implementation to feature parity with ServiceProfiles. We’ve delivered a new implementation of retries, timeouts, and per-route metrics—some of the earliest code ever shipped in Linkerd—that fixes some long-standing corner case …","description":"","keywords":["linkerd"],"section":"blog","title":"Towards a Sustainable Service Mesh","uri":"https://travisbeckham.github.io/2024/10/23/making-linkerd-sustainable/"},{"ancestors":["Blog"],"content":"Welcome to the October 2024 Edge Release Roundup post, where we dive into the most recent edge releases to help keep everyone up to date on the latest and greatest!\nHow to give feedback Edge releases are a snapshot of our current development work on main; by definition, they always have the most recent features but they may have incomplete features, features that end up getting rolled back later, or (like all software) even bugs. That said, edge releases are intended for production use, and go through a rigorous set of automated and manual tests before being released.\nWe would be delighted to hear how these releases work out for you! You can open a GitHub issue or discussion, join us on Slack, or visit the Buoyant Linkerd Forum – all are great ways to reach us.\nCommunity contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to Lauri Kuittinen, Vadim Makerov, and @kristjankullerkann-wearemp for their contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nRecommendations and breaking changes All these releases are recommended for general use. There is one breaking change: starting in edge-24.9.2, the timestamp in JSON-formatted proxy logs are now ISO8601 strings (for example, 2024-09-09T13:38:56.919918Z). This is an oft-requested feature but, of course, still a breaking change.\nAdditionally, it’s important to realize that edge-24.9.3 introduces a change to the Link CRD that’s not compatible with previous versions of the Linkerd CLI. If you’re using Linkerd multicluster, you’ll need to either upgrade every linkerd cluster to edge-24.9.3 at the same time, or plan to hold off until edge-24.10.2.\nThe releases September’s releases bring in some new improvements to Linkerd statistics and also some important bugfixes. Of course, each edge release includes many dependency updates which we won’t list here, but you can find them in the release notes for each release.\nedge-24.9.3 (September 27, 2024) edge-24.9.3 fixes a panic that would occur if a retried response arrived before the retried request was complete. This may sound bizarre, but it is allowed by the HTTP/2 specification, and using wire-gprc with retries enabled tended to trip over this in the real world.\nThis release also supports configuring the timeout and failure threshold for health probes for the multicluster gateway, to improve reliability in the face of networking issues between clusters. A caution with edge-24.9.3 is that its CLI will have trouble with linkerd clusters that are running earlier releases (looking into the future a bit, edge-24.10.2 fixes this issue).\nedge-24.9.2 (September 12, 2024) Starting in edge-24.9.2, JSON-formatted proxy logs render timestamps as ISO8601 strings, rather than fractional seconds since proxy startup (fixing issue 12505). For example, you might see a log like\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2024-09-09T13:38:56.919918Z\u0026#34;, …","description":"What you need to know about the most recent Linkerd edge releases, October\n2024 edition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: October 2024","uri":"https://travisbeckham.github.io/2024/10/15/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"Welcome to the September 2024 Edge Release Roundup post, where we dive into the most recent edge releases to help keep everyone up to date on the latest and greatest!\nHow to give feedback Edge releases are a snapshot of our current development work on main; by definition, they always have the most recent features but they may have incomplete features, features that end up getting rolled back later, or (like all software) even bugs. That said, edge releases are intended for production use, and go through a rigorous set of automated and manual tests before being released.\nWe would be delighted to hear how these releases work out for you! You can open a GitHub issue or discussion, join us on Slack, or visit the Buoyant Linkerd Forum – all are great ways to reach us.\nCommunity contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to @mozemke for their contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nRecommendations and breaking changes All these releases are recommended for general use. Happily, there are no breaking changes here.\nThe releases August’s edge releases look small, but edge-24.8.1 and edge-24.8.2 provided the finishing touches for Linkerd 2.16, which shipped on August 13! Of course, each edge release has bugfixes and many dependency updates; we won’t list them all here, but you can find them in the release notes for each release.\nOne thing to be aware of here: as of edge-24.8.1, the GRPCRoute CRD is optional; if you don’t install it before installing Linkerd, Linkerd will run without GRPCRoute support, and you’ll need to restart the Linkerd control plane if you add the GRPCRoute CRD after installing Linkerd.\nedge-24.8.3 (August 29, 2024) This release starts the Linkerd 2.17 development cycle, with two fixes for Linkerd Viz: it correctly supports setting the group ID using the linkerd-viz Helm chart (thanks, @mozemke!) and it cleans up font downloading to avoid WAF errors.\nedge-24.8.2 (August 5, 2024) This final touch for Linkerd 2.16 makes certain that Linkerd won’t attempt to bind to IPv6 addresses at all unless IPv6 is enabled.\nedge-24.8.1 (August 2, 2024) This release makes GRPCRoute optional: if you don’t have the GRPCRoute CRD installed when Linkerd starts, Linkerd will run without any GRPCRoute functionality rather than failing to start. (If you add the GRPCRoute CRD after Linkerd is running, you’ll need to restart the Linkerd control plane to enable GRPCRoute support.)\nedge-24.8.1 also improves the status text when an HTTPRoute is incorrectly configured with parentRef pointing to a headless service, to make this situation easier to debug, and makes certain that trace-level logs honor proxy.logHTTPHeaders.\nInstalling the latest edge release Installing the latest edge release needs just a single command.\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSfL https://run.linkerd.io/install-edge | sh …","description":"What you need to know about the most recent Linkerd edge releases, September\n2024 edition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: September 2024","uri":"https://travisbeckham.github.io/2024/09/06/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"Today we’re happy to announce Linkerd 2.16, a major step forward for Linkerd that adds a whole host of new features, including support for IPv6; an “audit mode” for Linkerd’s zero trust security policies; a new implementation of retries, timeouts, and per-route metrics for HTTPRoute and GPRCRoute resources; and much more.\nNew route metrics, retries, and timeouts The 2.16 release continues our goal of ensuring Linkerd is the truly future-proof service mesh. We expect the Gateway API to emerge as the standard for traffic configuration in the Kubernetes space, and when that happens, Linkerd users will be ready. (Of course, we’re also actively involved in the Gateway API to ensure the evolving design continues to make sense for Linkerd users!)\nTo this end, Linkerd 2.16 now publishes metrics for Gateway API HTTPRoute and GRPCRoute resources, so you can capture granular per-route success rates, latencies, request volumes, and other metrics without changing any application code.\nLinkerd 2.16 also adds retry and timeout configuration to these same Gateway API resources, bringing the feature sets for Gateway API and ServiceProfiles to parity (as promised in our February Linkerd 2.15 announcement). This configuration is backed by a new implementation that improves upon Linkerd’s earlier retry and timeout logic in two key ways:\nRequests that time out can now be retried; and Retries and timeouts can now be combined with circuit breaking. Enabling Linkerd’s new retry and timeout support is as simple as adding annotations to Gateway API resources. For example:\nkind: HTTPRoute apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: myapp-default-route namespace: myns annotations: retry.linkerd.io/http: 5xx retry.linkerd.io/limit: \u0026#34;2\u0026#34; retry.linkerd.io/timeout: 300ms spec: parentRefs: - name: myapp kind: Service group: core port: 80 rules: - matches: - path: type: PathPrefix value: \u0026#34;/foo/\u0026#34; In short, Linkerd’s new implementation of per-route metrics, retries, and timeouts are now provided in a principled, future-proof way that is composable with existing features such as circuit breaking, and configured using the Gateway API resources that we believe are the future of service mesh configuration. Learn more.\nAudit mode for security policies Linkerd’s “zero trust” authorization policies provide a powerful and expressive mechanism for controlling which network traffic is allowed. They support a wide variety of approaches to network security, including micro-segmentation and “deny by default” policies. In contrast to ambient or host-proxy approaches, Linkerd’s sidecar design provides a clear security boundary that fits directly into the zero trust model, where each pod makes its own authorization decisions independently, maintains its (and only its) TLS keys, and makes policy decisions based on cryptographic workload identity, not IP addresses.\nHowever, introducing authorization policy in a live system can be tricky. To address this, Linkerd 2.16 introduces a …","description":"","keywords":["linkerd","2.16","features"],"section":"blog","title":"Announcing Linkerd 2.16! Metrics, retries, and timeouts for HTTP and gRPC\nroutes; IPv6 support; policy audit mode; and lots more","uri":"https://travisbeckham.github.io/2024/08/13/announcing-linkerd-2.16/"},{"ancestors":["Blog"],"content":"Welcome to the August 2024 Edge Release Roundup post, where we dive into the most recent edge releases to help keep everyone up to date on the latest and greatest!\nHow to give feedback Edge releases are a snapshot of our current development work on main; by definition, they always have the most recent features but they may have incomplete features, features that end up getting rolled back later, or (like all software) even bugs. That said, edge releases are intended for production use, and go through a rigorous set of automated and manual tests before being released.\nWe would be delighted to hear how these releases work out for you! You can open a GitHub issue or discussion, join us on Slack, or visit the Buoyant Linkerd Forum – all are great ways to reach us.\nCommunity contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to @djryanj for his contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nRecommendations and breaking changes All these releases are recommended for general use. Happily, there are no breaking changes here.\nThe releases The big story for this month is definitely edge-24.7.5: this massive release brings together a lot of threads of work to provide us the bones of the upcoming Linkerd 2.16 release. Of course, each edge release has bugfixes and many dependency updates; we won’t list them all here, but you can find them in the release notes for each release.\nedge-24.7.5 (July 26, 2024) This release supports Server-scoped default policy, policy audit mode, GRPCRoute, and new retry and timeout configuration (including for Gateway API resources). There’s a lot to unpack here:\nServer-scoped default policy: you can use the new accessPolicy field of a Server to override the default inbound policy for that Server. The default is deny, for backward compatibility.\nPolicy audit mode: setting the default inbound policy or a Server’s accessPolicy to audit allows traffic to flow, but logs anything that would be denied.\nGRPCRoute: this release includes support for the Gateway API GRPCRoute. Remember to set enableHttpRoutes to false when installing if you don’t want Linkerd to manage the Gateway API CRDs for you!\nNew retry and timeout configuration: you can now configure retries and timeouts with annotations on Service, HTTPRoute, or GRPCRoute resources, with HTTPRoute and GRPCRoute taking precedence over Service if there are overlaps. Note that these are counted retries, rather than the budgeted retries provided when you configure retries in a Server: you will configure a maximum number of retries rather than a percentage of retries.\nedge-24.7.4 (July 25, 2024) This release correctly supports IPv6 in the Linkerd CNI network-validator and repair-controller containers.\nedge-24.7.3 (July 19, 2024) This release updates the documentation on what networkValidator.connectAddr in the Helm chart means …","description":"What you need to know about the most recent Linkerd edge releases, August 2024\nedition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: August 2024","uri":"https://travisbeckham.github.io/2024/08/05/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"Welcome to the July 2024 Edge Release Roundup post, where we dive into the most recent edge releases to help keep everyone up to date on the latest and greatest!\nHow to give feedback Remember, edge releases are a snapshot of our current development work on main; by definition, they always have the most recent features but they may have incomplete features, features that end up getting rolled back later, or (like all software) even bugs. That said, edge releases are intended for production use, and go through a rigorous set of automated and manual tests before being released.\nWe would be delighted to hear how these releases work out for you! You can open a GitHub issue or discussion, join us on Slack, or visit the Buoyant Linkerd Forum – all are great ways to reach us.\nCommunity contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to Adrian Callejas and John Howard for their contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nRecommendations and breaking changes All these releases are recommended for general use, but there are two breaking changes:\nFirst, as of edge-24.6.2, we change the proxy’s /shutdown endpoint to disabled by default. If you want to reenable it, you’ll need to set proxy.enableShutdownEndpoint to true on installation or upgrade.\nSecond, as of edge-24.6.4, it’s no longer possible - or necessary! - to explicitly set the resource requests for proxy-init. There’s more information on this in the section for edge-24.6.4.\nThe releases We’ve mostly been fixing bugs in these edge releases. Of course, each edge release has many dependency updates; we won’t list them all here, but you can find them in the release notes for each release.\nedge-24.6.4 (June 27, 2024) This release changes the proxy-init container to always request the same amount of memory and CPU as the proxy itself, and removes the ability to explicitly set proxy-init’s requests because there’s now no need to do so. (This doesn’t increase the resources required for the pod as a whole, because the proxy-init container completes before the proxy starts, letting the proxy reuse resources requested by the proxy-init container. For full details, check out issue #11320).\nIt also continues work on upcoming GRPCRoute support. Finally, if proxy.logHTTPHeaders is somehow empty, it correctly defaults to “off”.\nedge-24.6.3 (June 20, 2024) edge-24.6.3 adds the linkerd.io/control-plane-ns label to the ext-namespace-metadata-linkerd-config Role, for parity with the other resources created when installing Linkerd.\nedge-24.6.2 (June 14, 2024) Starting in this release, the proxy’s /shutdown endpoint is disabled by default. It can be reenabled by setting proxy.enableShutdownEndpoint to true when installing or upgrading. Beyond that, edge-24.6.2 fixes several bugs: EndpointSlices with no hostname field are supported (thanks, Adrian Callejas!), …","description":"What you need to know about the most recent Linkerd edge releases, July 2024\nedition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: July 2024","uri":"https://travisbeckham.github.io/2024/07/01/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"Welcome to the June 2024 Edge Release Roundup post, where we dive into the most recent edge releases to help keep everyone up to date on the latest and greatest!\nHow to give feedback Remember, edge releases are a snapshot of our current development work on main; by definition, they always have the most recent features but they may have incomplete features, features that end up getting rolled back later, or (like all software) even bugs. That said, edge releases are intended for production use, and go through a rigorous set of automated and manual tests before being released.\nWe would be delighted to hear how these releases work out for you! You can open a GitHub issue or discussion, join us on Slack, or visit the Buoyant Linkerd Forum – all are great ways to reach us.\nCommunity contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to knowmost, Marwan Ahmed, and Nico Feulner for their contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nRecommendations and breaking changes We recommend edge-24.5.5 for anyone considering an edge-24.5.* release; it has important fixes for the Linkerd CNI plugin on GKE. edge-24.5.1 is specifically not recommended for users of GKE, due to a bug with the default Linkerd configuration in that release.\nStarting in edge-24.5.1, the patchs metric introduced in edge-24.3.4 is renamed to patches.\nFinally, starting in edge-24.5.2, Linkerd will install the GRPCRoute CRD in the gateway.networking.k8s.io API group, in preparation for later GRPCRoute support. (You can disable this by setting enableHttpRoutes to false when installing, which will also prevent Linkerd from installing the HTTPRoute CRD in the gateway.networking.k8s.io API group.)\nThe releases This group of releases has focused on IPv6 support - delivered in edge-24.5.2! - and finalizing fixes for some edge cases in the way Linkerd handles EndpointSlices and HTTPRoutes. Of course, each edge release has many dependency updates; we won’t list them all here, but you can find them in the release notes for each release.\nedge-24.5.5 (May 31, 2024) This release switches IPv6 off by default for the entire control plane, including the Linkerd CNI plugin. Set disableIPv6 to false to enable IPv6.\nedge-24.5.4 (May 23, 2024) We recommend edge-24.5.5 instead of this release. In this release, IPv6 support is off by default for most of the control plane, but it is mistakenly on by default for the Linkerd CNI plugin.\nThis release adds support for JSON output to the linkerd inject, linkerd uninject and linkerd profile commands, and a --token flag to linkerd diagnostics policy that allows specifying the context token to use so that you can see how specific clients will see policies. It also adds support for setting the group ID for the control plane (thanks, Nico Feulner!), switches IPv6 to off by default for the control plane, adds support …","description":"What you need to know about the most recent Linkerd edge releases, June 2024\nedition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: June 2024","uri":"https://travisbeckham.github.io/2024/06/06/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"Earlier this week at Kubecon EU, Microsoft announced the Service Mesh Interface (SMI). SMI is a specification that exposes core service mesh features like telemetry, traffic shifting, and policy, purely via Kubernetes primitives.\nThe Linkerd community has been working heavily with our friends from Microsoft and Hashicorp on SMI for the past few months. In fact, the top two contributors to SMI are Linkerd’s Thomas Rampelberg and Hashicorp’s Nick Jackson—a healthy sign of serious community involvement.\nThe reason we’ve invested so much time and energy in SMI is that we believe it will significantly opens up the ecosystem of tools and products built top of the service mesh. With SMI, awesome tools like Flagger, Kubecost, and Kiali can build on SMI rather than tying their implementation to a specific mesh. This is great for Linkerd users and great for the community as a whole.\nLinkerd 2.3 already implements SMI’s Traffic Metrics spec. In next month’s 2.4, we’ll add support for Traffic Split (which means that, yes, next month Linkerd will support traffic shifting for canaries and blue-green deploys!), and in later releases we’ll flesh out Linkerd’s support for the remaining SMI interfaces.\nLinkerd is the fastest, lightest, simplest service mesh in the world, and we’re excited about SMI’s ability to bring Linkerd to even more users around the globe.\nLinkerd is a community project and is hosted by the Cloud Native Computing Foundation. If you have feature requests, questions, or comments, we’d love to have you join our rapidly-growing community! Linkerd is hosted on GitHub, and we have a thriving community on Slack, Twitter, and the mailing lists. Come and join the fun!\nImage credit: I G\n","description":"","keywords":["community","linkerd"],"section":"blog","title":"Linkerd and SMI","uri":"https://travisbeckham.github.io/2024/05/24/linkerd-and-smi/"},{"ancestors":["Blog"],"content":"Smooth workflows are critical in the cloud-native world. This CNCF Cloud Native Live session explores building a rock-solid Internal Developer Platform (IDP) Backstage that empowers developers with self-service tools while streamlining security and observability. Watch the recording with Linkerd Tech Evangelist Flynn and Backstage Maintainer Ben Lambert as they show you how a service mesh like Linkerd can supercharge your IDP, ensuring seamless communication and clear visibility.\n","description":"This CNCF Cloud Native Live session explores building a rock-solid Internal\nDeveloper Platform (IDP) Backstage that empowers developers with self-service\ntools while streamlining security and observability.","keywords":["backstage"],"section":"blog","title":"Building a Secure, Reliable, Observable IDP with Backstage \u0026 Linkerd","uri":"https://travisbeckham.github.io/2024/05/23/building-a-secure-reliable-observable-idp-with-backstage-linkerd/"},{"ancestors":["Blog"],"content":"Welcome to the April 2024 Edge Release Roundup post, where we dive into the most recent edge releases to help keep everyone up to date on the latest and greatest! This month, we’re covering the releases from edge-24.4.5 back to edge-24.3.3 – there’s a lot here so we’ll get right into it.\nHow to give feedback Remember, edge releases are a snapshot of our current development work on main; by definition, they always have the most recent features but they may have incomplete features, features that end up getting rolled back later, or (gasp!) even bugs. If you’re running edge releases, it’s very important that you send us feedback on how things are going for you!\nWe would be delighted to hear how these releases work out for you! You can open a GitHub issue or discussion, join us on Slack, or visit the Buoyant Linkerd Forum – all are great ways to reach us.\nCommunity contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to Adarsh Jaiswal, Akshay Dongaonkar, Cemal Y. Dalar, Firas Medini, Grigoriy Mikhalkin, hanghuge, Heiko Voigt, Ilia Lazebnik, Hirotaka Tagawa, and occupyhabit for their contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nBreaking changes and recommendations There are no breaking changes in these releases. However, we have two specific recommendations:\nWe recommend edge-24.4.4 instead of edge-24.4.3 due to a metrics-naming regression in edge-24.4.3. We recommend edge-24.3.4 instead of edge-24.3.3 since edge-24.3.4 contains an important fix for Gateway API users. The releases Recent edge releases have been mostly focused on upcoming IPv6 support and some significant bugfixes. Of course, each edge release has many dependency updates; we won’t list them all here, but you can find them in the release notes for each release.\n(Last Roundup we did these in chronological order, but we’re switching this time to reverse chronological order, starting with the most recent.)\nedge-24.4.5 (April 25, 2024) This edge release fixes support for native sidecars in the Linkerd CNI plugin, continues work on upcoming IPv6 support, and allows setting revisionHistoryLimit when installing with Helm to specify how many ReplicaSets to keep around for rollback purposes (thanks, Ilia Lazebnik!)\nIt also allows setting certain HTTP/2 server parameters using environment variables in the proxy container (see proxy PR 2924 if you think you need this!).\nedge-24.4.4 (April 18, 2024) This edge release fixes a metrics naming regression introduced in the previous release, restoring the outbound_http_... metrics to their correct names instead of duplicating http.\nedge-24.4.3 (April 18, 2024) We recommend edge-24.4.4 instead of this release due to a metrics-naming regression in edge-24.4.3.\nThis edge release fixes the second of two issues where where policy.linkerd.io HTTPRoutes could be endlessly patched even when they weren’t …","description":"What you need to know about the most recent Linkerd edge releases, April 2024\nedition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: April 2024","uri":"https://travisbeckham.github.io/2024/04/25/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"With the changes to Linkerd’s release model, edge releases have clearly become more important! In light of that, we’ll be doing these Edge Release Roundup posts every month, to help keep everyone up to date on the latest and greatest. Going forward, we’ll cover everything since the previous roundup, but for this first post of the new series, we’ll take a longer look back, and tackle all the edge releases from February to the present.\nMajor Process Changes There have been two significant changes in the edge release process that we want to highlight.\nAutomated Edge Releases\nAs of edge-24.2.5, we’ve automated the process of creating edge releases. While this is mostly an internal process change, it also means that you may see edge releases where the release notes are just a bunch of commit logs (especially from dependabot). We’ll still make additional callouts for more significant changes, though.\nHelm Chart Release Numbers\nAs of edge-24.3.1, we’re changing the versioning scheme for our Helm charts to match the date-oriented scheme for the edge releases themselves. The only difference is that the Helm chart uses a four-digit year number – for example, the Helm charts for edge-24.3.1 has version 2024.3.1. This should simplify things for everyone.\nCommunity Contributions We couldn’t do what we do without the Linkerd community, and this batch of releases is definitely no exception. Huge thanks to Aurel Canciu, Adam Toy, Rui Chen, Michael Bell, and Jan Kantert for their contributions! You’ll find more information about all of these contributions in the release-by-release details below.\nBreaking Changes There is a breaking change in edge-24.2.4.\nUp through edge-24.2.3, the ExternalWorkload CRD (version v1alpha1) specified the identity that an external workload should use as the meshTls stanza. In edge-24.2.4, we updated ExternalWorkload to v1beta1 and changed the spelling of this stanza to meshTLS (note the capitalization) to better align with the rest of our CRDs. Unfortunately, in the process we also broke the v1alpha1 ExternalWorkload CRD: anyone using ExternalWorkload in edge-24.2.3 would find that edge-24.2.4 didn’t honor those ExternalWorkloads.\nWe don’t think that this will have affected anyone in practice, but this section will list all the breaking changes we know of, whether we think they could affect anyone or not.\nThe Releases From the feature perspective, recent edge releases have been focused on automating the edge release process, and on improving mesh expansion. We’ve also fixed quite a few things and, of course, each edge release has many dependency updates.\nedge-24.2.1 (changelog) This edge release improves the performance and stability of the destination controller and the ExternalWorkloads EndpointSlice controller.\nThe destination controller can ride through outdated Server CRDs in Pod-to-Pod multicluster, and it will only process Server updates for workloads actually affected by the Server. EndpointSlices generated for …","description":"What you need to know about the most recent Linkerd edge releases, March 2024\nedition!","keywords":["linkerd","edge","release","roundup"],"section":"blog","title":"Linkerd Edge Release Roundup: March 2024","uri":"https://travisbeckham.github.io/2024/03/12/linkerd-edge-release-roundup/"},{"ancestors":["Blog"],"content":"Today we’re happy to announce the release of Linkerd 2.15, which adds support for workloads outside of Kubernetes. This new “mesh expansion” feature allows Linkerd users for the first time to bring applications running on VMs, physical machines, and other non-Kubernetes locations into the mesh, delivering Linkerd’s uniform layer of secure, reliable, and observable connectivity across both Kubernetes and non-Kubernetes workload alike.\nThe 2.15 release also introduces support for SPIFFE, a standard for workload identity which allows Linkerd to provide cryptographic identity and authentication to off-cluster workloads, and for native sidecar containers, a new Kubernetes feature that eases some of the long-standing annoyances of the sidecar model in Kubernetes, especially with Job workloads.\nFinally, this release introduces some important changes in the way that we’re publishing Linkerd. While Linkerd will always be open source, as of 2.15, we will no longer be producing open source stable release artifacts. If you’re running Linkerd in production today, please see the section A new model for stable releases below for what this means for you.\nAs usual, the 2.15 release includes a massive list of bugfixes and improvements. Read on for details!\nMesh expansion As we promised last November, Linkerd 2.15 introduces mesh expansion: the ability to deploy Linkerd’s ultralight Rust microproxies anywhere outside of Kubernetes and connect them to a Linkerd control plane running on a Kubernetes cluster. This allows Linkerd to handle non-Kubernetes workloads, upleveling all TCP communication to and from these workloads secure, reliable, and observable. Non-Kubernetes applications get the full set of Linkerd features, including mutual TLS, retries, timeouts, circuit breaking, latency-aware load balancing, dynamic per-request routing, zero trust authorization policies, and much more.\nMesh expansion is an important part of achieving our goal of making Linkerd the universal networking layer for cloud native organizations. While we love Kubernetes, we recognize that even the most sophisticated organizations often still have significant investments in applications that don’t run outside of it. With Linkerd 2.15, regardless of whether your workloads are running on resource-constrained ARM64 edge devices, legacy “big iron” VMs, or physical machines in your server closet, Linkerd’s uniform layer of security, reliability, and observability is at your disposal.\nThis move was made significantly easier by Linkerd’s core design of ultralight microproxies written in the Rust programming language. The use of Rust, which has the “ability to prevent memory-related bugs, manage concurrency, and generate small, efficient binaries” allows Linkerd not just to avoid the memory vulnerabilities that are endemic to languages like C and C++, but to provide minimal resource footprint and—most importantly—a minimal operational burden to the user. Linkerd’s Rust microproxies are key to its …","description":"","keywords":["linkerd","2.15","features","vault"],"section":"blog","title":"Announcing Linkerd 2.15 with mesh expansion, native sidecars, and SPIFFE","uri":"https://travisbeckham.github.io/2024/02/21/announcing-linkerd-2.15/"},{"ancestors":["Blog"],"content":"This blog post is based on a workshop that I delivered way back in September 2023(!) at Buoyant’s Service Mesh Academy. If this seems interesting, check out the full recording!\nLinkerd Certificate Management with Vault Linkerd’s ability to automatically secure communications using mTLS has always been one of its headline features. Of course, mTLS requires certificates, and managing certificates can be very tricky: you need to generate them, rotate them, and distribute them to all the places that need them… while still being careful to avoid exposing any private keys.\nFor many of the demos we do, we sidestep all this by letting linkerd install silently generate the certificates we need, then ignoring them beyond that. This is vaguely OK for a demo, but it’s totally unacceptable for the real world. In the real world:\nThe secret key for Linkerd’s trust anchor should never be stored on the cluster at all.\nHowever, you’ll need access to the secret key to rotate the identity issuer certificate, which should happen frequently.\nFinally, your organization may require that the trust anchor certificate must be issued by a corporate certifying authority (CA), rather than being some self-signed monstrosity. (They might require that of the identity issuer certificate too: in many situations, the corporate security folks don’t like delegating issuing authority, for various reasons.)\nUltimately, the way to tackle all of these issues is to use an external CA to issue at least the trust anchor certificate. There are several ways to set that up: in this article, we’ll walk through a fairly real-world scenario:\nWe’ll install Linkerd without generating any certificates by hand, and without having Linkerd generate the certificates itself;\nWe’ll have Vault running outside the cluster to store keys and generate certificates; and\nWe’ll have cert-manager running inside the cluster to get the things Linkerd needs from Vault, and store them where Linkerd needs to find them.\nNote that our goal is not to teach you how to use Vault, in particular: it’s to show a practical, relatively low-effort way to actually use external PKI with Linkerd to bootstrap a zero-trust environment in Kubernetes. Many companies have existing external PKI already set up (whether with Vault or something else); being able to make use of it without too much work is a huge win\nThe Setup In order to demo all this simply, we’ll be running Kubernetes in a k3d cluster. We’ll run Vault in Docker to make things easy to demo, but we will not be running Docker in Kubernetes: Vault will run as a separate Docker container that happens to be connected to the same Docker network as our k3d cluster.\nThe big win of this setup is that you can run it completely on a laptop with no external dependencies. If you want to replicate this with a cluster in the cloud, that’s no problem: just figure out a reasonable place outside the cluster to run your Vault instance, and make sure that both your Kubernetes cluster and your …","description":"","keywords":["linkerd","2.14","features","vault"],"section":"blog","title":"Workshop Recap: Linkerd Certificate Management with Vault","uri":"https://travisbeckham.github.io/2024/02/06/linkerd-certificates-with-vault/"},{"ancestors":["Blog"],"content":"We are excited to announce this month’s Linkerd Hero: TJ Miller. Congrats, TJ!\nWhat are Linkerd Heroes? Linkerd Heroes are community members who best represent the spirit of the Linkerd community. Whether it’s helping others, answering questions, sharing their successes (and failures!) with the world at large, or contributing code or docs, Linkerd Heroes are instrumental in making the Linkerd community the amazing place it is today.\nCongrats, TJ Miller! This month’s hero is TJ Miller. TJ spearheaded the PR for injecting the Linkerd proxy as a native sidecar container. Linkerd has relied on the sidecar pattern since its humble beginnings, but Kubernetes version 1.29 finally adds first-class support for sidecars to Kubernetes itself. Allowing Linkerd to use this new feature is a definite improvement for many users, and we thank TJ for thoroughly researching this feature and implementing it impeccably. Working with him and bringing this exciting improvement to full fruition was a pleasure!\nNominate next month’s Linkerd Hero Linkerd Heroes take many forms. Perhaps someone has answered your or the community’s pressing questions on Slack. Maybe you’ve read a blog post or watched a conference talk that helped spur your decision-making process or advance your Linkerd implementation. Or perhaps someone contributed an awesome Linkerd feature or bugfix that has made your life a lot easier. Who is your Linkerd Hero? Submit your nomination for next month’s hero here!\nFor those who nominated someone, thank you for participating!\n","description":"We are excited to announce this month's Linkerd Hero: TJ Miller, for\ncontributing to Linkerd and making it better for the entire community!","keywords":["community","hero","contributor","code"],"section":"blog","title":"Announcing the December 2023 Linkerd Hero!","uri":"https://travisbeckham.github.io/2023/12/22/announcing-the-december-2023-linkerd-hero/"},{"ancestors":["Blog"],"content":"We are excited to announce this month’s Linkerd Hero: Takumi Sue. Congrats, Takumi!\nWhat are Linkerd Heroes? Linkerd Heroes are community members who best represent the spirit of the Linkerd community. Whether it’s helping others, answering questions, sharing their successes (and failures!) with the world at large, or contributing code or docs, Linkerd Heroes are instrumental in making the Linkerd community the amazing place it is today.\nCongrats, Takumi Sue! This month’s hero is Takumi Sue. Over the past three years, Takumi has regularly contributed fixes to Linkerd’s code base. From CLI fixes to Helm template and control plane changes, Takumi has positively impacted the project. Thank you for being such a prolific contributor and dedicating your time to making Linkerd an even more awesome project!\nNominate next month’s Linkerd Hero Linkerd Heroes take many forms. Perhaps someone has answered your or the community’s pressing questions on Slack. Maybe you’ve read a blog post or watched a conference talk that helped spur your decision-making process or advance your Linkerd implementation. Or perhaps someone contributed an awesome Linkerd feature or bugfix that has made your life a lot easier. Who is your Linkerd Hero? Submit your nomination for next month’s hero here!\nFor those who nominated someone, thank you for participating!\n","description":"We are excited to announce this month's Linkerd Hero: Takumi Sue, for\ncontributing to Linkerd and making it better for the entire community!","keywords":["community","hero","contributor","code"],"section":"blog","title":"Announcing the November 2023 Linkerd Hero!","uri":"https://travisbeckham.github.io/2023/11/22/announcing-the-november-2023-linkerd-hero/"},{"ancestors":["Blog"],"content":"In the upcoming 2.15 release, I’m happy to report that we’ll be shipping both mesh expansion and support for SPIFFE and SPIRE. We’ve heard from many Linkerd adopters over the past 6 months who’ve let us know that these features are a priority for them, and we’ve been working hard to deliver them with all the simplicity and security that you’ve come to expect from Linkerd.\nThis work on Linkerd is jointly sponsored by Buoyant (creators of Linkerd) and by SUSE, who have recently announced a partnership on this topic.\nIn this blog post I want to describe what these features are, why we’re adding them to Linkerd, and how we tackled some of the design decisions behind the scenes.\nWhat is mesh expansion? Mesh expansion is the ability to run Linkerd’s dataplane outside of the Kubernetes cluster, allowing you to bring non-K8s workloads, including legacy applications running on VMs, into the mesh. Once in the mesh, all communication to and from these non-Kubernetes applications get the same security, reliability, and observability features that Linkerd provides for on-cluster resources, including encryption, authentication, authorization using true workload identity (not IP address!), retries, timeouts, latency-aware load balancing, failover, and more.\nIn short: mesh expansion means Linkerd will support non-Kubernetes workloads, a major step forward for the project. And a major challenge for the project as well.\nHappily, Linkerd has one big advantage in tackling mesh expansion, stemming from an early design decision. In Linkerd, the “dataplane” refers to Linkerd’s Rust microproxies, which handle the actual sensitive information traveling between application components. These microproxies are actually not Kubernetes-specific at all. They are specific to Linkerd, i.e. they’re not general-purpose proxies, but they don’t know anything about Kubernetes; they don’t talk to the Kubernetes API; and they don’t have any requirements about being run on a Kubernetes cluster or even in a container. Linkerd’s microproxies are delivered as static binaries which can be compiled for a variety of architectures and platforms, meaning that they can run on almost any machine in the world (including ARM64, which Linkerd has supported for years.)\n(This microproxy approach is also what makes Linkerd unique in the service mesh space, and allows it to deliver all the security and operational benefits of a sidecar approach without the drawbacks found in other service mesh projects. I’ve written a lot about this in my writeup on eBPF vs sidecars.)\nSo for Linkerd, at least, running the proxies outside of Kubernetes is easy. But that is just one small part of a much larger set of requirements for mesh expansion.\nWhat does mesh expansion require? The biggest challenge for mesh expansion is that, as you might expect, Kubernetes is no longer available to provide the infrastructure we can build on! In our design process for mesh expansion, we uncovered four key areas to this problem: …","description":"","keywords":["linkerd"],"section":"blog","title":"Mesh expansion and SPIFFE support arriving in the upcoming Linkerd 2.15","uri":"https://travisbeckham.github.io/2023/11/07/linkerd-mesh-expansion/"},{"ancestors":["Docs","Features"],"content":"Linkerd can proxy all TCP connections. For HTTP connections (including HTTP/1.0, HTTP/1.1, HTTP/2, and gRPC connections), it will automatically enable advanced L7 features including request-level metrics, latency-aware load balancing, retries, and more.\n(See TCP Proxying and Protocol Detection for details of how this detection happens automatically, and how it can sometimes fail.)\nNote that while Linkerd does zero-config mutual TLS, it cannot decrypt TLS connections initiated by the outside world. For example, if you have a TLS connection from outside the cluster, or if your application does HTTP/2 plus TLS, Linkerd will treat these connections as raw TCP streams. To take advantage of Linkerd’s full array of L7 features, communication between meshed pods must be TLS’d by Linkerd, not by the application itself.\n","description":"Linkerd will automatically enable advanced features (including metrics, load balancing, retries, and more) for HTTP, HTTP/2, and gRPC connections.","keywords":null,"section":"2.16","title":"HTTP, HTTP/2, and gRPC Proxying","uri":"https://travisbeckham.github.io/2.16/features/http-grpc/"},{"ancestors":["Docs"],"content":"Linkerd is a service mesh for Kubernetes. It makes running services easier and safer by giving you runtime debugging, observability, reliability, and security—all without requiring any changes to your code.\nFor a brief introduction to the service mesh model, we recommend reading The Service Mesh: What Every Software Engineer Needs to Know about the World’s Most Over-Hyped Technology.\nLinkerd is fully open source, licensed under Apache v2, and is a Cloud Native Computing Foundation graduated project. Linkerd is developed in the open in the Linkerd GitHub organization.\nLinkerd has two basic components: a control plane and a data plane. Once Linkerd’s control plane has been installed on your Kubernetes cluster, you add the data plane to your workloads (called “meshing” or “injecting” your workloads) and voila! Service mesh magic happens.\nYou can get started with Linkerd in minutes!\nHow it works Linkerd works by installing a set of ultralight, transparent “micro-proxies” next to each service instance. These proxies automatically handle all traffic to and from the service. Because they’re transparent, these proxies act as highly instrumented out-of-process network stacks, sending telemetry to, and receiving control signals from, the control plane. This design allows Linkerd to measure and manipulate traffic to and from your service without introducing excessive latency.\nIn order to be as small, lightweight, and safe as possible, Linkerd’s micro-proxies are written in Rust and specialized for Linkerd. You can learn more about the these micro-proxies in our blog post, Under the hood of Linkerd’s state-of-the-art Rust proxy, Linkerd2-proxy, (If you want to know why Linkerd doesn’t use Envoy, you can learn why in our blog post, Why Linkerd doesn’t use Envoy.)\nGetting Linkerd Linkerd is available in a variety of packages and channels. See the Linkerd Releases page for details.\nNext steps Get started with Linkerd in minutes, or check out the architecture for more details on Linkerd’s components and how they all fit together.\n","description":"","keywords":null,"section":"2.16","title":"Overview","uri":"https://travisbeckham.github.io/2.16/overview/"},{"ancestors":["Docs"],"content":"Welcome to Linkerd! 🎈\nIn this guide, we’ll walk you through how to install Linkerd into your Kubernetes cluster. Then we’ll deploy a sample application to show off what Linkerd can do.\nThis guide is designed to walk you through the basics of Linkerd. First, you’ll install the CLI (command-line interface) onto your local machine. Using this CLI, you’ll then install the control plane onto your Kubernetes cluster. Finally, you’ll “mesh” an application by adding Linkerd’s data plane to it.\nEdge release content This page contains instructions using the latest edge release of Linkerd. For other types of releases, including stable releases, please see Linkerd Releases. Step 0: Setup Before anything else, we need to ensure you have access to modern Kubernetes cluster and a functioning kubectl command on your local machine. (If you don’t already have a Kubernetes cluster, one easy option is to run one on your local machine. There are many ways to do this, including kind, k3d, Docker for Desktop, and more.)\nValidate your Kubernetes setup by running:\nkubectl version You should see output with both a Client Version and Server Version component.\nNow that we have our cluster, we’ll install the Linkerd CLI and use it validate that your cluster is capable of hosting Linkerd.\nNote If you’re using a GKE “private cluster”, or if you’re using Cilium as a CNI, there may be some cluster-specific configuration before you can proceed to the next step. Step 1: Install the CLI If this is your first time running Linkerd, you will need to download the linkerd CLI onto your local machine. The CLI will allow you to interact with your Linkerd deployment.\nTo install the CLI manually, run:\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSfL https://run.linkerd.io/install-edge | sh Be sure to follow the instructions to add it to your path:\nexport PATH=$HOME/.linkerd2/bin:$PATH This will install the CLI for the latest edge release of Linkerd. (For more information about what edge releases are, see our Releases and Versions page.)\nOnce installed, verify the CLI is running correctly with:\nlinkerd version You should see the CLI version, and also Server version: unavailable. This is because you haven’t installed the control plane on your cluster. Don’t worry—we’ll fix that soon enough.\nMake sure that your Linkerd version and Kubernetes version are compatible by checking Linkerd’s supported Kubernetes versions.\nStep 2: Validate your Kubernetes cluster Kubernetes clusters can be configured in many different ways. Before we can install the Linkerd control plane, we need to check and validate that everything is configured correctly. To check that your cluster is ready to install Linkerd, run:\nlinkerd check --pre If there are any checks that do not pass, make sure to follow the provided links and fix those issues before proceeding.\nStep 3: Install Linkerd onto your cluster Now that you have the CLI running locally and a cluster that is ready to go, it’s time to install Linkerd on your Kubernetes …","description":"","keywords":null,"section":"2.16","title":"Getting Started","uri":"https://travisbeckham.github.io/2.16/getting-started/"},{"ancestors":["Docs","Features"],"content":"Linkerd is capable of proxying all TCP traffic, including TLS connections, WebSockets, and HTTP tunneling.\nIn most cases, Linkerd can do this without configuration. To accomplish this, Linkerd performs protocol detection to determine whether traffic is HTTP or HTTP/2 (including gRPC). If Linkerd detects that a connection is HTTP or HTTP/2, Linkerd automatically provides HTTP-level metrics and routing.\nIf Linkerd cannot determine that a connection is using HTTP or HTTP/2, Linkerd will proxy the connection as a plain TCP connection, applying mTLS and providing byte-level metrics as usual.\n(Note that HTTPS calls to or from meshed pods are treated as TCP, not as HTTP. Because the client initiates the TLS connection, Linkerd is not be able to decrypt the connection to observe the HTTP transactions.)\nConfiguring protocol detection Note If your proxy logs contain messages like protocol detection timed out after 10s, or you are experiencing 10-second delays when establishing connections, you are likely running into a protocol detection timeout. This section will help you understand how to fix this. In some cases, Linkerd’s protocol detection will time out because it doesn’t see any bytes from the client. This situation is commonly encountered when using protocols where the server sends data before the client does (such as SMTP) or protocols that proactively establish connections without sending data (such as Memcache). In this case, the connection will proceed as a TCP connection after a 10-second protocol detection delay.\nTo avoid this delay, you will need to provide some configuration for Linkerd. There are two basic mechanisms for configuring protocol detection: opaque ports and skip ports:\nOpaque ports instruct Linkerd to skip protocol detection and proxy the connection as a TCP stream Skip ports bypass the proxy entirely. Opaque ports are generally preferred as they allow Linkerd to provide mTLS, TCP-level metrics, policy, etc. Skip ports circumvent Linkerd’s ability to provide security features.\nLinkerd maintains a default list of opaque ports that corresponds to the standard ports used by protocols that interact poorly with protocol detection. As of the 2.12 release, that list is: 25 (SMTP), 587 (SMTP), 3306 (MySQL), 4444 (Galera), 5432 (Postgres), 6379 (Redis), 9300 (ElasticSearch), and 11211 (Memcache).\nProtocols that may require configuration The following table contains common protocols that may require additional configuration.\nProtocol Standard port(s) In default list? Notes SMTP 25, 587 Yes MySQL 3306 Yes MySQL with Galera 3306, 4444, 4567, 4568 Partially Ports 4567 and 4568 are not in Linkerd’s default set of opaque ports PostgreSQL 5432 Yes Redis 6379 Yes ElasticSearch 9300 Yes Memcache 11211 Yes If you are using one of those protocols, follow this decision tree to determine which configuration you need to apply.\nMarking ports as opaque You can use the config.linkerd.io/opaque-ports annotation to mark a port as opaque. Note that this …","description":"Linkerd is capable of proxying all TCP traffic, including TLS'd connections, WebSockets, and HTTP tunneling.","keywords":null,"section":"2.16","title":"TCP Proxying and Protocol Detection","uri":"https://travisbeckham.github.io/2.16/features/protocol-detection/"},{"ancestors":["Docs"],"content":"Linkerd offers many features, outlined below. For our walkthroughs and guides, please see the Linkerd task docs. For a reference, see the Linkerd reference docs.\nLinkerd’s features HTTP, HTTP/2, and gRPC Proxying Linkerd will automatically enable advanced features (including metrics, load balancing, retries, and more) for HTTP, HTTP/2, and gRPC connections. TCP Proxying and Protocol Detection Linkerd is capable of proxying all TCP traffic, including TLS\u0026#39;d connections, WebSockets, and HTTP tunneling. Retries and Timeouts Linkerd can perform service-specific retries and timeouts. Automatic mTLS Linkerd automatically enables mutual Transport Layer Security (TLS) for all communication between meshed applications. Ingress Linkerd can work alongside your ingress controller of choice. Telemetry and Monitoring Linkerd automatically collects metrics from all services that send traffic through it. Load Balancing Linkerd automatically load balances requests across all destination endpoints on HTTP, HTTP/2, and gRPC connections. Authorization Policy Linkerd can restrict which types of traffic are allowed between meshed services. Automatic Proxy Injection Linkerd will automatically inject the data plane proxy into your pods based annotations. CNI Plugin Linkerd can optionally use a CNI plugin instead of an init-container to avoid NET_ADMIN capabilities. Dashboard and on-cluster metrics stack Linkerd provides a full on-cluster metrics stack, including CLI tools and dashboards. Distributed Tracing You can enable distributed tracing support in Linkerd. Dynamic Request Routing Linkerd can route individual HTTP requests based on their properties. Fault Injection Linkerd provides mechanisms to programmatically inject failures into services. High Availability The Linkerd control plane can run in high availability (HA) mode. HTTP Access Logging Linkerd proxies can be configured to emit HTTP access logs. HTTPRoutes Linkerd can use the HTTPRoute resource to configure per-route policies. Iptables-nft Support Linkerd\u0026#39;s init container can use iptables-nft on systems that require it. IPv6 Support Linkerd is compatible with both IPv6-only and dual-stack clusters. Multi-cluster communication Linkerd can transparently and securely connect services that are running in different clusters. Non-Kubernetes workloads (mesh expansion) Service Profiles Linkerd\u0026#39;s service profiles enable per-route metrics as well as retries and timeouts. Traffic Split (canaries, blue/green deploys) Linkerd can dynamically send a portion of traffic to different services. ","description":"","keywords":null,"section":"2.16","title":"Features","uri":"https://travisbeckham.github.io/2.16/features/"},{"ancestors":["Docs","Features"],"content":"Timeouts and automatic retries are two of the most powerful and useful mechanisms a service mesh has for gracefully handling partial or transient application failures.\nTimeouts and retries can be configured using HTTPRoute, GRPCRoute, or Service resources. Retries and timeouts are always performed on the outbound (client) side.\nNote If working with headless services, outbound policy cannot be retrieved. Linkerd reads service discovery information based off the target IP address, and if that happens to be a pod IP address then it cannot tell which service the pod belongs to. These can be setup by following the guides:\nConfiguring Retries Configuring Timeouts ","description":"Linkerd can perform service-specific retries and timeouts.","keywords":null,"section":"2.16","title":"Retries and Timeouts","uri":"https://travisbeckham.github.io/2.16/features/retries-and-timeouts/"},{"ancestors":["Docs","Features"],"content":"By default, Linkerd automatically enables mutually-authenticated Transport Layer Security (mTLS) for all TCP traffic between meshed pods. This means that Linkerd adds authenticated, encrypted communication to your application with no extra work on your part. (And because the Linkerd control plane also runs on the data plane, this means that communication between Linkerd’s control plane components are also automatically secured via mTLS.)\nSee Caveats and future work below for some details.\nWhat is mTLS? mTLS, or mutual TLS, is simply “regular TLS” with the extra stipulation that the client is also authenticated. TLS guarantees authenticity, but by default this only happens in one direction–the client authenticates the server but the server doesn’t authenticate the client. mTLS makes the authenticity symmetric.\nmTLS is a large topic. For a broad overview of what mTLS is and how it works in Kuberentes clusters, we suggest reading through A Kubernetes engineer’s guide to mTLS.\nWhich traffic can Linkerd automatically mTLS? Linkerd transparently applies mTLS to all TCP communication between meshed pods. However, there are still ways in which you may still have non-mTLS traffic in your system, including:\nTraffic to or from non-meshed pods (e.g. Kubernetes healthchecks) Traffic on ports that were marked as skip ports, which bypass the proxy entirely. You can verify which traffic is mTLS’d in a variety of ways. External systems such as Buoyant Cloud can also automatically generate reports of TLS traffic patterns on your cluster.\nOperational concerns Linkerd’s mTLS requires some preparation for production use, especially for long-lived clusters or clusters that expect to have cross-cluster traffic.\nThe trust anchor generated by the default linkerd install CLI command expires after 365 days. After that, it must be manually rotated—a non-trivial task. Alternatively, you can provide the trust anchor yourself and control the expiration date, e.g. setting it to 10 years rather than one year.\nKubernetes clusters that make use of Linkerd’s multi-cluster communication must share a trust anchor. Thus, the default linkerd install setup will not work for this situation and you must provide an explicit trust anchor.\nSimilarly, the default cluster issuer certificate and key expire after a year. These must be rotated before they expire. Alternatively, you can set up automatic rotation with cert-manager.\nExternal systems such as Buoyant Cloud can be used to monitor cluster credentials and to send reminders if they are close to expiration.\nHow does Linkerd’s mTLS implementation work? The Linkerd control plane contains a certificate authority (CA) called identity. This CA issues TLS certificates to each Linkerd data plane proxy. Each certificate is bound to the Kubernetes ServiceAccount identity of the containing pod. These TLS certificates expire after 24 hours and are automatically rotated. The proxies use these certificates to encrypt and authenticate TCP traffic to …","description":"Linkerd automatically enables mutual Transport Layer Security (TLS) for all communication between meshed applications.","keywords":null,"section":"2.16","title":"Automatic mTLS","uri":"https://travisbeckham.github.io/2.16/features/automatic-mtls/"},{"ancestors":["Docs"],"content":"As a complement to the Linkerd feature docs and the Linkerd reference docs, we’ve provided guides and examples of common tasks that you may need to perform when using Linkerd.\nCommon Linkerd tasks Adding non-Kubernetes workloads to your mesh Adding your services to Linkerd In order for your services to take advantage of Linkerd, they also need to be *meshed* by injecting Linkerd\u0026#39;s data plane proxy into their pods. Automatic Multicluster Failover Use the Linkerd Failover extension to failover between clusters. Automatically Rotating Control Plane TLS Credentials Use cert-manager to automatically rotate control plane TLS credentials. Automatically Rotating Webhook TLS Credentials Use cert-manager to automatically rotate webhook TLS credentials. Bringing your own Prometheus Use an existing Prometheus instance with Linkerd. Circuit Breakers Protect against service failures using circuit breakers Configuring Dynamic Request Routing Configuring HTTPRoute resources to perform dynamic request routing. Configuring Per-Route Authorization Policy Fine-grained authorization policies can be configured for individual HTTP routes. Configuring Proxy Concurrency Limit the Linkerd proxy\u0026#39;s CPU usage. Configuring Proxy Discovery Cache Changing proxy discover cache timeouts when using slow clients. Configuring Retries Configure Linkerd to automatically retry failing requests. Configuring Timeouts Configure Linkerd to automatically fail requests that take too long. Control Plane Debug Endpoints Linkerd\u0026#39;s control plane components provide debug endpoints. Customizing Linkerd\u0026#39;s Configuration with Kustomize Use Kustomize to modify Linkerd\u0026#39;s configuration in a programmatic way. Debugging 502s Determine why Linkerd is returning 502 responses. Debugging gRPC applications with request tracing Follow a long-form example of debugging a failing gRPC application using live request tracing. Debugging HTTP applications with per-route metrics Follow a long-form example of debugging a failing HTTP application using per-route metrics. Distributed tracing with Linkerd Use Linkerd to help instrument your application with distributed tracing. Exporting Metrics Integrate Linkerd\u0026#39;s metrics with your existing metrics infrastructure. Exposing the Dashboard Make it easy for others to access Linkerd and Grafana dashboards without the CLI. Generating your own mTLS root certificates Generate your own mTLS root certificate instead of letting Linkerd do it for you. Getting Per-Route Metrics Configure per-route metrics for your application. Getting started with Linkerd SMI extension Use Linkerd SMI extension to work with Service Mesh Interface(SMI) resources. Graceful Pod Shutdown Gracefully handle pod shutdown signal. Grafana Grafana install instructions and how to link it with the Linkerd Dashboard Handling ingress traffic Linkerd can work alongside your ingress controller of choice. Injecting Faults Practice chaos engineering by injecting faults into services with Linkerd. Installing Linkerd …","description":"","keywords":null,"section":"2.16","title":"Tasks","uri":"https://travisbeckham.github.io/2.16/tasks/"},{"ancestors":["Docs"],"content":" Architecture Deep dive into the architecture of Linkerd. Authorization Policy Reference guide to Linkerd\u0026#39;s policy resources. Circuit Breaking How Linkerd implements circuit breaking. CLI Reference documentation for all the CLI commands. Cluster Configuration Configuration settings unique to providers and install methods. Extensions List List of Linkerd extensions that can be added to the installation for additional functionality ExternalWorkload HTTPRoute Reference guide to HTTPRoute resources. IPTables Reference A table with all of the chains and associated rules Multi-cluster communication Multi-cluster communication Proxy Configuration Linkerd provides a set of annotations that can be used to override the data plane proxy\u0026#39;s configuration. Proxy Log Level Syntax of the proxy log level. Proxy Metrics The Linkerd proxy natively exports Prometheus metrics for all incoming and outgoing traffic. Retries How Linkerd implements retries. Service Profiles Details on the specification and what is possible with service profiles. Supported Kubernetes Versions Reference documentation for which Linkerd version supports which Kubernetes version Timeouts How Linkerd implements timeouts. ","description":"","keywords":null,"section":"2.16","title":"Reference","uri":"https://travisbeckham.github.io/2.16/reference/"},{"ancestors":["Docs","Features"],"content":"For reasons of simplicity, Linkerd does not provide its own ingress controller. Instead, Linkerd is designed to work alongside your ingress controller of choice.\nSee the Using Ingress with Linkerd Guide for examples of how to get it all working together.\n","description":"Linkerd can work alongside your ingress controller of choice.","keywords":null,"section":"2.16","title":"Ingress","uri":"https://travisbeckham.github.io/2.16/features/ingress/"},{"ancestors":["Docs","Features"],"content":"One of Linkerd’s most powerful features is its extensive set of tooling around observability—the measuring and reporting of observed behavior in meshed applications. While Linkerd doesn’t have insight directly into the internals of service code, it has tremendous insight into the external behavior of service code.\nTo gain access to Linkerd’s observability features you only need to install the Viz extension:\nlinkerd viz install | kubectl apply -f - Linkerd’s telemetry and monitoring features function automatically, without requiring any work on the part of the developer. These features include:\nRecording of top-line (“golden”) metrics (request volume, success rate, and latency distributions) for HTTP, HTTP/2, and gRPC traffic. Recording of TCP-level metrics (bytes in/out, etc) for other TCP traffic. Reporting metrics per service, per caller/callee pair, or per route/path (with Service Profiles). Generating topology graphs that display the runtime relationship between services. Live, on-demand request sampling. This data can be consumed in several ways:\nThrough the Linkerd CLI, e.g. with linkerd viz stat and linkerd viz routes. Through the Linkerd dashboard, and pre-built Grafana dashboards. Directly from Linkerd’s built-in Prometheus instance Golden metrics Success Rate This is the percentage of successful requests during a time window (1 minute by default).\nIn the output of the command linkerd viz routes -o wide, this metric is split into EFFECTIVE_SUCCESS and ACTUAL_SUCCESS. For routes configured with retries, the former calculates the percentage of success after retries (as perceived by the client-side), and the latter before retries (which can expose potential problems with the service).\nTraffic (Requests Per Second) This gives an overview of how much demand is placed on the service/route. As with success rates, linkerd viz routes -o wide splits this metric into EFFECTIVE_RPS and ACTUAL_RPS, corresponding to rates after and before retries respectively.\nLatencies Times taken to service requests per service/route are split into 50th, 95th and 99th percentiles. Lower percentiles give you an overview of the average performance of the system, while tail percentiles help catch outlier behavior.\nLifespan of Linkerd metrics Linkerd is not designed as a long-term historical metrics store. While Linkerd’s Viz extension does include a Prometheus instance, this instance expires metrics at a short, fixed interval (currently 6 hours).\nRather, Linkerd is designed to supplement your existing metrics store. If Linkerd’s metrics are valuable, you should export them into your existing historical metrics store.\nSee Exporting Metrics for more.\n","description":"Linkerd automatically collects metrics from all services that send traffic through it.","keywords":null,"section":"2.16","title":"Telemetry and Monitoring","uri":"https://travisbeckham.github.io/2.16/features/telemetry/"},{"ancestors":["Docs","Features"],"content":"For HTTP, HTTP/2, and gRPC connections, Linkerd automatically load balances requests across all destination endpoints without any configuration required. (For TCP connections, Linkerd will balance connections.)\nLinkerd uses an algorithm called EWMA, or exponentially weighted moving average, to automatically send requests to the fastest endpoints. This load balancing can improve end-to-end latencies.\nService discovery For destinations that are not in Kubernetes, Linkerd will balance across endpoints provided by DNS.\nFor destinations that are in Kubernetes, Linkerd will look up the IP address in the Kubernetes API. If the IP address corresponds to a Service, Linkerd will load balance across the endpoints of that Service and apply any policy from that Service’s Service Profile. On the other hand, if the IP address corresponds to a Pod, Linkerd will not perform any load balancing or apply any Service Profiles.\nNote If working with headless services, endpoints of the service cannot be retrieved. Therefore, Linkerd will not perform load balancing and instead route only to the target IP address. Load balancing gRPC Linkerd’s load balancing is particularly useful for gRPC (or HTTP/2) services in Kubernetes, for which Kubernetes’s default load balancing is not effective.\n","description":"Linkerd automatically load balances requests across all destination endpoints on HTTP, HTTP/2, and gRPC connections.","keywords":null,"section":"2.16","title":"Load Balancing","uri":"https://travisbeckham.github.io/2.16/features/load-balancing/"},{"ancestors":["Docs"],"content":"Linkerd is generally robust, but things can always go wrong! You’ll find information here about the most common things that cause people trouble.\nWhen in Doubt, Start With linkerd check Whenever you see anything that looks unusual about your mesh, always start with linkerd check. It will check a long series of things that have caused trouble for others and make sure that your configuration is sane, and it will point you to help for any problems it finds. It’s hard to overstate how useful this command is.\nCommon Errors Failfast Failfast means that no endpoints are available. HTTP 502 Errors HTTP 502 means connection errors between proxies. HTTP 503 and 504 Errors HTTP 503 and 504 mean overloaded workloads. Protocol Detection Errors Protocol detection errors indicate that Linkerd doesn\u0026#39;t understand the protocol in use. ","description":"","keywords":null,"section":"2.16","title":"Common Errors","uri":"https://travisbeckham.github.io/2.16/common-errors/"},{"ancestors":["Docs","Tasks"],"content":"In this guide, we’ll walk you through an example of mesh expansion: setting up and configuring an example non-Kubernetes workload and adding it to your Linkerd mesh.\nOverall flow In this guide, we’ll take you through how to:\nInstall the Linkerd proxy onto a virtual or physical machine outside the Kubernetes cluster. Configure network rules so traffic is routed through the proxy. Register the external workload in the mesh. Exercise traffic patterns and apply authorization policies that affect the external workload. We’ll be using SPIRE as our identity mechanism to generate a workload identity.\nPrerequisites You will need:\nA functioning Linkerd installation and its trust anchor. A cluster that you have elevated privileges to. For local development, you can use kind or k3d. A physical or virtual machine. NET_CAP privileges on the machine, so iptables rules can be modified. IP connectivity from the machine to every pod in the mesh. A working DNS setup such that the machine is able to resolve DNS names for in-cluster Kubernetes workloads. Getting the current trust anchor and key To be able to use mutual TLS across cluster boundaries, the off-cluster machine and the cluster need to have a shared trust anchor. For the purposes of this tutorial, we will assume that you have access to the trust anchor certificate and secret key for your Linkerd deployment and placed it in files called ca.key and ca.crt.\nInstall SPIRE on your machine Linkerd’s proxies normally obtain TLS certificates from the identity component of Linkerd’s control plane. In order to attest their identity, they use the Kubernetes Service Account token that is provided to each Pod.\nSince our external workload lives outside of Kubernetes, the concept of Service Account tokens does not exist. Instead, we turn to the SPIFFE framework and its SPIRE implementation to create identities for off-cluster resources. Thus, for mesh expansion, we configure the Linkerd proxy to obtain its certificates directly from SPIRE instead of the Linkerd’s identity service. The magic of SPIFFE is that these certificates are compatible with those generated by Linkerd on the cluster.\nIn production, you may already have your own identity infrastructure built on top of SPIFFE that can be used by the proxies on external machines. For this tutorial however, we can take you through installing and setting up a minimal SPIRE environment on your machine. To begin with you need to install SPIRE by downloading it from the SPIRE GitHub releases page. For example:\nwget https://github.com/spiffe/SPIRE/releases/download/v1.8.2/SPIRE-1.8.2-linux-amd64-musl.tar.gz tar zvxf SPIRE-1.8.2-linux-amd64-musl.tar.gz cp -r SPIRE-1.8.2/. /opt/SPIRE/ Then you need to configure the SPIRE server on your machine:\ncat \u0026gt;/opt/SPIRE/server.cfg \u0026lt;\u0026lt;EOL server { bind_address = \u0026#34;127.0.0.1\u0026#34; bind_port = \u0026#34;8081\u0026#34; trust_domain = \u0026#34;root.linkerd.cluster.local\u0026#34; data_dir = \u0026#34;/opt/SPIRE/data/server\u0026#34; log_level = \u0026#34;DEBUG\u0026#34; ca_ttl = \u0026#34;168h\u0026#34; default_x509_svid_ttl = \u0026#34;48h\u0026#34; } …","description":"","keywords":null,"section":"2.16","title":"Adding non-Kubernetes workloads to your mesh","uri":"https://travisbeckham.github.io/2.16/tasks/adding-non-kubernetes-workloads/"},{"ancestors":["Docs","Tasks"],"content":"Adding Linkerd’s control plane to your cluster doesn’t change anything about your application. In order for your services to take advantage of Linkerd, they need to be meshed, by injecting Linkerd’s data plane proxy into their pods.\nFor most applications, meshing a service is as simple as adding a Kubernetes annotation and restarting the service. However, services that communicate using certain non-HTTP protocols (including MySQL, SMTP, Memcache, and others) may need a little configuration.\nRead on for more!\nMeshing a service with annotations Meshing a Kubernetes resource is typically done by annotating the resource (or its namespace) with the linkerd.io/inject: enabled Kubernetes annotation. This annotation triggers automatic proxy injection when the resources are created or updated. (See the proxy injection page for more on how this works.)\nFor convenience, Linkerd provides a linkerd inject text transform command will add this annotation to a given Kubernetes manifest. Of course, these annotations can be set by any other mechanism.\nNote Adding the annotation to existing pods does not automatically mesh them. For existing pods, after adding the annotation you will also need to recreate or update the resource (e.g. by using kubectl rollout restart to perform a rolling update) to trigger proxy injection. Examples To add Linkerd’s data plane proxies to a service defined in a Kubernetes manifest, you can use linkerd inject to add the annotations before applying the manifest to Kubernetes.\nYou can transform an existing deployment.yml file to add annotations in the correct places and apply it to the cluster:\ncat deployment.yml | linkerd inject - | kubectl apply -f - You can mesh every deployment in a namespace by combining this with kubectl get:\nkubectl get -n NAMESPACE deploy -o yaml | linkerd inject - | kubectl apply -f - Verifying the data plane pods have been injected To verify that your services have been added to the mesh, you can query Kubernetes for the list of containers in the pods and ensure that the proxy is listed:\nkubectl -n NAMESPACE get po -o jsonpath=\u0026#39;{.items[0].spec.containers[*].name}\u0026#39; If everything was successful, you’ll see linkerd-proxy in the output, e.g.:\nlinkerd-proxy CONTAINER Handling MySQL, SMTP, and other non-HTTP protocols Linkerd’s protocol detection works by looking at the first few bytes of client data to determine the protocol of the connection. Some protocols, such as MySQL and SMTP, don’t send these bytes. If your application uses these protocols without TLSing them, you may require additional configuration to avoid a 10-second delay when establishing connections.\nSee Configuring protocol detection for details.\nMore reading For more information on how the inject command works and all of the parameters that can be set, see the linkerd inject reference page.\nFor details on how autoinjection works, see the proxy injection page.\n","description":"In order for your services to take advantage of Linkerd, they also need to be *meshed* by injecting Linkerd's data plane proxy into their pods.","keywords":null,"section":"2.16","title":"Adding your services to Linkerd","uri":"https://travisbeckham.github.io/2.16/tasks/adding-your-service/"},{"ancestors":["Docs","Reference"],"content":"At a high level, Linkerd consists of a control plane and a data plane.\nThe control plane is a set of services that and provide control over Linkerd as a whole.\nThe data plane consists of transparent micro-proxies that run “next” to each service instance, as sidecar containers in the pods. These proxies automatically handle all TCP traffic to and from the service, and communicate with the control plane for configuration.\nLinkerd also provides a CLI that can be used to interact with the control and data planes.\nLinkerd’s architecture CLI The Linkerd CLI is typically run outside of the cluster (e.g. on your local machine) and is used to interact with the Linkerd.\nControl plane The Linkerd control plane is a set of services that run in a dedicated Kubernetes namespace (linkerd by default). The control plane has several components, enumerated below.\nThe destination service The destination service is used by the data plane proxies to determine various aspects of their behavior. It is used to fetch service discovery information (i.e. where to send a particular request and the TLS identity expected on the other end); to fetch policy information about which types of requests are allowed; to fetch service profile information used to inform per-route metrics, retries, and timeouts; and more.\nThe identity service The identity service acts as a TLS Certificate Authority that accepts CSRs from proxies and returns signed certificates. These certificates are issued at proxy initialization time and are used for proxy-to-proxy connections to implement mTLS.\nThe proxy injector The proxy injector is a Kubernetes admission controller that receives a webhook request every time a pod is created. This injector inspects resources for a Linkerd-specific annotation (linkerd.io/inject: enabled). When that annotation exists, the injector mutates the pod’s specification and adds the proxy-init and linkerd-proxy containers to the pod, along with the relevant start-time configuration.\nData plane The Linkerd data plane comprises ultralight micro-proxies which are deployed as sidecar containers inside application pods. These proxies transparently intercept TCP connections to and from each pod, thanks to iptables rules put in place by the linkerd-init (or, alternatively, by Linkerd’s CNI plugin).\nProxy The Linkerd2-proxy is an ultralight, transparent micro-proxy written in Rust. Linkerd2-proxy is designed specifically for the service mesh use case and is not designed as a general-purpose proxy.\nThe proxy’s features include:\nTransparent, zero-config proxying for HTTP, HTTP/2, and arbitrary TCP protocols. Automatic Prometheus metrics export for HTTP and TCP traffic. Transparent, zero-config WebSocket proxying. Automatic, latency-aware, layer-7 load balancing. Automatic layer-4 load balancing for non-HTTP traffic. Automatic TLS. An on-demand diagnostic tap API. And lots more. The proxy supports service discovery via DNS and the destination gRPC API.\nYou can read more about these …","description":"Deep dive into the architecture of Linkerd.","keywords":null,"section":"2.16","title":"Architecture","uri":"https://travisbeckham.github.io/2.16/reference/architecture/"},{"ancestors":["Docs","Features"],"content":"Linkerd’s authorization policy allows you to control which types of traffic are allowed to meshed pods. For example, you can restrict communication to a particular service (or HTTP route on a service) to only come from certain other services; you can enforce that mTLS must be used on a certain port; and so on.\nNote Linkerd can only enforce policy on meshed pods, i.e. pods where the Linkerd proxy has been injected. If policy is a strict requirement, you should pair the usage of these features with HA mode, which enforces that the proxy must be present when pods start up. Policy overview By default Linkerd allows all traffic to transit the mesh, and uses a variety of mechanisms, including retries and load balancing, to ensure that requests are delivered successfully.\nSometimes, however, we want to restrict which types of traffic are allowed. Linkerd’s policy features allow you to deny access to resources unless certain conditions are met, including the TLS identity of the client.\nLinkerd’s policy is configured using two mechanisms:\nA set of default policies, which can be set at the cluster, namespace, workload, and pod level through Kubernetes annotations. A set of CRDs that specify fine-grained policy for specific ports, routes, workloads, etc. These mechanisms work in conjunction. For example, a default cluster-wide policy of deny would prohibit any traffic to any meshed pod; traffic would then need to be explicitly allowed through the use of CRDs.\nDefault policies The config.linkerd.io/default-inbound-policy annotation can be set at a namespace, workload, and pod level, and will determine the default traffic policy at that point in the hierarchy. Valid default policies include:\nall-unauthenticated: allow all requests. This is the default. all-authenticated: allow requests from meshed clients only. cluster-authenticated: allow requests from meshed clients in the same cluster. deny: deny all requests. audit: Same as all-unauthenticated but requests get flagged in logs and metrics. As well as several other default policies—see the Policy reference for more.\nEvery cluster has a cluster-wide default policy (by default, all-unauthenticated), set at install time. Annotations that are present at the workload or namespace level at pod creation time can override that value to determine the default policy for that pod. (Note that the default policy is fixed at proxy initialization time, and thus, after a pod is created, changing the annotation will not change the default policy for that pod.)\nFine-grained policies For finer-grained policy that applies to specific ports, routes, or more, Linkerd uses a set of CRDs. In contrast to default policy annotations, these policy CRDs can be changed dynamically and policy behavior will be updated on the fly.\nTwo policy CRDs represent “targets” for policy: subsets of traffic over which policy can be applied.\nServer: all traffic to a port, for a set of pods in a namespace HTTPRoute: a subset of HTTP requests for a …","description":"Linkerd can restrict which types of traffic are allowed between meshed services.","keywords":null,"section":"2.16","title":"Authorization Policy","uri":"https://travisbeckham.github.io/2.16/features/server-policy/"},{"ancestors":["Docs","Reference"],"content":"Linkerd’s authorization policy allows you to control which types of traffic are allowed to meshed pods. See the Authorization Policy feature description for more information on what this means.\nLinkerd’s policy is configured using two mechanisms:\nA set of default policies, which can be set at the cluster, namespace, and workload level through Kubernetes annotations. A set of CRDs that specify fine-grained policy for specific ports, routes, workloads, etc. Default policies During a Linkerd install, the proxy.defaultInboundPolicy field is used to specify the cluster-wide default policy. This field can be one of the following:\nall-unauthenticated: allow all traffic. This is the default. all-authenticated: allow traffic from meshed clients in the same or from a different cluster (with multi-cluster). cluster-authenticated: allow traffic from meshed clients in the same cluster. cluster-unauthenticated: allow traffic from both meshed and non-meshed clients in the same cluster. deny: all traffic are denied. audit: Same as all-unauthenticated but requests get flagged in logs and metrics. This cluster-wide default can be overridden for specific resources by setting the annotation config.linkerd.io/default-inbound-policy on either a pod spec or its namespace.\nDynamic policy resources For dynamic control of policy, and for finer-grained policy than what the default polices allow, Linkerd provides a set of CRDs which control traffic policy in the cluster: Server, HTTPRoute, ServerAuthorization, AuthorizationPolicy, MeshTLSAuthentication, and NetworkAuthentication.\nThe general pattern for authorization is:\nA Server describes a set of pods, and a single port on those pods. Optionally, an HTTPRoute references that Server and describes a subset of HTTP traffic to it. A MeshTLSAuthentication or NetworkAuthentication decribes who is allowed access. An AuthorizationPolicy references the HTTPRoute or Server (the thing to be authorized) and the MeshTLSAuthentication or NetworkAuthentication (the clients that have authorization). Server A Server selects a port on a set of pods in the same namespace as the server. It typically selects a single port on a pod, though it may select multiple ports when referring to the port by name (e.g. admin-http). While the Server resource is similar to a Kubernetes Service, it has the added restriction that multiple Server instances must not overlap: they must not select the same pod/port pairs. Linkerd ships with an admission controller that prevents overlapping Servers from being created.\nNote When a Server resource is present, all traffic to the port on its pods will be denied unless explicitly authorized or audit mode is enabled (with accessPolicy:audit). Thus, Servers are typically paired with e.g. an AuthorizationPolicy that references the Server, or that reference an HTTPRoute that in turn references the Server. Server Spec A Server spec may contain the following top level fields:\nfield value accessPolicy accessPolicy declares …","description":"Reference guide to Linkerd's policy resources.","keywords":null,"section":"2.16","title":"Authorization Policy","uri":"https://travisbeckham.github.io/2.16/reference/authorization-policy/"},{"ancestors":["Docs","Reference","CLI"],"content":" List authorizations for a resource.\nCheck out the Authorization Policy and Restricting Access to Services documentation for all the details about authorization policy in Linkerd.\nFlags Flag Usage --namespace -n Namespace of resource ","description":"","keywords":null,"section":"2.16","title":"authz","uri":"https://travisbeckham.github.io/2.16/reference/cli/authz/"},{"ancestors":["Docs","Tasks"],"content":"The Linkerd Failover extension is a controller which automatically shifts traffic from a primary service to one or more fallback services whenever the primary becomes unavailable. This can help add resiliency when you have a service which is replicated in multiple clusters. If the local service is unavailable, the failover controller can shift that traffic to the backup cluster.\nLet’s see a simple example of how to use this extension by installing the Emojivoto application on two Kubernetes clusters and simulating a failure in one cluster. We will see the failover controller shift traffic to the other cluster to ensure the service remains available.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Prerequisites You will need two clusters with Linkerd installed and for the clusters to be linked together with the multicluster extension. Follow the steps in the multicluster guide to generate a shared trust root, install Linkerd, Linkerd Viz, and Linkerd Multicluster, and to link the clusters together. For the remainder of this guide, we will assume the cluster context names are “east” and “west” respectively. Please substitute your cluster context names where appropriate.\nInstalling the Failover Extension Failovers are described using SMI TrafficSplit resources. We install the Linkerd SMI extension and the Linkerd Failover extension. These can be installed in both clusters, but since we’ll only be initiating failover from the “west” cluster in this example, we’ll only install them in that cluster:\n# Install linkerd-smi in west cluster \u0026gt; helm --kube-context=west repo add linkerd-smi https://linkerd.github.io/linkerd-smi \u0026gt; helm --kube-context=west repo up \u0026gt; helm --kube-context=west install linkerd-smi -n linkerd-smi --create-namespace linkerd-smi/linkerd-smi # Install linkerd-failover in west cluster \u0026gt; helm --kube-context=west repo add linkerd-edge https://helm.linkerd.io/edge \u0026gt; helm --kube-context=west repo up \u0026gt; helm --kube-context=west install linkerd-failover -n linkerd-failover --create-namespace --devel linkerd-edge/linkerd-failover Installing and Exporting Emojivoto We’ll now install the Emojivoto example application into both clusters:\n\u0026gt; linkerd --context=west inject https://run.linkerd.io/emojivoto.yml | kubectl --context=west apply -f - \u0026gt; linkerd --context=east inject https://run.linkerd.io/emojivoto.yml | kubectl --context=east apply -f - Next we’ll “export” the web-svc in the east cluster by setting the mirror.linkerd.io/exported=true label. This will instruct the multicluster extension to create a mirror service called web-svc-east in the west cluster, making the east Emojivoto application available in the west cluster:\n\u0026gt; kubectl --context=east -n emojivoto label svc/web-svc mirror.linkerd.io/exported=true \u0026gt; kubectl …","description":"Use the Linkerd Failover extension to failover between clusters.","keywords":null,"section":"2.16","title":"Automatic Multicluster Failover","uri":"https://travisbeckham.github.io/2.16/tasks/automatic-failover/"},{"ancestors":["Docs","Features"],"content":"Linkerd automatically adds the data plane proxy to pods when the linkerd.io/inject: enabled annotation is present on a namespace or any workloads, such as deployments or pods. This is known as “proxy injection”.\nSee Adding Your Service for a walkthrough of how to use this feature in practice.\nNote Proxy injection is also where proxy configuration happens. While it’s rarely necessary, you can configure proxy settings by setting additional Kubernetes annotations at the resource level prior to injection. See the full list of proxy configuration options. Details Proxy injection is implemented as a Kubernetes admission webhook. This means that the proxies are added to pods within the Kubernetes cluster itself, regardless of whether the pods are created by kubectl, a CI/CD system, or any other system.\nFor each pod, two containers are injected:\nlinkerd-init, a Kubernetes Init Container that configures iptables to automatically forward all incoming and outgoing TCP traffic through the proxy. (Note that this container is not injected if the Linkerd CNI Plugin has been enabled.) linkerd-proxy, the Linkerd data plane proxy itself. Note that simply adding the annotation to a resource with pre-existing pods will not automatically inject those pods. You will need to update the pods (e.g. with kubectl rollout restart etc.) for them to be injected. This is because Kubernetes does not call the webhook until it needs to update the underlying resources.\nExclusions At install time, Kubernetes is configured to avoid calling Linkerd’s proxy injector for resources in the kube-system and cert-manager namespaces. This is to prevent injection on components that are themselves required for Linkerd to function.\nThe injector will not run on components in these namespaces, regardless of any linkerd.io/inject annotations.\nOverriding injection Automatic injection can be disabled for a pod or deployment for which it would otherwise be enabled, by adding the linkerd.io/inject: disabled annotation.\nManual injection The linkerd inject CLI command is a text transform that, by default, simply adds the inject annotation to a given Kubernetes manifest.\nAlternatively, this command can also perform the full injection purely on the client side with the --manual flag. This was the default behavior prior to Linkerd 2.4; however, having injection to the cluster side makes it easier to ensure that the data plane is always present and configured correctly, regardless of how pods are deployed.\nSee the linkerd inject reference for more information.\n","description":"Linkerd will automatically inject the data plane proxy into your pods based annotations.","keywords":null,"section":"2.16","title":"Automatic Proxy Injection","uri":"https://travisbeckham.github.io/2.16/features/proxy-injection/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd’s automatic mTLS feature generates TLS certificates for proxies and automatically rotates them without user intervention. These certificates are derived from a trust anchor, which is shared across clusters, and an issuer certificate, which is specific to the cluster.\nWhile Linkerd automatically rotates the per-proxy TLS certificates, it does not rotate the issuer certificate. In this doc, we’ll describe how to set up automatic rotation of the issuer certificate and its corresponding private key using the cert-manager project.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Cert manager Cert-manager is a popular project for making TLS credentials from external sources available to Kubernetes clusters.\nCert-manager is very flexible. You can configure it to pull certificates from secrets managemenet solutions such as Vault. In this guide, we’ll focus on a self-sufficient setup: we will configure cert-manager to act as an on-cluster CA and have it re-issue Linkerd’s issuer certificate and private key on a periodic basis, derived from the trust anchor.\nCert manager as an on-cluster CA As a first step, install cert-manager on your cluster.\nNext, create the namespace that cert-manager will use to store its Linkerd-related resources. For simplicity, we suggest reusing the default Linkerd control plane namespace:\nkubectl create namespace linkerd Save the signing key pair as a Secret Next, using the step tool, create a signing key pair and store it in a Kubernetes Secret in the namespace created above:\nstep certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure \u0026amp;\u0026amp; kubectl create secret tls \\ linkerd-trust-anchor \\ --cert=ca.crt \\ --key=ca.key \\ --namespace=linkerd For a longer-lived trust anchor certificate, pass the --not-after argument to the step command with the desired value (e.g. --not-after=87600h).\nCreate an Issuer referencing the secret With the Secret in place, we can create a cert-manager “Issuer” resource that references it:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: linkerd-trust-anchor namespace: linkerd spec: ca: secretName: linkerd-trust-anchor EOF Create a Certificate resource referencing the Issuer Finally, we can create a cert-manager “Certificate” resource which uses this Issuer to generate the desired certificate:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: linkerd-identity-issuer namespace: linkerd spec: secretName: linkerd-identity-issuer duration: 48h renewBefore: 25h issuerRef: name: linkerd-trust-anchor kind: Issuer commonName: identity.linkerd.cluster.local dnsNames: - identity.linkerd.cluster.local isCA: true privateKey: algorithm: ECDSA usages: - cert sign - crl sign - …","description":"Use cert-manager to automatically rotate control plane TLS credentials.","keywords":null,"section":"2.16","title":"Automatically Rotating Control Plane TLS Credentials","uri":"https://travisbeckham.github.io/2.16/tasks/automatically-rotating-control-plane-tls-credentials/"},{"ancestors":["Docs","Tasks"],"content":"The Linkerd control plane contains several components, called webhooks, which are called directly by Kubernetes itself. The traffic from Kubernetes to the Linkerd webhooks is secured with TLS and therefore each of the webhooks requires a secret containing TLS credentials. These certificates are different from the ones that the Linkerd proxies use to secure pod-to-pod communication and use a completely separate trust chain. For more information on rotating the TLS credentials used by the Linkerd proxies, see Automatically Rotating Control Plane TLS Credentials.\nBy default, when Linkerd is installed with the Linkerd CLI or with the Linkerd Helm chart, TLS credentials are automatically generated for all of the webhooks. If these certificates expire or need to be regenerated for any reason, performing a Linkerd upgrade (using the Linkerd CLI or using Helm) will regenerate them.\nThis workflow is suitable for most users. However, if you need these webhook certificates to be rotated automatically on a regular basis, it is possible to use cert-manager to automatically manage them.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Install Cert manager As a first step, install cert-manager on your cluster and create the namespaces that cert-manager will use to store its webhook-related resources. For simplicity, we suggest using the default namespace linkerd uses:\n# control plane core kubectl create namespace linkerd kubectl label namespace linkerd \\ linkerd.io/is-control-plane=true \\ config.linkerd.io/admission-webhooks=disabled \\ linkerd.io/control-plane-ns=linkerd kubectl annotate namespace linkerd linkerd.io/inject=disabled # viz (ignore if not using the viz extension) kubectl create namespace linkerd-viz kubectl label namespace linkerd-viz linkerd.io/extension=viz # jaeger (ignore if not using the jaeger extension) kubectl create namespace linkerd-jaeger kubectl label namespace linkerd-jaeger linkerd.io/extension=jaeger Save the signing key pair as a Secret Next, we will use the step tool, to create a signing key pair which will be used to sign each of the webhook certificates:\nstep certificate create webhook.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure --san webhook.linkerd.cluster.local kubectl create secret tls webhook-issuer-tls --cert=ca.crt --key=ca.key --namespace=linkerd # ignore if not using the viz extension kubectl create secret tls webhook-issuer-tls --cert=ca.crt --key=ca.key --namespace=linkerd-viz # ignore if not using the jaeger extension kubectl create secret tls webhook-issuer-tls --cert=ca.crt --key=ca.key --namespace=linkerd-jaeger Create Issuers referencing the secrets With the Secrets in place, we can create cert-manager “Issuer” resources that reference them:\nkubectl apply -f - \u0026lt;\u0026lt;EOF …","description":"Use cert-manager to automatically rotate webhook TLS credentials.","keywords":null,"section":"2.16","title":"Automatically Rotating Webhook TLS Credentials","uri":"https://travisbeckham.github.io/2.16/tasks/automatically-rotating-webhook-tls-credentials/"},{"ancestors":["Docs","Tasks"],"content":"Even though the linkerd-viz extension comes with its own Prometheus instance, there can be cases where using an external instance makes more sense for various reasons.\nThis tutorial shows how to configure an external Prometheus instance to scrape both the control plane as well as the proxy’s metrics in a format that is consumable both by a user as well as Linkerd control plane components like web, etc.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. There are two important points to tackle here.\nConfiguring external Prometheus instance to get the Linkerd metrics. Configuring the linkerd-viz extension to use that Prometheus. Prometheus Scrape Configuration The following scrape configuration has to be applied to the external Prometheus instance.\nNote The below scrape configuration is a subset of the full linkerd-prometheus scrape configuration. Before applying, it is important to replace templated values (present in {{}}) with direct values for the below configuration to work.\n- job_name: \u0026#39;linkerd-controller\u0026#39; kubernetes_sd_configs: - role: pod namespaces: names: - \u0026#39;{{.Values.linkerdNamespace}}\u0026#39; - \u0026#39;{{.Values.namespace}}\u0026#39; relabel_configs: - source_labels: - __meta_kubernetes_pod_container_port_name action: keep regex: admin-http - source_labels: [__meta_kubernetes_pod_container_name] action: replace target_label: component - job_name: \u0026#39;linkerd-service-mirror\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_label_linkerd_io_control_plane_component - __meta_kubernetes_pod_container_port_name action: keep regex: linkerd-service-mirror;admin-http$ - source_labels: [__meta_kubernetes_pod_container_name] action: replace target_label: component - job_name: \u0026#39;linkerd-proxy\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: - __meta_kubernetes_pod_container_name - __meta_kubernetes_pod_container_port_name - __meta_kubernetes_pod_label_linkerd_io_control_plane_ns action: keep regex: ^{{default .Values.proxyContainerName \u0026#34;linkerd-proxy\u0026#34; .Values.proxyContainerName}};linkerd-admin;{{.Values.linkerdNamespace}}$ - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: pod # special case k8s\u0026#39; \u0026#34;job\u0026#34; label, to not interfere with prometheus\u0026#39; \u0026#34;job\u0026#34; # label # __meta_kubernetes_pod_label_linkerd_io_proxy_job=foo =\u0026gt; # k8s_job=foo - source_labels: [__meta_kubernetes_pod_label_linkerd_io_proxy_job] action: replace target_label: k8s_job # drop __meta_kubernetes_pod_label_linkerd_io_proxy_job - action: labeldrop regex: __meta_kubernetes_pod_label_linkerd_io_proxy_job # __meta_kubernetes_pod_label_linkerd_io_proxy_deployment=foo =\u0026gt; # deployment=foo - action: labelmap regex: …","description":"Use an existing Prometheus instance with Linkerd.","keywords":null,"section":"2.16","title":"Bringing your own Prometheus","uri":"https://travisbeckham.github.io/2.16/tasks/external-prometheus/"},{"ancestors":["Docs","Reference","CLI"],"content":" Check the Linkerd installation for potential problems.\nThe check command will perform a series of checks to validate that the linkerd CLI and control plane are configured correctly. If the command encounters a failure it will print additional information about the failure and exit with a non-zero exit code.\nTake a look at the troubleshooting documentation for a full list of all the possible checks, what they do and how to fix them.\nExamples # Check that the Linkerd control plane is up and running linkerd check # Check that the Linkerd control plane can be installed in the \u0026#34;test\u0026#34; namespace linkerd check --pre --linkerd-namespace test # Check that the Linkerd data plane proxies in the \u0026#34;app\u0026#34; namespace are up and running linkerd check --proxy --namespace app Example output $ linkerd check kubernetes-api -------------- √ can initialize the client √ can query the Kubernetes API kubernetes-version ------------------ √ is running the minimum Kubernetes API version linkerd-existence ----------------- √ control plane namespace exists √ controller pod is running √ can initialize the client √ can query the control plane API linkerd-api ----------- √ control plane pods are ready √ control plane self-check √ [kubernetes] control plane can talk to Kubernetes √ [prometheus] control plane can talk to Prometheus linkerd-service-profile ----------------------- √ no invalid service profiles linkerd-version --------------- √ can determine the latest version √ cli is up-to-date control-plane-version --------------------- √ control plane is up-to-date √ control plane and cli versions match Status check results are √ Flags Flag Usage --cli-version-override Used to override the version of the cli (mostly for testing) --crds Only run checks which determine if the Linkerd CRDs have been installed --expected-version Overrides the version used when checking if Linkerd is running the latest version (mostly for testing) --linkerd-cni-enabled When running pre-installation checks (–pre), assume the linkerd-cni plugin is already installed, and a NET_ADMIN check is not needed --namespace -n Namespace to use for –proxy checks (default: all namespaces) --output -o Output format. One of: table, json, short --pre Only run pre-installation checks, to determine if the control plane can be installed --proxy Only run data-plane checks, to determine if the data plane is healthy --wait Maximum allowed time for all tests to pass ","description":"","keywords":null,"section":"2.16","title":"check","uri":"https://travisbeckham.github.io/2.16/reference/cli/check/"},{"ancestors":["Docs","Tasks"],"content":"Circuit breaking is a powerful feature where Linkerd will temporarily stop routing requests to an endpoint if that endpoint is deemed to be unhealthy, instead routing that request to other replicas in the Service.\nIn this tutoral, we’ll see how to enable circuit breaking on a Service to improve client success rate when a backend replica is unhealthy.\nSee the reference documentation for more details on how Linkerd implements circuit breaking.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Prerequisites To use this guide, you’ll need a Kubernetes cluster running:\nLinkerd and Linkerd-Viz. If you haven’t installed these yet, follow the Installing Linkerd Guide. Set up the demo Remember those puzzles where one guard always tells the truth and one guard always lies? This demo involves one pod (named good) which always returns an HTTP 200 and one pod (named bad) which always returns an HTTP 500. We’ll also create a load generator to send traffic to a Service which includes these two pods.\nFor load generation we’ll use Slow-Cooker and for the backend pods we’ll use BB.\nTo add these components to your cluster and include them in the Linkerd data plane, run:\ncat \u0026lt;\u0026lt;EOF | linkerd inject - | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: circuit-breaking-demo --- apiVersion: apps/v1 kind: Deployment metadata: name: good namespace: circuit-breaking-demo spec: replicas: 1 selector: matchLabels: class: good template: metadata: labels: class: good app: bb spec: containers: - name: terminus image: buoyantio/bb:v0.0.6 args: - terminus - \u0026#34;--h1-server-port=8080\u0026#34; ports: - containerPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: bad namespace: circuit-breaking-demo spec: replicas: 1 selector: matchLabels: class: bad template: metadata: labels: class: bad app: bb spec: containers: - name: terminus image: buoyantio/bb:v0.0.6 args: - terminus - \u0026#34;--h1-server-port=8080\u0026#34; - \u0026#34;--percent-failure=100\u0026#34; ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: bb namespace: circuit-breaking-demo spec: ports: - name: http port: 8080 targetPort: 8080 selector: app: bb --- apiVersion: apps/v1 kind: Deployment metadata: name: slow-cooker namespace: circuit-breaking-demo spec: replicas: 1 selector: matchLabels: app: slow-cooker template: metadata: labels: app: slow-cooker spec: containers: - args: - -c - | sleep 5 # wait for pods to start /slow_cooker/slow_cooker --qps 10 http://bb:8080 command: - /bin/sh image: buoyantio/slow_cooker:1.3.0 name: slow-cooker EOF We can now look at the success rate of the good and bad pods:\n\u0026gt; linkerd viz -n circuit-breaking-demo stat deploy NAME MESHED SUCCESS RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 TCP_CONN bad 1/1 6.43% 4.7rps 1ms 1ms 4ms 2 good 1/1 100.00% 5.9rps 1ms 1ms 1ms 3 …","description":"Protect against service failures using circuit breakers","keywords":null,"section":"2.16","title":"Circuit Breakers","uri":"https://travisbeckham.github.io/2.16/tasks/circuit-breakers/"},{"ancestors":["Docs","Reference"],"content":"Circuit breaking is a pattern for improving the reliability of distributed applications. In circuit breaking, an application which makes network calls to remote backends monitors whether those calls succeed or fail, in an attempt to determine whether that backend is in a failed state. If a given backend is believed to be in a failed state, its circuit breaker is “tripped”, and no subsequent requests are sent to that backend until it is determined to have returned to normal.\nThe Linkerd proxy is capable of performing endpoint-level circuit breaking on HTTP requests using a configurable failure accrual strategy. This means that the Linkerd proxy performs circuit breaking at the level of individual endpoints in a load balancer (i.e., each Pod in a given Service), and failures are tracked at the level of HTTP response status codes.\nCircuit breaking is a client-side behavior, and is therefore performed by the outbound side of the Linkerd proxy.1 Outbound proxies implement circuit breaking in the load balancer, by marking failing endpoints as unavailable. When an endpoint is unavailable, the load balancer will not select it when determining where to send a given request. This means that if only some endpoints have tripped their circuit breakers, the proxy will simply not select those endpoints while they are in a failed state. When all endpoints in a load balancer are unavailable, requests may be failed with 503 Service Unavailable errors, or, if the Service is one of multiple backendRefs in an HTTPRoute, the entire backend Service will be considered unavailable and a different backend may be selected.\nThe outbound_http_balancer_endpoints gauge metric reports the number of “ready” and “pending” endpoints in a load balancer, with the “pending” number including endpoints made unavailable by failure accrual.\nFailure Accrual Policies A failure accrual policy determines how failures are tracked for endpoints, and what criteria result in an endpoint becoming unavailable (“tripping the circuit breaker”). Currently, the Linkerd proxy implements one failure accrual policy, consecutive failures. Additional failure accrual policies may be added in the future.\nNote HTTP responses are classified as failures if their status code is a [5xx server error]. Future Linkerd releases may add support for configuring what status codes are classified as failures. Consecutive Failures In this failure accrual policy, an endpoint is marked as failing after a configurable number of failures occur consecutively (i.e., without any successes). For example, if the maximum number of failures is 7, the endpoint is made unavailable once 7 failures occur in a row with no successes.\nProbation and Backoffs Once a failure accrual policy makes an endpoint unavailble, the circuit breaker will attempt to determine whether the endpoint is still in a failing state, and transition it back to available if it has recovered. This process is called probation. When an endpoint enters probation, it is …","description":"How Linkerd implements circuit breaking.","keywords":null,"section":"2.16","title":"Circuit Breaking","uri":"https://travisbeckham.github.io/2.16/reference/circuit-breaking/"},{"ancestors":["Docs","Reference"],"content":"The Linkerd CLI is the primary way to interact with Linkerd. It can install the control plane to your cluster, add the proxy to your service and provide detailed metrics for how your service is performing.\nAs reference, check out the commands below:\nCommand Description authz List authorizations for a resource check Check the Linkerd installation for potential problems completion Output shell completion code for the specified shell (bash, zsh or fish) diagnostics Commands used to diagnose Linkerd components identity Display the certificate(s) of one or more selected pod(s) inject Add the Linkerd proxy to a Kubernetes config install Output Kubernetes configs to install Linkerd install-cni Output Kubernetes configs to install Linkerd CNI jaeger jaeger manages the jaeger extension of Linkerd service mesh multicluster Manages the multicluster setup for Linkerd profile Output service profile config for Kubernetes prune Output extraneous Kubernetes resources in the linkerd control plane uninject Remove the Linkerd proxy from a Kubernetes config uninstall Output Kubernetes resources to uninstall Linkerd control plane upgrade Output Kubernetes configs to upgrade an existing Linkerd control plane version Print the client and server version information viz viz manages the linkerd-viz extension of Linkerd service mesh Global flags The following flags are available for all linkerd CLI commands:\nFlag Usage --api-addr Override kubeconfig and communicate directly with the control plane at host:port (mostly for testing) --context Name of the kubeconfig context to use --help -h help for linkerd --kubeconfig Path to the kubeconfig file to use for CLI requests --linkerd-namespace -l Namespace in which Linkerd is installed [$LINKERD_NAMESPACE] --verbose Turn on debug logging --as Username to impersonate for Kubernetes operations ","description":"Reference documentation for all the CLI commands.","keywords":null,"section":"2.16","title":"CLI","uri":"https://travisbeckham.github.io/2.16/reference/cli/"},{"ancestors":["Docs","Reference"],"content":" GKE Private Clusters If you are using a private GKE cluster, you are required to create a firewall rule that allows the GKE operated api-server to communicate with the Linkerd control plane. This makes it possible for features such as automatic proxy injection to receive requests directly from the api-server.\nIn this example, we will use gcloud to simplify the creation of the said firewall rule.\nSetup:\nCLUSTER_NAME=your-cluster-name gcloud config set compute/zone your-zone-or-region Get the cluster MASTER_IPV4_CIDR:\nMASTER_IPV4_CIDR=$(gcloud container clusters describe $CLUSTER_NAME \\ | grep \u0026#34;masterIpv4CidrBlock: \u0026#34; \\ | awk \u0026#39;{print $2}\u0026#39;) Get the cluster NETWORK:\nNETWORK=$(gcloud container clusters describe $CLUSTER_NAME \\ | grep \u0026#34;^network: \u0026#34; \\ | awk \u0026#39;{print $2}\u0026#39;) Get the cluster auto-generated NETWORK_TARGET_TAG:\nNETWORK_TARGET_TAG=$(gcloud compute firewall-rules list \\ --filter network=$NETWORK --format json \\ | jq \u0026#34;.[] | select(.name | contains(\\\u0026#34;$CLUSTER_NAME\\\u0026#34;))\u0026#34; \\ | jq -r \u0026#39;.targetTags[0]\u0026#39; | head -1) The format of the network tag should be something like gke-cluster-name-xxxx-node.\nVerify the values:\necho $MASTER_IPV4_CIDR $NETWORK $NETWORK_TARGET_TAG # example output 10.0.0.0/28 foo-network gke-foo-cluster-c1ecba83-node Create the firewall rules for proxy-injector, policy-validator and tap:\ngcloud compute firewall-rules create gke-to-linkerd-control-plane \\ --network \u0026#34;$NETWORK\u0026#34; \\ --allow \u0026#34;tcp:8443,tcp:8089,tcp:9443\u0026#34; \\ --source-ranges \u0026#34;$MASTER_IPV4_CIDR\u0026#34; \\ --target-tags \u0026#34;$NETWORK_TARGET_TAG\u0026#34; \\ --priority 1000 \\ --description \u0026#34;Allow traffic on ports 8443, 8089, 9443 for linkerd control-plane components\u0026#34; Finally, verify that the firewall is created:\ngcloud compute firewall-rules describe gke-to-linkerd-control-plane Cilium Turn Off Socket-Level Load Balancing Cilium can be configured to replace kube-proxy functionality through eBPF. When running in kube-proxy replacement mode, connections to a ClusterIP service will be established directly to the service’s backend at the socket level (i.e. during TCP connection establishment). Linkerd relies on ClusterIPs being present on packets in order to do service discovery.\nWhen packets do not contain a ClusterIP address, Linkerd will instead forward directly to the pod endpoint that was selected by Cilium. Consequentially, while mTLS and telemetry will still function correctly, features such as peak EWMA load balancing, and dynamic request routing may not work as expected.\nThis behavior can be turned off in Cilium by turning off socket-level load balancing for pods through the CLI option --config bpf-lb-sock-hostns-only=true, or through the Helm value socketLB.hostNamespaceOnly=true.\nDisable Exclusive Mode If you’re using Cilium as your CNI and then want to install linkerd-cni on top of it, make sure you install Cilium with the option cni.exclusive=false. This avoids Cilium taking ownership over the CNI configurations directory. Other CNI plugins like linkerd-cni install themselves and operate in chain …","description":"Configuration settings unique to providers and install methods.","keywords":null,"section":"2.16","title":"Cluster Configuration","uri":"https://travisbeckham.github.io/2.16/reference/cluster-configuration/"},{"ancestors":["Docs","Features"],"content":"Linkerd’s data plane works by transparently routing all TCP traffic to and from every meshed pod to its proxy. (See the Architecture doc.) This allows Linkerd to act without the application being aware.\nBy default, this rewiring is done with an Init Container that uses iptables to install routing rules for the pod, at pod startup time. However, this requires the CAP_NET_ADMIN capability; and in some clusters, this capability is not granted to pods.\nTo handle this, Linkerd can optionally run these iptables rules in a CNI plugin rather than in an Init Container. This avoids the need for a CAP_NET_ADMIN capability.\nNote Linkerd’s CNI plugin is designed to run in conjunction with your existing CNI plugin, using CNI chaining. It handles only the Linkerd-specific configuration and does not replace the need for a CNI plugin. Note If you’re installing Linkerd’s CNI plugin on top of Cilium, make sure to install the latter with the option cni.exclusive=false, so Cilium doesn’t take ownership over the CNI configurations directory, and allows other plugins to deploy their configurations there. Installation Usage of the Linkerd CNI plugin requires that the linkerd-cni DaemonSet be successfully installed on your cluster first, before installing the Linkerd control plane.\nUsing the CLI To install the linkerd-cni DaemonSet, run:\nlinkerd install-cni | kubectl apply -f - Once the DaemonSet is up and running, meshed pods should no longer use the linkerd-init Init Container. To accomplish this, use the --linkerd-cni-enabled flag when installing the control plane:\nlinkerd install --linkerd-cni-enabled | kubectl apply -f - Using this option will set a cniEnabled flag in the linkerd-config ConfigMap. Proxy injections will read this field and omit the linkerd-init Init Container.\nUsing Helm First ensure that your Helm local cache is updated:\nhelm repo update helm search repo linkerd2-cni Install the CNI DaemonSet:\n# install the CNI plugin first helm install linkerd-cni -n linkerd-cni --create-namespace linkerd/linkerd2-cni # ensure the plugin is installed and ready linkerd check --pre --linkerd-cni-enabled At that point you are ready to install Linkerd with CNI enabled. Follow the Installing Linkerd with Helm instructions.\nAdditional configuration The linkerd install-cni command includes additional flags that you can use to customize the installation. See linkerd install-cni --help for more information. Note that many of the flags are similar to the flags that can be used to configure the proxy when running linkerd inject. If you change a default when running linkerd install-cni, you will want to ensure that you make a corresponding change when running linkerd inject.\nThe most important flags are:\n--dest-cni-net-dir: This is the directory on the node where the CNI Configuration resides. It defaults to: /etc/cni/net.d. --dest-cni-bin-dir: This is the directory on the node where the CNI Plugin binaries reside. It defaults to: /opt/cni/bin. --cni-log-level: Setting this to …","description":"Linkerd can optionally use a CNI plugin instead of an init-container to avoid NET_ADMIN capabilities.","keywords":null,"section":"2.16","title":"CNI Plugin","uri":"https://travisbeckham.github.io/2.16/features/cni/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output shell completion code for the specified shell (bash, zsh or fish).\nExamples # bash \u0026lt;= 3.2: # To load shell completion into your current shell session source /dev/stdin \u0026lt;\u0026lt;\u0026lt; \u0026#34;$(linkerd completion bash)\u0026#34; # bash \u0026gt;= 4.0: source \u0026lt;(linkerd completion bash) # To load shell completion for every shell session # bash \u0026lt;= 3.2 on osx: brew install bash-completion # ensure you have bash-completion 1.3+ linkerd completion bash \u0026gt; $(brew --prefix)/etc/bash_completion.d/linkerd # bash \u0026gt;= 4.0 on osx: brew install bash-completion@2 linkerd completion bash \u0026gt; $(brew --prefix)/etc/bash_completion.d/linkerd # bash \u0026gt;= 4.0 on linux: linkerd completion bash \u0026gt; /etc/bash_completion.d/linkerd # You will need to start a new shell for this setup to take effect. # zsh: # If shell completion is not already enabled in your environment you will need # to enable it. You can execute the following once: echo \u0026#34;autoload -U compinit \u0026amp;\u0026amp; compinit\u0026#34; \u0026gt;\u0026gt; ~/.zshrc # create a linkerd \u0026#39;plugins\u0026#39; folder and add it to your $fpath mkdir $ZSH/plugins/linkerd \u0026amp;\u0026amp; echo \u0026#34;fpath=($ZSH/plugins/linkerd $fpath)\u0026#34; \u0026gt;\u0026gt; ~/.zshrc # To load completions for each session, execute once: linkerd completion zsh \u0026gt; \u0026#34;${fpath[1]}/_linkerd\u0026#34; \u0026amp;\u0026amp; exec $SHELL # You will need to start a new shell for this setup to take effect. # fish: linkerd completion fish | source # To load fish shell completions for each session, execute once: linkerd completion fish \u0026gt; ~/.config/fish/completions/linkerd.fish ","description":"","keywords":null,"section":"2.16","title":"completion","uri":"https://travisbeckham.github.io/2.16/reference/cli/completion/"},{"ancestors":["Docs","Tasks"],"content":" Prerequisites To use this guide, you’ll need to have Linkerd installed on your cluster. Follow the Installing Linkerd Guide if you haven’t already done this (make sure you have at least linkerd stable-2.13.0 or edge-23.3.2).\nYou also need to have the Helm CLI installed.\nHTTPRoute for Dynamic Request Routing With dynamic request routing, you can route HTTP traffic based on the contents of request headers. This can be useful for performing things like A/B testing and many other strategies for traffic management.\nIn this tutorial, we’ll make use of the podinfo project to showcase dynamic request routing, by deploying in the cluster two backend and one frontend podinfo pods. Traffic will flow to just one backend, and then we’ll switch traffic to the other one just by adding a header to the frontend requests.\nSetup First we create the test namespace, annotated by linkerd so all pods that get created there get injected with the linkerd proxy:\nkubectl create ns test --dry-run=client -o yaml \\ | linkerd inject - \\ | kubectl apply -f - Then we add podinfo’s Helm repo, and install two instances of it. The first one will respond with the message “A backend”, the second one with “B backend”.\nhelm repo add podinfo https://stefanprodan.github.io/podinfo helm install backend-a -n test \\ --set ui.message=\u0026#39;A backend\u0026#39; podinfo/podinfo helm install backend-b -n test \\ --set ui.message=\u0026#39;B backend\u0026#39; podinfo/podinfo We add another podinfo instance which will forward requests only to the first backend instance backend-a:\nhelm install frontend -n test \\ --set backend=http://backend-a-podinfo:9898/env podinfo/podinfo Once those three pods are up and running, we can port-forward requests from our local machine to the frontend:\nkubectl -n test port-forward svc/frontend-podinfo 9898 \u0026amp; Sending Requests Requests to /echo on port 9898 to the frontend pod will get forwarded the pod pointed by the Service backend-a-podinfo:\n$ curl -sX POST localhost:9898/echo \\ | grep -o \u0026#39;PODINFO_UI_MESSAGE=. backend\u0026#39; PODINFO_UI_MESSAGE=A backend Introducing HTTPRoute Let’s apply the following HTTPRoute resource to enable header-based routing:\ncat \u0026lt;\u0026lt;EOF | kubectl -n test apply -f - apiVersion: policy.linkerd.io/v1beta2 kind: HTTPRoute metadata: name: backend-router namespace: test spec: parentRefs: - name: backend-a-podinfo kind: Service group: core port: 9898 rules: - matches: - headers: - name: \u0026#34;x-request-id\u0026#34; value: \u0026#34;alternative\u0026#34; backendRefs: - name: \u0026#34;backend-b-podinfo\u0026#34; port: 9898 - backendRefs: - name: \u0026#34;backend-a-podinfo\u0026#34; port: 9898 EOF Note Two versions of the HTTPRoute resource may be used with Linkerd:\nThe upstream version provided by the Gateway API, with the gateway.networking.k8s.io API group A Linkerd-specific CRD provided by Linkerd, with the policy.linkerd.io API group The two HTTPRoute resource definitions are similar, but the Linkerd version implements experimental features not yet available with the upstream Gateway API resource definition. See the HTTPRoute reference documentation …","description":"Configuring HTTPRoute resources to perform dynamic request routing.","keywords":null,"section":"2.16","title":"Configuring Dynamic Request Routing","uri":"https://travisbeckham.github.io/2.16/tasks/configuring-dynamic-request-routing/"},{"ancestors":["Docs","Tasks"],"content":" In addition to enforcing authorization at the service level, finer-grained authorization policies can also be configured for individual HTTP routes. In this example, we’ll use the Books demo app to demonstrate how to control which clients can access particular routes on a service.\nThis is an advanced example that demonstrates more complex policy configuration. For a basic introduction to Linkerd authorization policy, start with the Restricting Access to Services example. For more comprehensive documentation of the policy resources, see the Authorization policy reference.\nPrerequisites To use this guide, you’ll need to have Linkerd installed on your cluster, along with its Viz extension. Follow the Installing Linkerd Guide if you haven’t already done this.\nInstall the Books demo application Inject and install the Books demo application:\n$ kubectl create ns booksapp \u0026amp;\u0026amp; \\ curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSfL https://run.linkerd.io/booksapp.yml \\ | linkerd inject - \\ | kubectl -n booksapp apply -f - This command creates a namespace for the demo, downloads its Kubernetes resource manifest, injects Linkerd into the application, and uses kubectl to apply it to your cluster. The app comprises the Kubernetes deployments and services that run in the booksapp namespace.\nConfirm that the Linkerd data plane was injected successfully:\n$ linkerd check -n booksapp --proxy -o short You can take a quick look at all the components that were added to your cluster by running:\n$ kubectl -n booksapp get all Once the rollout has completed successfully, you can access the app itself by port-forwarding webapp locally:\n$ kubectl -n booksapp port-forward svc/webapp 7000 \u0026amp; Open http://localhost:7000/ in your browser to see the frontend.\nFrontend Creating a Server resource Both the books service and the webapp service in the demo application are clients of the authors service.\nHowever, these services send different requests to the authors service. The books service should only send GET requests to the /authors/:id.json route, to get the author associated with a particular book. Meanwhile, the webapp service may also send DELETE and PUT requests to /authors, and POST requests to /authors.json, as it allows the user to create and delete authors.\nSince the books service should never need to create or delete authors, we will create separate authorization policies for the webapp and books services, restricting which services can access individual routes of the authors service.\nFirst, let’s run the linkerd viz authz command to list the authorization resources that currently exist for the authors deployment:\n$ linkerd viz authz -n booksapp deploy/authors ROUTE SERVER AUTHORIZATION UNAUTHORIZED SUCCESS RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 default default:all-unauthenticated default/all-unauthenticated 0.0rps 70.31% 8.1rps 1ms 43ms 49ms probe default:all-unauthenticated default/probe 0.0rps 100.00% 0.3rps 1ms 1ms 1ms By default, the authors deployment uses the cluster’s default …","description":"Fine-grained authorization policies can be configured for individual HTTP routes.","keywords":null,"section":"2.16","title":"Configuring Per-Route Authorization Policy","uri":"https://travisbeckham.github.io/2.16/tasks/configuring-per-route-policy/"},{"ancestors":["Docs","Tasks"],"content":"The Linkerd data plane’s proxies are multithreaded, and are capable of running a variable number of worker threads so that their resource usage matches the application workload.\nIn a vacuum, of course, proxies will exhibit the best throughput and lowest latency when allowed to use as many CPU cores as possible. However, in practice, there are other considerations to take into account.\nA real world deployment is not a load test where clients and servers perform no other work beyond saturating the proxy with requests. Instead, the service mesh model has proxy instances deployed as sidecars to application containers. Each proxy only handles traffic to and from the pod it is injected into. This means that throughput and latency are limited by the application workload. If an application container instance can only handle so many requests per second, it may not actually matter that the proxy could handle more. In fact, giving the proxy more CPU cores than it requires to keep up with the application may harm overall performance, as the application may have to compete with the proxy for finite system resources.\nTherefore, it is more important for individual proxies to handle their traffic efficiently than to configure all proxies to handle the maximum possible load. The primary method of tuning proxy resource usage is limiting the number of worker threads used by the proxy to forward traffic. There are multiple methods for doing this.\nUsing the proxy-cpu-limit Annotation The simplest way to configure the proxy’s thread pool is using the config.linkerd.io/proxy-cpu-limit annotation. This annotation configures the proxy injector to set an environment variable that controls the number of CPU cores the proxy will use.\nWhen installing Linkerd using the linkerd install CLI command, the --proxy-cpu-limit argument sets this annotation globally for all proxies injected by the Linkerd installation. For example,\n# first, install the Linkerd CRDs linkerd install --crds | kubectl apply -f - # install Linkerd, with a proxy CPU limit configured. linkerd install --proxy-cpu-limit 2 | kubectl apply -f - For more fine-grained configuration, the annotation may be added to any injectable Kubernetes resource, such as a namespace, pod, or deployment.\nFor example, the following will configure any proxies in the my-deployment deployment to use two CPU cores:\nkind: Deployment apiVersion: apps/v1 metadata: name: my-deployment # ... spec: template: metadata: annotations: config.linkerd.io/proxy-cpu-limit: \u0026#39;1\u0026#39; # ... Note Unlike Kubernetes CPU limits and requests, which can be expressed in milliCPUs, the proxy-cpu-limit annotation should be expressed in whole numbers of CPU cores. Fractional values will be rounded up to the nearest whole number. Using Kubernetes CPU Limits and Requests Kubernetes provides CPU limits and CPU requests to configure the resources assigned to any pod or container. These may also be used to configure the Linkerd proxy’s CPU usage. However, depending on …","description":"Limit the Linkerd proxy's CPU usage.","keywords":null,"section":"2.16","title":"Configuring Proxy Concurrency","uri":"https://travisbeckham.github.io/2.16/tasks/configuring-proxy-concurrency/"},{"ancestors":["Docs","Tasks"],"content":"The Linkerd proxy maintains in-memory state, such as discovery results, requests and connections. This state is cached to allow the proxy to process traffic more efficiently. Cached discovery results also improve resiliency in the face of control plane outages.\nTo ensure the CPU and memory footprint is low, cached entries are dropped if they go unused for some amount of time. If an entry is not referenced within the timeout, it will be evicted. If it is referenced, the timer resets.\nThese timeouts are handle via these two config values:\nproxy.outboundDiscoveryCacheUnusedTimeout: Defines the eviction timeout for cached service discovery results, connections and clients. Defaults to 5s. proxy.inboundDiscoveryCacheUnusedTimeout: Defines the eviction timeout for cached policy discovery results. Defaults to 90s. These values can be configured globally (affecting all the data plane) via Helm or the CLI at install/upgrade time, or with annotations at a namespace or workload level for affecting only workloads under a given namespace or specific workloads.\nConfiguring via Helm When installing/upgrading Linkerd via Helm, you can use the proxy.outboundDiscoveryCacheUnusedTimeout and proxy.inboundDiscoveryCacheUnusedTimeout values. For example:\nhelm upgrade linkerd-control-plane \\ --set proxy.outboundDiscoveryCacheUnusedTimeout=60s \\ --set proxy.inboundDiscoveryCacheUnusedTimeout=120s \\ linkerd/linkerd-control-plane Configuring via the Linkerd CLI As with any Helm value, these are available via the --set flag:\nlinkerd upgrade \\ --set proxy.outboundDiscoveryCacheUnusedTimeout=60s \\ --set proxy.inboundDiscoveryCacheUnusedTimeout=120s \\ | kubectl apply -f - Configuring via Annotations You can also use the config.linkerd.io/proxy-outbound-discovery-cache-unused-timeout and config.linkerd.io/proxy-inbound-discovery-cache-unused-timeout annotations at the namespace or pod template level:\nkind: Deployment apiVersion: apps/v1 metadata: name: my-deployment # ... spec: template: metadata: annotations: config.linkerd.io/proxy-outbound-discovery-cache-unused-timeout: \u0026#39;60s\u0026#39; config.linkerd.io/proxy-inbound-discovery-cache-unused-timeout: \u0026#39;120s\u0026#39; # ... Note that these values need to be present before having injected your workloads. For applying to existing workloads, you’ll need to roll them out.\nWhen to Change Timeouts In the vast majority of cases the default values will just work. You should think about experimenting with larger values when using slow clients (5 RPS or less across two or more replicas) where clients would experience unexpected connection closure errors as soon as the control plane comes down. A higher cache idle timeout for discovery results can help mitigating these problems.\n","description":"Changing proxy discover cache timeouts when using slow clients.","keywords":null,"section":"2.16","title":"Configuring Proxy Discovery Cache","uri":"https://travisbeckham.github.io/2.16/tasks/configuring-proxy-discovery-cache/"},{"ancestors":["Docs","Tasks"],"content":"In order for Linkerd to do automatic retries of failures, there are two questions that need to be answered:\nWhich requests should be retried? How many times should the requests be retried? Both of these questions can be answered by adding annotations to the Service, HTTPRoute, or GRPCRoute resource you’re sending requests to.\nThe reason why these pieces of configuration are required is because retries can potentially be dangerous. Automatically retrying a request that changes state (e.g. a request that submits a financial transaction) could potentially impact your user’s experience negatively. In addition, retries increase the load on your system. A set of services that have requests being constantly retried could potentially get taken down by the retries instead of being allowed time to recover.\nCheck out the retries section of the books demo for a tutorial of how to configure retries.\nWarning Retries configured in this way are incompatible with ServiceProfiles. If a ServiceProfile is defined for a Service, proxies will use the ServiceProfile retry configuration and ignore any retry annotations. Retries For HTTPRoutes that are idempotent, you can add the retry.linkerd.io/http: 5xx annotation which instructs Linkerd to retry any requests which fail with an HTTP response status in the 500s.\nNote that requests will not be retried if the body exceeds 64KiB.\nRetry Limits You can also add the retry.linkerd.io/limit annotation to specify the maximum number of times a request may be retried. By default, this limit is 1.\ngRPC Retries Retries can also be configured for gRPC traffic by adding the retry.linkerd.io/grpc annotation to a GRPCRoute or Service resource. The value of this annotation is a comma seperated list of gRPC status codes that should be retried.\n","description":"Configure Linkerd to automatically retry failing requests.","keywords":null,"section":"2.16","title":"Configuring Retries","uri":"https://travisbeckham.github.io/2.16/tasks/configuring-retries/"},{"ancestors":["Docs","Tasks"],"content":"To limit how long Linkerd will wait before failing an outgoing request to another service, you can configure timeouts. Timeouts specify the maximum amount of time to wait for a response from a remote service to complete after the request is sent. If the timeout elapses without receiving a response, Linkerd will cancel the request and return a [504 Gateway Timeout] response.\nTimeouts can be specified by adding annotations to HTTPRoute, GRPCRoute, or Service resources.\nWarning Timeouts configured in this way are incompatible with ServiceProfiles. If a ServiceProfile is defined for a Service, proxies will use the ServiceProfile timeout configuration and ignore any timeout annotations. Timeouts Check out the timeouts section of the books demo for a tutorial of how to configure timeouts.\n","description":"Configure Linkerd to automatically fail requests that take too long.","keywords":null,"section":"2.16","title":"Configuring Timeouts","uri":"https://travisbeckham.github.io/2.16/tasks/configuring-timeouts/"},{"ancestors":["Docs","Tasks"],"content":"All of the control plane components expose runtime profiling information through the path /debug/pprof, using Go’s pprof package. This endpoint is disabled by default but can be enabled to gather profiling data.\nYou can consume the provided data with go tool pprof to generate output in many formats (PDF, DOT, PNG, etc).\nThe following diagnostics are provided (a summary with links is provided at /debug/pprof):\nallocs: A sampling of all past memory allocations block: Stack traces that led to blocking on synchronization primitives cmdline: The command line invocation of the current program goroutine: Stack traces of all current goroutines heap: A sampling of memory allocations of live objects. You can specify the gc GET parameter to run GC before taking the heap sample. mutex: Stack traces of holders of contended mutexes profile: CPU profile. You can specify the duration in the seconds GET parameter. After you get the profile file, use the go tool pprof command to investigate the profile. threadcreate: Stack traces that led to the creation of new OS threads trace: A trace of execution of the current program. You can specify the duration in the seconds GET parameter. After you get the trace file, use the go tool trace command to investigate the trace. Example Usage The pprof endpoint can be enabled by setting the --set enablePprof=true flag when installing or upgrading Linkerd or by setting the enablePprof=true Helm value.\nThis data is served over the admin-http port. To find this port, you can examine the pod’s yaml, or for the identity pod for example, issue a command like so:\nkubectl -n linkerd get po \\ $(kubectl -n linkerd get pod -l linkerd.io/control-plane-component=identity \\ -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\ -o=jsonpath=\u0026#39;{.spec.containers[*].ports[?(@.name==\u0026#34;admin-http\u0026#34;)].containerPort}\u0026#39; Then use the kubectl port-forward command to access that port from outside the cluster (in this example the port is 9990):\nkubectl -n linkerd port-forward \\ $(kubectl -n linkerd get pod -l linkerd.io/control-plane-component=identity \\ -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\ 9990 It is now possible to use go tool to inspect this data. For example to generate a graph in a PDF file describing memory allocations:\ngo tool pprof -seconds 5 -pdf http://localhost:9990/debug/pprof/allocs ","description":"Linkerd's control plane components provide debug endpoints.","keywords":null,"section":"2.16","title":"Control Plane Debug Endpoints","uri":"https://travisbeckham.github.io/2.16/tasks/using-debug-endpoints/"},{"ancestors":["Docs","Tasks"],"content":"Instead of forking the Linkerd install and upgrade process, Kustomize can be used to patch the output of linkerd install in a consistent way. This allows customization of the install to add functionality specific to installations.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. To get started, save the output of linkerd install to a YAML file. This will be the base resource that Kustomize uses to patch and generate what is added to your cluster.\nlinkerd install \u0026gt; linkerd.yaml Note When upgrading, make sure you populate this file with the content from linkerd upgrade. Using the latest kustomize releases, it would be possible to automate this with an exec plugin. Next, create a kustomization.yaml file. This file will contain the instructions for Kustomize listing the base resources and the transformations to do on those resources. Right now, this looks pretty empty:\nresources: - linkerd.yaml Now, let’s look at how to do some example customizations.\nNote Kustomize allows as many patches, transforms and generators as you’d like. These examples show modifications one at a time but it is possible to do as many as required in a single kustomization.yaml file. Add PriorityClass There are a couple components in the control plane that can benefit from being associated with a critical PriorityClass. While this configuration isn’t currently supported as a flag to linkerd install, it is not hard to add by using Kustomize.\nFirst, create a file named priority-class.yaml that will create define a PriorityClass resource.\napiVersion: scheduling.k8s.io/v1 description: Used for critical linkerd pods that must run in the cluster, but can be moved to another node if necessary. kind: PriorityClass metadata: name: linkerd-critical value: 1000000000 Note 1000000000 is the max. allowed user-defined priority, adjust accordingly. Next, create a file named patch-priority-class.yaml that will contain the overlay. This overlay will explain what needs to be modified.\napiVersion: apps/v1 kind: Deployment metadata: name: linkerd-identity namespace: linkerd spec: template: spec: priorityClassName: linkerd-critical Then, add this as a strategic merge option to kustomization.yaml:\nresources: - priority-class.yaml - linkerd.yaml patchesStrategicMerge: - patch-priority-class.yaml Applying this to your cluster requires taking the output of kustomize and piping it to kubectl apply. For example, you can run:\n# install the Linkerd CRDs linkerd install --crds | kubectl apply -f - # install the Linkerd control plane manifests using Kustomize kubectl kustomize . | kubectl apply -f - ","description":"Use Kustomize to modify Linkerd's configuration in a programmatic way.","keywords":null,"section":"2.16","title":"Customizing Linkerd's Configuration with Kustomize","uri":"https://travisbeckham.github.io/2.16/tasks/customize-install/"},{"ancestors":["Docs","Features"],"content":"Linkerd provides a full on-cluster metrics stack, including CLI tools and a web dashboard.\nTo access this functionality, install the viz extension:\nlinkerd viz install | kubectl apply -f - This extension installs the following components into your linkerd-viz namespace:\nA Prometheus instance metrics-api, tap, tap-injector, and web components These components work together to provide an on-cluster metrics stack.\nNote To limit excessive resource usage on the cluster, the metrics stored by this extension are transient. Only the past 6 hours are stored, and metrics do not persist in the event of pod restart or node outages. This may not be suitable for production use. Note This metrics stack may require significant cluster resources. Prometheus, in particular, will consume resources as a function of traffic volume within the cluster. Linkerd dashboard The Linkerd dashboard provides a high level view of what is happening with your services in real time. It can be used to view “golden metrics” (success rate, requests/second and latency), visualize service dependencies and understand the health of specific service routes.\nOne way to pull it up is by running linkerd viz dashboard from the command line.\nTop Line Metrics Grafana In earlier versions of Linkerd, the viz extension also pre-installed a Grafana dashboard. As of Linkerd 2.12, due to licensing changes in Grafana, this is no longer the case. However, you can still install Grafana on your own—see the Grafana docs for instructions on how to create the Grafana dashboards.\nExamples In these examples, we assume you’ve installed the emojivoto example application. Please refer to the Getting Started Guide for how to do this.\nYou can use your dashboard extension and see all the services in the demo app. Since the demo app comes with a load generator, we can see live traffic metrics by running:\nlinkerd -n emojivoto viz stat deploy This will show the “golden” metrics for each deployment:\nSuccess rates Request rates Latency distribution percentiles To dig in a little further, it is possible to use top to get a real-time view of which paths are being called:\nlinkerd -n emojivoto viz top deploy To go even deeper, we can use tap shows the stream of requests across a single pod, deployment, or even everything in the emojivoto namespace:\nlinkerd -n emojivoto viz tap deploy/web All of this functionality is also available in the dashboard, if you would like to use your browser instead:\nTop Line Metrics Deployment Detail Top Tap Futher reading See Exporting metrics for alternative ways to consume Linkerd’s metrics.\n","description":"Linkerd provides a full on-cluster metrics stack, including CLI tools and dashboards.","keywords":null,"section":"2.16","title":"Dashboard and on-cluster metrics stack","uri":"https://travisbeckham.github.io/2.16/features/dashboard/"},{"ancestors":["Docs","Tasks"],"content":"When the Linkerd proxy encounters connection errors while processing a request, it will typically return an HTTP 502 (Bad Gateway) response. It can be very difficult to figure out why these errors are happening because of the lack of information available.\nWhy do these errors only occur when Linkerd is injected? Linkerd turns connection errors into HTTP 502 responses. This can make issues which were previously undetected suddenly visible. This is a good thing. Linkerd also changes the way that connections to your application are managed: it re-uses persistent connections and establishes an additional layer of connection tracking. Managing connections in this way can sometimes expose underlying application or infrastructure issues such as misconfigured connection timeouts which can manifest as connection errors.\nWhy can’t Linkerd give a more informative error message? From the Linkerd proxy’s perspective, it just sees its connections to the application refused or closed without explanation. This makes it nearly impossible for Linkerd to report any error message in the 502 response. However, if these errors coincide with the introduction of Linkerd, it does suggest that the problem is related to connection re-use or connection tracking. Here are some common reasons why the application may be refusing or terminating connections.\nCommon Causes of Connection Errors Connection Idle Timeouts Some servers are configured with a connection idle timeout (for example, this timeout in the Go HTTP server. This means that the server will close any connections which do not receive any traffic in the specified time period. If any requests are already in transit when the connection shutdown is initiated, those requests will fail. This scenario is likely to occur if you have traffic with a regular period (such as liveness checks, for example) and an idle timeout equal to that period.\nTo remedy this, ensure that your server’s idle timeouts are sufficiently long so that they do not close connections which are actively in use.\nHalf-closed Connection Timeouts During the shutdown of a TCP connection, each side of the connection must be closed independently. When one side is closed but the other is not, the connection is said to be “half-closed”. It is valid for the connection to be in this state, however, the operating system’s connection tracker can lose track of connections which remain half-closed for long periods of time. This can lead to responses not being delivered and to port conflicts when establishing new connections which manifest as 502 responses.\nYou can use a script to detect half-closed connections on your Kubernetes cluster. If you detect a large number of half-closed connections, you have a couple of ways to remedy the situation.\nOne solution would be to update your application to not leave connections half-closed for long periods of time or to stop using software that does this. Unfortunately, this is not always an option.\nAnother option is to increase …","description":"Determine why Linkerd is returning 502 responses.","keywords":null,"section":"2.16","title":"Debugging 502s","uri":"https://travisbeckham.github.io/2.16/tasks/debugging-502s/"},{"ancestors":["Docs","Tasks"],"content":"The demo application emojivoto has some issues. Let’s use that and Linkerd to diagnose an application that fails in ways which are a little more subtle than the entire service crashing. This guide assumes that you’ve followed the steps in the Getting Started guide and have Linkerd and the demo application running in a Kubernetes cluster. If you’ve not done that yet, go get started and come back when you’re done!\nIf you glance at the Linkerd dashboard (by running the linkerd viz dashboard command), you should see all the resources in the emojivoto namespace, including the deployments. Each deployment running Linkerd shows success rate, requests per second and latency percentiles.\nTop Level Metrics That’s pretty neat, but the first thing you might notice is that the success rate is well below 100%! Click on web and let’s dig in.\nDeployment Detail You should now be looking at the Deployment page for the web deployment. The first thing you’ll see here is that the web deployment is taking traffic from vote-bot (a deployment included with emojivoto to continually generate a low level of live traffic). The web deployment also has two outgoing dependencies, emoji and voting.\nWhile the emoji deployment is handling every request from web successfully, it looks like the voting deployment is failing some requests! A failure in a dependent deployment may be exactly what is causing the errors that web is returning.\nLet’s scroll a little further down the page, we’ll see a live list of all traffic that is incoming to and outgoing from web. This is interesting:\nTop There are two calls that are not at 100%: the first is vote-bot’s call to the /api/vote endpoint. The second is the VoteDoughnut call from the web deployment to its dependent deployment, voting. Very interesting! Since /api/vote is an incoming call, and VoteDoughnut is an outgoing call, this is a good clue that this endpoint is what’s causing the problem!\nFinally, to dig a little deeper, we can click on the tap icon in the far right column. This will take us to the live list of requests that match only this endpoint. You’ll see Unknown under the GRPC status column. This is because the requests are failing with a gRPC status code 2, which is a common error response as you can see from the code. Linkerd is aware of gRPC’s response classification without any other configuration!\nTap At this point, we have everything required to get the endpoint fixed and restore the overall health of our applications.\n","description":"Follow a long-form example of debugging a failing gRPC application using live request tracing.","keywords":null,"section":"2.16","title":"Debugging gRPC applications with request tracing","uri":"https://travisbeckham.github.io/2.16/tasks/debugging-your-service/"},{"ancestors":["Docs","Tasks"],"content":"This demo is of a Ruby application that helps you manage your bookshelf. It consists of multiple microservices and uses JSON over HTTP to communicate with the other services. There are three services:\nwebapp: the frontend\nauthors: an API to manage the authors in the system\nbooks: an API to manage the books in the system\nFor demo purposes, the app comes with a simple traffic generator. The overall topology looks like this:\nTopology Prerequisites To use this guide, you’ll need to have Linkerd installed on your cluster. Follow the Installing Linkerd Guide if you haven’t already done this.\nInstall the app To get started, let’s install the books app onto your cluster. In your local terminal, run:\nkubectl create ns booksapp \u0026amp;\u0026amp; \\ curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSfL https://run.linkerd.io/booksapp.yml \\ | kubectl -n booksapp apply -f - This command creates a namespace for the demo, downloads its Kubernetes resource manifest and uses kubectl to apply it to your cluster. The app comprises the Kubernetes deployments and services that run in the booksapp namespace.\nDownloading a bunch of containers for the first time takes a little while. Kubernetes can tell you when all the services are running and ready for traffic. Wait for that to happen by running:\nkubectl -n booksapp rollout status deploy webapp You can also take a quick look at all the components that were added to your cluster by running:\nkubectl -n booksapp get all Once the rollout has completed successfully, you can access the app itself by port-forwarding webapp locally:\nkubectl -n booksapp port-forward svc/webapp 7000 \u0026gt;/dev/null \u0026amp; (We redirect to /dev/null just so you don’t get flooded with “Handling connection” messages for the rest of the exercise.)\nOpen http://localhost:7000/ in your browser to see the frontend.\nFrontend Unfortunately, there is an error in the app: if you click Add Book, it will fail 50% of the time. This is a classic case of non-obvious, intermittent failure—the type that drives service owners mad because it is so difficult to debug. Kubernetes itself cannot detect or surface this error. From Kubernetes’s perspective, it looks like everything’s fine, but you know the application is returning errors.\nFailure Add Linkerd to the service Now we need to add the Linkerd data plane proxies to the service. The easiest option is to do something like this:\nkubectl get -n booksapp deploy -o yaml \\ | linkerd inject - \\ | kubectl apply -f - This command retrieves the manifest of all deployments in the booksapp namespace, runs them through linkerd inject, and then re-applies with kubectl apply. The linkerd inject command annotates each resource to specify that they should have the Linkerd data plane proxies added, and Kubernetes does this when the manifest is reapplied to the cluster. Best of all, since Kubernetes does a rolling deploy, the application stays running the entire time. (See Automatic Proxy Injection for more details on how this works.)\nDebugging Let’s use Linkerd to …","description":"Follow a long-form example of debugging a failing HTTP application using per-route metrics.","keywords":null,"section":"2.16","title":"Debugging HTTP applications with per-route metrics","uri":"https://travisbeckham.github.io/2.16/tasks/books/"},{"ancestors":["Docs","Reference","CLI"],"content":" Commands used to diagnose Linkerd components.\nThis command provides subcommands to diagnose the functionality of Linkerd.\nExamples # Get control-plane component metrics linkerd diagnostics controller-metrics # Get metrics from the web deployment in the emojivoto namespace. linkerd diagnostics proxy-metrics -n emojivoto deploy/web # Get the endpoints for authorities in Linkerd\u0026#39;s control-plane itself linkerd diagnostics endpoints web.linkerd-viz.svc.cluster.local:8084 Subcommands controller-metrics Fetch metrics directly from Linkerd control plane containers.\nThis command initiates port-forward to each control plane process, and queries the /metrics endpoint on them.\nFlags Flag Usage --wait -w Time allowed to fetch diagnostics endpoints Introspect Linkerd’s service discovery state.\nThis command provides debug information about the internal state of the control-plane’s destination container. It queries the same Destination service endpoint as the linkerd-proxy’s, and returns the addresses associated with that destination.\nExamples # get all endpoints for the authorities emoji-svc.emojivoto.svc.cluster.local:8080 and web-svc.emojivoto.svc.cluster.local:80 linkerd diagnostics endpoints emoji-svc.emojivoto.svc.cluster.local:8080 web-svc.emojivoto.svc.cluster.local:80 # get that same information in json format linkerd diagnostics endpoints -o json emoji-svc.emojivoto.svc.cluster.local:8080 web-svc.emojivoto.svc.cluster.local:80 # get the endpoints for authorities in Linkerd\u0026#39;s control-plane itself linkerd diagnostics endpoints web.linkerd-viz.svc.cluster.local:8084 Flags Flag Usage --destination-pod Target a specific destination Pod when there are multiple running --output -o Output format; one of: “table” or “json” --token The context token to use when making the request to the destination API install-sp proxy-metrics Fetch metrics directly from Linkerd proxies.\nThis command initiates a port-forward to a given pod or set of pods, and queries the /metrics endpoint on the Linkerd proxies.\nThe RESOURCE argument specifies the target resource to query metrics for: (TYPE/NAME)\nExamples:\ncronjob/my-cronjob deploy/my-deploy ds/my-daemonset job/my-job po/mypod1 rc/my-replication-controller sts/my-statefulset Valid resource types include:\ncronjobs daemonsets deployments jobs pods replicasets replicationcontrollers statefulsets Examples # Get metrics from pod-foo-bar in the default namespace. linkerd diagnostics proxy-metrics po/pod-foo-bar # Get metrics from the web deployment in the emojivoto namespace. linkerd diagnostics proxy-metrics -n emojivoto deploy/web # Get metrics from the linkerd-destination pod in the linkerd namespace. linkerd diagnostics proxy-metrics -n linkerd $( kubectl --namespace linkerd get pod \\ --selector linkerd.io/control-plane-component=destination \\ --output name ) Flags Flag Usage --namespace -n Namespace of resource --obfuscate Obfuscate sensitive information ","description":"","keywords":null,"section":"2.16","title":"diagnostics","uri":"https://travisbeckham.github.io/2.16/reference/cli/diagnostics/"},{"ancestors":["Docs","Features"],"content":"Tracing can be an invaluable tool in debugging distributed systems performance, especially for identifying bottlenecks and understanding the latency cost of each component in your system. Linkerd can be configured to emit trace spans from the proxies, allowing you to see exactly what time requests and responses spend inside.\nUnlike most of the features of Linkerd, distributed tracing requires both code changes and configuration. (You can read up on Distributed tracing in the service mesh: four myths for why this is.)\nFurthermore, Linkerd provides many of the features that are often associated with distributed tracing, without requiring configuration or application changes, including:\nLive service topology and dependency graphs Aggregated service health, latencies, and request volumes Aggregated path / route health, latencies, and request volumes For example, Linkerd can display a live topology of all incoming and outgoing dependencies for a service, without requiring distributed tracing or any other such application modification:\nThe Linkerd dashboard showing an automatically generated topology graph Likewise, Linkerd can provide golden metrics per service and per route, again without requiring distributed tracing or any other such application modification:\nLinkerd dashboard showing an automatically generated route metrics Using distributed tracing That said, distributed tracing certainly has its uses, and Linkerd makes this as easy as it can. Linkerd’s role in distributed tracing is actually quite simple: when a Linkerd data plane proxy sees a tracing header in a proxied HTTP request, Linkerd will emit a trace span for that request. This span will include information about the exact amount of time spent in the Linkerd proxy. When paired with software to collect, store, and analyze this information, this can provide significant insight into the behavior of the mesh.\nTo use this feature, you’ll also need to introduce several additional components in your system., including an ingress layer that kicks off the trace on particular requests, a client library for your application (or a mechanism to propagate trace headers), a trace collector to collect span data and turn them into traces, and a trace backend to store the trace data and allow the user to view/query it.\nFor details, please see our guide to adding distributed tracing to your application with Linkerd.\n","description":"You can enable distributed tracing support in Linkerd.","keywords":null,"section":"2.16","title":"Distributed Tracing","uri":"https://travisbeckham.github.io/2.16/features/distributed-tracing/"},{"ancestors":["Docs","Tasks"],"content":"Using distributed tracing in practice can be complex, for a high level explanation of what you get and how it is done, we’ve assembled a list of myths.\nThis guide will walk you through configuring and enabling tracing for emojivoto. Jump to the end for some recommendations on the best way to make use of distributed tracing with Linkerd.\nTo use distributed tracing, you’ll need to:\nInstall the Linkerd-Jaeger extension. Modify your application to emit spans. In the case of emojivoto, once all these steps are complete there will be a topology that looks like:\nTopology Prerequisites To use this guide, you’ll need to have Linkerd installed on your cluster. Follow the Installing Linkerd Guide if you haven’t already done this. Install the Linkerd-Jaeger extension The first step of getting distributed tracing setup is installing the Linkerd-Jaeger extension onto your cluster. This extension consists of a collector, a Jaeger backend, and a Jaeger-injector. The collector consumes spans emitted from the mesh and your applications and sends them to the Jaeger backend which stores them and serves a dashboard to view them. The Jaeger-injector is responsible for configuring the Linkerd proxies to emit spans.\nTo install the Linkerd-Jaeger extension, run the command:\nlinkerd jaeger install | kubectl apply -f - You can verify that the Linkerd-Jaeger extension was installed correctly by running:\nlinkerd jaeger check Install Emojivoto Add emojivoto to your cluster and inject it with the Linkerd proxy:\nlinkerd inject https://run.linkerd.io/emojivoto.yml | kubectl apply -f - Before moving onto the next step, make sure everything is up and running with kubectl:\nkubectl -n emojivoto rollout status deploy/web Modify the application Unlike most features of a service mesh, distributed tracing requires modifying the source of your application. Tracing needs some way to tie incoming requests to your application together with outgoing requests to dependent services. To do this, some headers are added to each request that contain a unique ID for the trace. Linkerd uses the b3 propagation format to tie these things together.\nWe’ve already modified emojivoto to instrument its requests with this information, this commit shows how this was done. For most programming languages, it simply requires the addition of a client library to take care of this. Emojivoto uses the OpenCensus client, but others can be used.\nTo enable tracing in emojivoto, run:\nkubectl -n emojivoto set env --all deploy OC_AGENT_HOST=collector.linkerd-jaeger:55678 This command will add an environment variable that enables the applications to propagate context and emit spans.\nExplore Jaeger With vote-bot starting traces for every request, spans should now be showing up in Jaeger. To get to the UI, run:\nlinkerd jaeger dashboard Jaeger You can search for any service in the dropdown and click Find Traces. vote-bot is a great way to get started.\nSearch Clicking on a specific trace will provide all the details, you’ll …","description":"Use Linkerd to help instrument your application with distributed tracing.","keywords":null,"section":"2.16","title":"Distributed tracing with Linkerd","uri":"https://travisbeckham.github.io/2.16/tasks/distributed-tracing/"},{"ancestors":["Docs","Features"],"content":"Linkerd’s dynamic request routing allows you to control routing of HTTP and gRPC traffic based on properties of the request, including verb, method, query parameters, and headers. For example, you can route all requests that match a specific URL pattern to a given backend; or you can route traffic with a particular header to a different service.\nThis is an example of client-side policy, i.e. ways to dynamically configure Linkerd’s behavior when it is sending requests from a meshed pod.\nDynamic request routing is built on Kubernetes’s Gateway API types, especially HTTPRoute.\nThis feature extends Linkerd’s traffic routing capabilities beyond those of traffic splits, which only provide percentage-based splits.\nLearning more Guide to configuring routing policy ","description":"Linkerd can route individual HTTP requests based on their properties.","keywords":null,"section":"2.16","title":"Dynamic Request Routing","uri":"https://travisbeckham.github.io/2.16/features/request-routing/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd provides an extensive set of metrics for all traffic that passes through its data plane. These metrics are collected at the proxy level and reported on the proxy’s metrics endpoint.\nTypically, consuming these metrics is not done from the proxies directly, as each proxy only provides a portion of the full picture. Instead, a separate tool is used to collect metrics from all proxies and aggregate them together for consumption.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. One easy option is the linkerd-viz extension, which will create an on-cluster Prometheus instance as well as dashboards and CLI commands that make use of it. However, this extension only keeps metrics data for a brief window of time (6 hours) and does not persist data across restarts. Depending on your use case, you may want to export these metrics into an external metrics store.\nThere are several options for how to export these metrics to a destination outside of the cluster:\nFederate data from linkerd-viz to your own Prometheus cluster Use a Prometheus integration with linkerd-viz Extract data from linkerd-viz via Prometheus’s APIs Gather data from the proxies directly without linkerd-viz Using the Prometheus federation API If you are already using Prometheus as your own metrics store, we recommend taking advantage of Prometheus’s federation API, which is designed exactly for the use case of copying data from one Prometheus to another.\nSimply add the following item to your scrape_configs in your Prometheus config file (replace {{.Namespace}} with the namespace where the Linkerd Viz extension is running):\n- job_name: \u0026#39;linkerd\u0026#39; kubernetes_sd_configs: - role: pod namespaces: names: [\u0026#39;{{.Namespace}}\u0026#39;] relabel_configs: - source_labels: - __meta_kubernetes_pod_container_name action: keep regex: ^prometheus$ honor_labels: true metrics_path: \u0026#39;/federate\u0026#39; params: \u0026#39;match[]\u0026#39;: - \u0026#39;{job=\u0026#34;linkerd-proxy\u0026#34;}\u0026#39; - \u0026#39;{job=\u0026#34;linkerd-controller\u0026#34;}\u0026#39; Alternatively, if you prefer to use Prometheus’ ServiceMonitors to configure your Prometheus, you can use this ServiceMonitor YAML (replace {{.Namespace}} with the namespace where Linkerd Viz extension is running):\napiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: k8s-app: linkerd-prometheus release: monitoring name: linkerd-federate namespace: {{.Namespace}} spec: endpoints: - interval: 30s scrapeTimeout: 30s params: match[]: - \u0026#39;{job=\u0026#34;linkerd-proxy\u0026#34;}\u0026#39; - \u0026#39;{job=\u0026#34;linkerd-controller\u0026#34;}\u0026#39; path: /federate port: admin-http honorLabels: true relabelings: - action: keep regex: \u0026#39;^prometheus$\u0026#39; sourceLabels: - \u0026#39;__meta_kubernetes_pod_container_name\u0026#39; jobLabel: app namespaceSelector: matchNames: - {{.Namespace}} selector: matchLabels: component: prometheus That’s it! Your Prometheus cluster is now configured to federate Linkerd’s …","description":"Integrate Linkerd's metrics with your existing metrics infrastructure.","keywords":null,"section":"2.16","title":"Exporting Metrics","uri":"https://travisbeckham.github.io/2.16/tasks/exporting-metrics/"},{"ancestors":["Docs","Tasks"],"content":"Instead of using linkerd viz dashboard every time you’d like to see what’s going on, you can expose the dashboard via an ingress. This will also expose Grafana, if you have it linked against Linkerd viz through the grafana.url setting.\nNginx Nginx with basic auth Nginx with oauth2-proxy Traefik Ambassador DNS Rebinding Protection Tweaking Host Requirement Nginx Nginx with basic auth A sample ingress definition is:\napiVersion: v1 kind: Secret type: Opaque metadata: name: web-ingress-auth namespace: linkerd-viz data: auth: YWRtaW46JGFwcjEkbjdDdTZnSGwkRTQ3b2dmN0NPOE5SWWpFakJPa1dNLgoK --- # apiVersion: networking.k8s.io/v1beta1 # for k8s \u0026lt; v1.19 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web-ingress namespace: linkerd-viz annotations: nginx.ingress.kubernetes.io/upstream-vhost: $service_name.$namespace.svc.cluster.local:8084 nginx.ingress.kubernetes.io/configuration-snippet: | proxy_set_header Origin \u0026#34;\u0026#34;; proxy_hide_header l5d-remote-ip; proxy_hide_header l5d-server-id; nginx.ingress.kubernetes.io/auth-type: basic nginx.ingress.kubernetes.io/auth-secret: web-ingress-auth nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required\u0026#39; spec: ingressClassName: nginx rules: - host: dashboard.example.com http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 8084 This exposes the dashboard at dashboard.example.com and protects it with basic auth using admin/admin. Take a look at the ingress-nginx documentation for details on how to change the username and password.\nNginx with oauth2-proxy A more secure alternative to basic auth is using an authentication proxy, such as oauth2-proxy.\nFor reference on how to deploy and configure oauth2-proxy in kubernetes, see this blog post by Don Bowman.\ntl;dr: If you deploy oauth2-proxy via the helm chart, the following values are required:\nconfig: existingSecret: oauth2-proxy configFile: |- email_domains = [ \u0026#34;example.com\u0026#34; ] upstreams = [ \u0026#34;file:///dev/null\u0026#34; ] ingress: enabled: true annotations: kubernetes.io/ingress.class: nginx path: /oauth2 ingress: hosts: - linkerd.example.com Where the oauth2-proxy secret would contain the required oauth2 config such as, client-id client-secret and cookie-secret.\nOnce setup, a sample ingress would be:\n# apiVersion: networking.k8s.io/v1beta1 # for k8s \u0026lt; v1.19 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: web namespace: linkerd-viz annotations: nginx.ingress.kubernetes.io/upstream-vhost: $service_name.$namespace.svc.cluster.local:8084 nginx.ingress.kubernetes.io/configuration-snippet: | proxy_set_header Origin \u0026#34;\u0026#34;; proxy_hide_header l5d-remote-ip; proxy_hide_header l5d-server-id; nginx.ingress.kubernetes.io/auth-signin: https://$host/oauth2/start?rd=$escaped_request_uri nginx.ingress.kubernetes.io/auth-url: https://$host/oauth2/auth spec: ingressClassName: nginx rules: - host: linkerd.example.com http: paths: - path: / pathType: Prefix backend: service: name: web port: number: 8084 Traefik A sample ingress definition is: …","description":"Make it easy for others to access Linkerd and Grafana dashboards without the CLI.","keywords":null,"section":"2.16","title":"Exposing the Dashboard","uri":"https://travisbeckham.github.io/2.16/tasks/exposing-dashboard/"},{"ancestors":["Docs","Reference"],"content":"Linkerd provides a mix of built-in and third-party extensions to add additional functionality to the base installation. The following is the list of known extensions:\nName Description jaeger Built-in extension that adds distributed tracing to Linkerd using OpenTelemetry collector and Jaeger. multicluster Built-in extension that enables multicluster support for Linkerd. smi The Linkerd SMI extension helps users to have SMI functionality in Linkerd-enabled Kubernetes clusters. tapshark Wireshark inspired ncurses-style CLI for Linkerd Tap viz Built-in extension that provides observability and visualization components for Linkerd. easyauth Simplify the Linkerd Authorization Policies management. If you have an extension for Linkerd and it is not on the list, please edit this page!\n","description":"List of Linkerd extensions that can be added to the installation for additional functionality","keywords":null,"section":"2.16","title":"Extensions List","uri":"https://travisbeckham.github.io/2.16/reference/extension-list/"},{"ancestors":["Docs","Reference"],"content":"Linkerd’s mesh expansion functionality allows you to join workloads outside of Kubernetes into the mesh.\nAt its core, this behavior is controlled by an ExternalWorkload resource, which is used by Linkerd to describe a workload that lives outside of Kubernetes for discovery and policy. This resource contains information such as the workload’s identity, the concrete IP address as well as ports that this workload accepts connections on.\nExternalWorkloads An ExternalWorkload is a namespace resource that defines a set of ports and an IP address that is reachable from within the mesh. Linkerd uses that information and translates it into EndpointSlices that are then attached to Service objects.\nSpec meshTLS (required) - specified the identity information that Linkerd requires to establish encrypted connections to this workload workloadIPs (required, at most 1) - an IP address that this workload is reachable on ports - a list of port definitions that the workload exposes MeshTLS identity (required) - the TLS identity of the workload, proxies require this value to establish TLS connections with the workload serverName (required) - this value is what the workload’s proxy expects to see in the ClientHello SNI TLS extension when other peers attempt to initiate a TLS connection Port name - must be unique within the ports set. Each named port can be referred to by services. port (required) - a port number that the workload is listening on protocol - protocol exposed by the port Status conditions - a list of condition objects Condition lastProbeTime - the last time the healthcheck endpoint was probed lastTransitionTime - the last time the condition transitioned from one status to another status - status of the condition (one of True, False, Unknown) type - type of the condition (Ready is used for indicating discoverability) reason - contains a programmatic identifier indicating the reason for the condition’s last transition message - message is a human-readable message indicating details about the transition. Example Below is an example of an ExternalWorkload resource that specifies a number of ports and is selected by a service.\napiVersion: workload.linkerd.io/v1beta1 kind: ExternalWorkload metadata: name: external-workload namespace: mixed-env labels: location: vm workload_name: external-workload spec: meshTLS: identity: \u0026#34;spiffe://root.linkerd.cluster.local/external-workload\u0026#34; serverName: \u0026#34;external-workload.cluster.local\u0026#34; workloadIPs: - ip: 193.1.4.11 ports: - port: 80 name: http - port: 9980 name: admin status: conditions: - type: Ready status: \u0026#34;True\u0026#34; --- apiVersion: v1 kind: Service metadata: name: external-workload namespace: mixed-env spec: type: ClusterIP selector: workload_name: external-workload ports: - port: 80 protocol: TCP name: http - port: 9980 protocol: TCP name: admin ","description":"","keywords":null,"section":"2.16","title":"ExternalWorkload","uri":"https://travisbeckham.github.io/2.16/reference/external-workload/"},{"ancestors":["Docs","Common Errors"],"content":"If Linkerd reports that a given service is in the failfast state, it means that the proxy has determined that there are no available endpoints for that service. In this situation there’s no point in the proxy trying to actually make a connection to the service - it already knows that it can’t talk to it - so it reports that the service is in failfast and immediately returns an error from the proxy.\nThe error will be either a 503 or a 504; see below for more information, but if you already know that the service is in failfast because you saw it in the logs, that’s the important part.\nTo get out of failfast, some endpoints for the service have to become available.\n","description":"Failfast means that no endpoints are available.","keywords":null,"section":"2.16","title":"Failfast","uri":"https://travisbeckham.github.io/2.16/common-errors/failfast/"},{"ancestors":["Docs","Features"],"content":"Fault injection is a form of chaos engineering where the error rate of a service is artificially increased to see what impact there is on the system as a whole. Traditionally, this would require modifying the service’s code to add a fault injection library that would be doing the actual work. Linkerd can do this without any service code changes, only requiring a little configuration.\nTo inject faults into your own services, follow the tutorial.\n","description":"Linkerd provides mechanisms to programmatically inject failures into services.","keywords":null,"section":"2.16","title":"Fault Injection","uri":"https://travisbeckham.github.io/2.16/features/fault-injection/"},{"ancestors":["Docs","Tasks"],"content":"In order to support mTLS connections between meshed pods, Linkerd needs a trust anchor certificate and an issuer certificate with its corresponding key.\nWhen installing with linkerd install, these certificates are automatically generated. Alternatively, you can specify your own with the --identity-* flags (see the linkerd install reference).\nOn the other hand when using Helm to install Linkerd, it’s not possible to automatically generate them and you’re required to provide them.\nYou can generate these certificates using a tool like openssl or step. All certificates must use the ECDSA P-256 algorithm which is the default for step. To generate ECDSA P-256 certificates with openssl, you can use the openssl ecparam -name prime256v1 command. In this tutorial, we’ll walk you through how to to use the step CLI to do this.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Generating the certificates with step Trust anchor certificate First generate the root certificate with its private key (using step version 0.10.1):\nstep certificate create root.linkerd.cluster.local ca.crt ca.key \\ --profile root-ca --no-password --insecure This generates the ca.crt and ca.key files. The ca.crt file is what you need to pass to the --identity-trust-anchors-file option when installing Linkerd with the CLI, and the identityTrustAnchorsPEM value when installing the linkerd-control-plane chart with Helm.\nNote we use --no-password --insecure to avoid encrypting those files with a passphrase.\nFor a longer-lived trust anchor certificate, pass the --not-after argument to the step command with the desired value (e.g. --not-after=87600h).\nIssuer certificate and key Then generate the intermediate certificate and key pair that will be used to sign the Linkerd proxies’ CSR.\nstep certificate create identity.linkerd.cluster.local issuer.crt issuer.key \\ --profile intermediate-ca --not-after 8760h --no-password --insecure \\ --ca ca.crt --ca-key ca.key This will generate the issuer.crt and issuer.key files.\nPassing the certificates to Linkerd You can finally provide these files when installing Linkerd with the CLI:\n# first, install the Linkerd CRDs linkerd install --crds | kubectl apply -f - # install the Linkerd control plane, with the certificates we just generated. linkerd install \\ --identity-trust-anchors-file ca.crt \\ --identity-issuer-certificate-file issuer.crt \\ --identity-issuer-key-file issuer.key \\ | kubectl apply -f - Or when installing with Helm, first install the linkerd-crds chart:\nhelm install linkerd-crds linkerd/linkerd-crds -n linkerd --create-namespace Then install the linkerd-control-plane chart:\nhelm install linkerd-control-plane -n linkerd \\ --set-file identityTrustAnchorsPEM=ca.crt \\ --set-file identity.issuer.tls.crtPEM=issuer.crt \\ --set-file …","description":"Generate your own mTLS root certificate instead of letting Linkerd do it for you.","keywords":null,"section":"2.16","title":"Generating your own mTLS root certificates","uri":"https://travisbeckham.github.io/2.16/tasks/generate-certificates/"},{"ancestors":["Docs","Tasks"],"content":"To get per-route metrics, you must create HTTPRoute resources. If a route has a parent_ref which points to a Service resource, Linkerd will generate outbound per-route traffic metrics for all HTTP traffic that it sends to that Service. If a route has a parent_ref which points to a Server resource, Linkerd will generate inbound per-route traffic metrcs for all HTTP traffic that it receives on that Server. Note that an HTTPRoute can have multiple parent_refs which means that the same HTTPRoute resource can be used to describe both outbound and inbound routes.\nFor a tutorial that shows off per-route metrics, check out the books demo.\nNote Routes configured in service profiles are different from [HTTPRoute] resources. If a ServiceProfile is defined for a Service, proxies will ignore any [HTTPRoute] for that Service. ","description":"Configure per-route metrics for your application.","keywords":null,"section":"2.16","title":"Getting Per-Route Metrics","uri":"https://travisbeckham.github.io/2.16/tasks/getting-per-route-metrics/"},{"ancestors":["Docs","Tasks"],"content":"Service Mesh Interface is a standard interface for service meshes on Kubernetes. It defines a set of resources that could be used across service meshes that implement it. You can read more about it in the specification\nCurrently, Linkerd supports SMI’s TrafficSplit specification which can be used to perform traffic splitting across services natively. This means that you can apply the SMI resources without any additional components/configuration but this obviously has some downsides, as Linkerd may not be able to add extra specific configurations specific to it, as SMI is more like a lowest common denominator of service mesh functionality.\nTo get around these problems, Linkerd can instead have an adaptor that converts SMI specifications into native Linkerd configurations that it can understand and perform the operation. This also removes the extra native coupling with SMI resources with the control-plane, and the adaptor can move independently and have it’s own release cycle. Linkerd SMI is an extension that does just that.\nThis guide will walk you through installing the SMI extension and configuring a TrafficSplit specification, to perform Traffic Splitting across services.\nPrerequisites To use this guide, you’ll need to have Linkerd installed on your cluster. Follow the Installing Linkerd Guide if you haven’t already done this. Install the Linkerd-SMI extension CLI Install the SMI extension CLI binary by running:\ncurl -sL https://linkerd.github.io/linkerd-smi/install | sh Alternatively, you can download the CLI directly via the releases page.\nThe first step is installing the Linkerd-SMI extension onto your cluster. This extension consists of a SMI-Adaptor which converts SMI resources into native Linkerd resources.\nTo install the Linkerd-SMI extension, run the command:\nlinkerd smi install | kubectl apply -f - You can verify that the Linkerd-SMI extension was installed correctly by running:\nlinkerd smi check Helm To install the linkerd-smi Helm chart, run:\nhelm repo add l5d-smi https://linkerd.github.io/linkerd-smi helm install l5d-smi/linkerd-smi --generate-name Install Sample Application First, let’s install the sample application.\n# create a namespace for the sample application kubectl create namespace trafficsplit-sample # install the sample application linkerd inject https://raw.githubusercontent.com/linkerd/linkerd2/main/test/integration/viz/trafficsplit/testdata/application.yaml | kubectl -n trafficsplit-sample apply -f - This installs a simple client, and two server deployments. One of the server deployments i.e failing-svc always returns a 500 error, and the other one i.e backend-svc always returns a 200.\nkubectl get deployments -n trafficsplit-sample NAME READY UP-TO-DATE AVAILABLE AGE backend 1/1 1 1 2m29s failing 1/1 1 1 2m29s slow-cooker 1/1 1 1 2m29s By default, the client will hit the backend-svcservice. This is evident by the edges sub command.\nlinkerd viz edges deploy -n trafficsplit-sample SRC DST SRC_NS DST_NS SECURED prometheus …","description":"Use Linkerd SMI extension to work with Service Mesh Interface(SMI) resources.","keywords":null,"section":"2.16","title":"Getting started with Linkerd SMI extension","uri":"https://travisbeckham.github.io/2.16/tasks/linkerd-smi/"},{"ancestors":["Docs","Tasks"],"content":"When Kubernetes begins to terminate a pod, it starts by sending all containers in that pod a TERM signal. When the Linkerd proxy sidecar receives this signal, it will immediately begin a graceful shutdown where it refuses all new requests and allows existing requests to complete before shutting down.\nThis means that if the pod’s main container attempts to make any new network calls after the proxy has received the TERM signal, those network calls will fail. This also has implications for clients of the terminating pod and for job resources.\nGraceful shutdown in Kubernetes Pods are ephemeral in nature, and may be killed due to a number of different reasons, such as:\nBeing scheduled on a node that fails (in which case the pod will be deleted). A lack of resources on the node where the pod is scheduled (in which case the pod is evicted). Manual deletion, e.g through kubectl delete. Since pods fundamentally represent processes running on nodes in a cluster, it is important to ensure that when killed, they have enough time to clean-up and terminate gracefully. When a pod is deleted, the container runtime will send a TERM signal to each container running in the pod.\nBy default, Kubernetes will wait 30 seconds to allow processes to handle the TERM signal. This is known as the grace period within which a process may shut itself down gracefully. If the grace period time runs out, and the process hasn’t gracefully exited, the container runtime will send a KILL signal, abruptly stopping the process. Grace periods may be overridden at a workload level. This is useful when a process needs additional time to clean-up (e.g making network calls, writing to disk, etc.)\nKubernetes also allows operators of services to define lifecycle hooks for their containers. Important in the context of graceful shutdown is the preStop hook, that will be called when a container is terminated due to:\nAn API request. Liveness/Readiness probe failure. Resource contention. If a pod has a preStop hook for a container, and the pod receives a TERM signal from the container runtime, the preStop hook will be executed, and it must finish before the TERM signal can be propagated to the container itself. It is worth noting in this case that the grace period will start when the preStop hook is executed, not when the container first starts processing the TERM signal.\nConfiguration options for graceful shutdown Linkerd offers a few options to configure pods and containers to gracefully shutdown.\n--wait-before-seconds: can be used as an install value (either through the CLI or through Helm), or alternatively, through a configuration annotation. This will add a preStop hook to the proxy container to delay its handling of the TERM signal. This will only work when the conditions described above are satisfied (i.e container runtime sends the TERM signal) config.linkerd.io/shutdown-grace-period: is an annotation that can be used on workloads to configure the graceful shutdown time for the proxy. If …","description":"Gracefully handle pod shutdown signal.","keywords":null,"section":"2.16","title":"Graceful Pod Shutdown","uri":"https://travisbeckham.github.io/2.16/tasks/graceful-shutdown/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd provides a full on-cluster metrics stack that can be leveraged by a Prometheus instance and subsequently by a Grafana instance, in order to show both the real-time and historical behavior of these metrics.\nFirst, you need to install Grafana from a variety of possible sources, and then load the suite of Grafana dashboards that have been pre-configured to consume the metrics exposed by Linkerd.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Install Prometheus Before installing Grafana, make sure you have a working instance of Prometheus properly configured to consume Linkerd metrics. The Linkerd Viz extension comes with such a pre-configured Prometheus instance, but you can also bring your own Prometheus.\nInstall Grafana The easiest and recommended way is to install Grafana’s official Helm chart:\nhelm repo add grafana https://grafana.github.io/helm-charts helm install grafana -n grafana --create-namespace grafana/grafana \\ -f https://raw.githubusercontent.com/linkerd/linkerd2/main/grafana/values.yaml This is fed the default values.yaml file, which configures as a default datasource Linkerd Viz’ Prometheus instance, sets up a reverse proxy (more on that later), and pre-loads all the Linkerd Grafana dashboards that are published on https://grafana.com/orgs/linkerd.\nNote The access to Linkerd Viz’ Prometheus instance is restricted through the prometheus-admin AuthorizationPolicy, granting access only to the metrics-api ServiceAccount. In order to also grant access to Grafana, you need to add an AuthorizationPolicy pointing to its ServiceAccount. You can apply authzpolicy-grafana.yaml which grants permission for the grafana ServiceAccount. A more complex and production-oriented source is the Grafana Operator. And there are also hosted solutions such as Grafana Cloud. Those projects provide instructions on how to easily import the same charts published on https://grafana.com/orgs/linkerd.\nNote Grafana’s official Helm chart uses an initContainer to download Linkerd’s configuration and dashboards. If you use the CNI plugin, when you add grafana’s pod into the mesh its initContainer will run before the proxy is started and the traffic cannot flow. You should either avoid meshing grafana’s pod, skip outbound port 443 via config.linkerd.io/skip-outbound-ports: \u0026#34;443\u0026#34; annotation or run the container with the proxy’s UID. See Allowing initContainer networking Hook Grafana with Linkerd Viz Dashboard It’s easy to configure Linkerd Viz dashboard and Grafana such that the former displays Grafana icons in all the relevant items, providing direct links to the appropriate Grafana Dashboards. For example, when looking at a list of deployments for a given namespace, you’ll be able to go straight into the Linkerd Deployments Grafana dashboard …","description":"Grafana install instructions and how to link it with the Linkerd Dashboard","keywords":null,"section":"2.16","title":"Grafana","uri":"https://travisbeckham.github.io/2.16/tasks/grafana/"},{"ancestors":["Docs","Tasks"],"content":"Ingress traffic refers to traffic that comes into your cluster from outside the cluster. For reasons of simplicity and composability, Linkerd itself doesn’t provide a built-in ingress solution for handling traffic coming into the cluster. Instead, Linkerd is designed to work with the many existing Kubernetes ingress options.\nCombining Linkerd and your ingress solution of choice requires two things:\nConfiguring your ingress to support Linkerd (if necessary). Meshing your ingress pods. Strictly speaking, meshing your ingress pods is not required to allow traffic into the cluster. However, it is recommended, as it allows Linkerd to provide features like L7 metrics and mutual TLS the moment the traffic enters the cluster.\nHandling external TLS One common job for ingress controllers is to terminate TLS from the outside world, e.g. HTTPS calls.\nLike all pods, traffic to a meshed ingress has both an inbound and an outbound component. If your ingress terminates TLS, Linkerd will treat this inbound TLS traffic as an opaque TCP stream, and will only be able to provide byte-level metrics for this side of the connection.\nOnce the ingress controller terminates the TLS connection and issues the corresponding HTTP or gRPC traffic to internal services, these outbound calls will have the full set of metrics and mTLS support.\nIngress mode Most ingress controllers can be meshed like any other service, i.e. by applying the linkerd.io/inject: enabled annotation at the appropriate level. (See Adding your services to Linkerd for more.)\nHowever, some ingress options need to be meshed in a special “ingress” mode, using the linkerd.io/inject: ingress annotation.\nThe instructions below will describe, for each ingress, whether it requires this mode of operation.\nIf you’re using “ingress” mode, we recommend that you set this ingress annotation at the workload level rather than at the namespace level, so that other resources in the ingress namespace are be meshed normally.\nWarning When an ingress is meshed in ingress mode, you must configure it to remove the l5d-dst-override header to avoid creating an open relay to cluster-local and external endpoints. Note Linkerd versions 2.13.0 through 2.13.4 had a bug whereby the l5d-dst-override header was required in ingress mode, or the request would fail. This bug was fixed in 2.13.5, and was not present prior to 2.13.0. Note Be sure to not deploy the ingress controller in the kube-system or cert-manager namespace, as Linkerd ignores these namespaces by default for injection. For more on ingress mode and why it’s necessary, see Ingress details below.\nCommon ingress options for Linkerd Common ingress options that Linkerd has been used with include:\nAmbassador (aka Emissary) Nginx (community version) Nginx (F5 NGINX version) Traefik Traefik 1.x Traefik 2.x GCE Gloo Contour Kong Haproxy EnRoute ngrok For a quick start guide to using a particular ingress, please visit the section for that ingress below. If your ingress is not on that …","description":"Linkerd can work alongside your ingress controller of choice.","keywords":null,"section":"2.16","title":"Handling ingress traffic","uri":"https://travisbeckham.github.io/2.16/tasks/using-ingress/"},{"ancestors":["Docs","Features"],"content":"For production workloads, Linkerd’s control plane can run in high availability (HA) mode. This mode:\nRuns three replicas of critical control plane components. Sets production-ready CPU and memory resource requests on control plane components. Sets production-ready CPU and memory resource requests on data plane proxies Requires that the proxy auto-injector be functional for any pods to be scheduled. Sets anti-affinity policies on critical control plane components to ensure, if possible, that they are scheduled on separate nodes and in separate zones by default. Enabling HA You can enable HA mode at control plane installation time with the --ha flag:\nlinkerd install --ha | kubectl apply -f - Also note the Viz extension also supports an --ha flag with similar characteristics:\nlinkerd viz install --ha | kubectl apply -f - You can override certain aspects of the HA behavior at installation time by passing other flags to the install command. For example, you can override the number of replicas for critical components with the --controller-replicas flag:\nlinkerd install --ha --controller-replicas=2 | kubectl apply -f - See the full install CLI documentation for reference.\nThe linkerd upgrade command can be used to enable HA mode on an existing control plane:\nlinkerd upgrade --ha | kubectl apply -f - Proxy injector failure policy The HA proxy injector is deployed with a stricter failure policy to enforce automatic proxy injection. This setup ensures that no annotated workloads are accidentally scheduled to run on your cluster, without the Linkerd proxy. (This can happen when the proxy injector is down.)\nIf proxy injection process failed due to unrecognized or timeout errors during the admission phase, the workload admission will be rejected by the Kubernetes API server, and the deployment will fail.\nHence, it is very important that there is always at least one healthy replica of the proxy injector running on your cluster.\nIf you cannot guarantee the number of healthy proxy injector on your cluster, you can loosen the webhook failure policy by setting its value to Ignore, as seen in the Linkerd Helm chart.\nNote See the Kubernetes documentation for more information on the admission webhook failure policy. Pod anti-affinity rules All critical control plane components are deployed with pod anti-affinity rules to ensure redundancy.\nLinkerd uses a requiredDuringSchedulingIgnoredDuringExecution pod anti-affinity rule to ensure that the Kubernetes scheduler does not colocate replicas of critical component on the same node. A preferredDuringSchedulingIgnoredDuringExecution pod anti-affinity rule is also added to try to schedule replicas in different zones, where possible.\nIn order to satisfy these anti-affinity rules, HA mode assumes that there are always at least three nodes in the Kubernetes cluster. If this assumption is violated (e.g. the cluster is scaled down to two or fewer nodes), then the system may be left in a non-functional state.\nNote that these …","description":"The Linkerd control plane can run in high availability (HA) mode.","keywords":null,"section":"2.16","title":"High Availability","uri":"https://travisbeckham.github.io/2.16/features/ha/"},{"ancestors":["Docs","Common Errors"],"content":"The Linkerd proxy will return a 502 error for connection errors between proxies. Unfortunately it’s fairly common to see an uptick in 502s when first meshing a workload that hasn’t previously been used with a mesh, because the mesh surfaces errors that were previously invisible!\nThere’s actually a whole page on debugging 502s.\n","description":"HTTP 502 means connection errors between proxies.","keywords":null,"section":"2.16","title":"HTTP 502 Errors","uri":"https://travisbeckham.github.io/2.16/common-errors/http-502/"},{"ancestors":["Docs","Common Errors"],"content":"503s and 504s show up when a Linkerd proxy is trying to make so many requests to a workload that it gets overwhelmed.\nWhen the workload next to a proxy makes a request, the proxy adds it to an internal dispatch queue. When things are going smoothly, the request is pulled from the queue and dispatched almost immediately. If the queue gets too long, though (which can generally happen only if the called service is slow to respond), the proxy will go into load-shedding, where any new request gets an immediate 503. The proxy can only get out of load-shedding when the queue shrinks.\nFailfast also plays a role here: if the proxy puts a service into failfast while there are requests in the dispatch queue, all the requests in the dispatch queue get an immediate 504 before the proxy goes into load-shedding.\nTo get out of failfast, some endpoints for the service have to become available.\nTo get out of load-shedding, the dispatch queue has to start emptying, which implies that the service has to get more capacity to process requests or that the incoming request rate has to drop.\n","description":"HTTP 503 and 504 mean overloaded workloads.","keywords":null,"section":"2.16","title":"HTTP 503 and 504 Errors","uri":"https://travisbeckham.github.io/2.16/common-errors/http-503-504/"},{"ancestors":["Docs","Features"],"content":"Linkerd proxies can be configured to generate an HTTP access log that records all HTTP requests that transit the proxy.\nThe config.linkerd.io/access-log annotation is used to enable proxy HTTP access logging. Adding this annotation to a namespace or workload configures the proxy injector to set an environment variable in the proxy container that configures access logging.\nHTTP access logging is disabled by default because it has a performance impact, compared to proxies without access logging enabled. Enabling access logging may increase tail latency and CPU consumption under load. The severity of this performance cost may vary depending on the traffic being proxied, and may be acceptable in some environments.\nNote The proxy’s HTTP access log is distinct from proxy debug logging, which is configured separately. See the documentation on modifying the proxy log level for details on configuring the proxy’s debug logging. Access Log Formats The value of the config.linkerd.io/access-log annotation determines the format of HTTP access log entries, and can be either “apache” or “json”.\nSetting the config.linkerd.io/access-log: \u0026#34;apache\u0026#34; annotation configures the proxy to emit HTTP access logs in the Apache Common Log Format. For example:\n10.42.0.63:51160 traffic.booksapp.serviceaccount.identity.linkerd.cluster.local - [2022-08-23T20:28:20.071809491Z] \u0026#34;GET http://webapp:7000/ HTTP/2.0\u0026#34; 200 10.42.0.63:51160 traffic.booksapp.serviceaccount.identity.linkerd.cluster.local - [2022-08-23T20:28:20.187706137Z] \u0026#34;POST http://webapp:7000/authors HTTP/2.0\u0026#34; 303 10.42.0.63:51160 traffic.booksapp.serviceaccount.identity.linkerd.cluster.local - [2022-08-23T20:28:20.301798187Z] \u0026#34;GET http://webapp:7000/authors/104 HTTP/2.0\u0026#34; 200 10.42.0.63:51160 traffic.booksapp.serviceaccount.identity.linkerd.cluster.local - [2022-08-23T20:28:20.409177224Z] \u0026#34;POST http://webapp:7000/books HTTP/2.0\u0026#34; 303 10.42.0.1:43682 - - [2022-08-23T20:28:23.049685223Z] \u0026#34;GET /ping HTTP/1.1\u0026#34; 200 Setting the config.linkerd.io/access-log: json annotation configures the proxy to emit access logs in a JSON format. For example:\n{\u0026#34;client.addr\u0026#34;:\u0026#34;10.42.0.70:32996\u0026#34;,\u0026#34;client.id\u0026#34;:\u0026#34;traffic.booksapp.serviceaccount.identity.linkerd.cluster.local\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;webapp:7000\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;processing_ns\u0026#34;:\u0026#34;39826\u0026#34;,\u0026#34;request_bytes\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;response_bytes\u0026#34;:\u0026#34;19627\u0026#34;,\u0026#34;status\u0026#34;:200,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-08-23T20:33:42.321746212Z\u0026#34;,\u0026#34;total_ns\u0026#34;:\u0026#34;14441135\u0026#34;,\u0026#34;trace_id\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;uri\u0026#34;:\u0026#34;http://webapp:7000/\u0026#34;,\u0026#34;user_agent\u0026#34;:\u0026#34;Go-http-client/1.1\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;HTTP/2.0\u0026#34;} {\u0026#34;client.addr\u0026#34;:\u0026#34;10.42.0.70:32996\u0026#34;,\u0026#34;client.id\u0026#34;:\u0026#34;traffic.booksapp.serviceaccount.identity.linkerd.cluster.local\u0026#34;,\u0026#34;host\u0026#34;:\u0026#34;webapp:7000\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;POST\u0026#34;,\u0026#34;processing_ns\u0026#34;:\u0026#34;30036\u0026#34;,\u0026#34;request_bytes\u0026#34;:\u0026#34;33\u0026#34;,\u0026#34;response_bytes\u0026#34;:\u0026#34;0\u0026#34;,\u0026#34;status\u0026#34;:303,\u0026#34;timestamp\u0026#34;:\u0026#34;2022-08-23T20:33:42.436964052Z\u0026#34;,\u0026#34;total_ns\u0026#34;:\u0026#34;14122403\u0026#34;,\u0026#34;trace_id\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;uri\u0026#34;:\u0026#34;http://webapp:7000/authors\u0026#34;,\u0026#34;user_agent\u0026#34;:\u0026#34;Go-http-client/1.1\u0026#34;,\u0026#34;version\u0026#34;:\u0026#34;HTTP/2.0\u0026#34;} …","description":"Linkerd proxies can be configured to emit HTTP access logs.","keywords":null,"section":"2.16","title":"HTTP Access Logging","uri":"https://travisbeckham.github.io/2.16/features/access-logging/"},{"ancestors":["Docs","Reference"],"content":" Linkerd and Gateway API HTTPRoutes The HTTPRoute resource was originally specified by the Kubernetes Gateway API project. Linkerd currently supports two versions of the HTTPRoute resource: the upstream version from the Gateway API, with the gateway.networking.kubernetes.io API group, and a Linkerd-specific version, with the policy.linkerd.io API group. While these two resource definitions are largely the same, the policy.linkerd.io HTTPRoute resource is an experimental version that contains features not yet stabilized in the upstream gateway.networking.k8s.io HTTPRoute resource, such as timeouts. Both the Linkerd and Gateway API resource definitions coexist within the same cluster, and both can be used to configure policies for use with Linkerd.\nIf the Gateway API CRDs already exist in your cluster, then Linkerd must be installed with the --set enableHttpRoutes=false flag during the linkerd install --crds step or with the enableHttpRoutes=false Helm value when installing the linkerd-crds Helm chart. This avoid conflicts by instructing Linkerd to not install the Gateway API CRDs and instead rely on the Gateway CRDs which already exist.\nThis documentation describes the policy.linkerd.io HTTPRoute resource. For a similar description of the upstream Gateway API HTTPRoute resource, refer to the Gateway API’s HTTPRoute specification.\nHTTPRoute Spec An HTTPRoute spec may contain the following top level fields:\nfield value parentRefs A set of ParentReferences which indicate which Servers or Services this HTTPRoute attaches to. hostnames A set of hostnames that should match against the HTTP Host header. rules An array of HTTPRouteRules. parentReference A reference to the parent resource this HTTPRoute is a part of.\nHTTPRoutes can be attached to a Server to allow defining an authorization policy for specific routes served on that Server.\nHTTPRoutes can also be attached to a Service, in order to route requests depending on path, headers, query params, and/or verb. Requests can then be rerouted to different backend services. This can be used to perform dynamic request routing.\nWarning Outbound HTTPRoutes and ServiceProfiles provide overlapping configuration. For backwards-compatibility reasons, a ServiceProfile will take precedence over HTTPRoutes which configure the same Service. If a ServiceProfile is defined for the parent Service of an HTTPRoute, proxies will use the ServiceProfile configuration, rather than the HTTPRoute configuration, as long as the ServiceProfile exists. ParentReferences are namespaced, and may reference either a parent in the same namespace as the HTTPRoute, or one in a different namespace. As described in [GEP-1426][ns-boundaries], a HTTPRoute with a parentRef that references a Service in the same namespace as the HTTPRoute is referred to as a producer route, while an HTTPRoute with a parentRef referencing a Service in a different namespace is referred to as a consumer route. A producer route will apply to requests originating from …","description":"Reference guide to HTTPRoute resources.","keywords":null,"section":"2.16","title":"HTTPRoute","uri":"https://travisbeckham.github.io/2.16/reference/httproute/"},{"ancestors":["Docs","Features"],"content":"To configure routing behavior and policy for HTTP traffic, Linkerd supports the HTTPRoute resource, defined by the Kubernetes Gateway API.\nNote Two versions of the HTTPRoute resource may be used with Linkerd:\nThe upstream version provided by the Gateway API, with the gateway.networking.k8s.io API group A Linkerd-specific CRD provided by Linkerd, with the policy.linkerd.io API group The two HTTPRoute resource definitions are similar, but the Linkerd version implements experimental features not yet available with the upstream Gateway API resource definition. See the HTTPRoute reference documentation for details.\nIf the Gateway API CRDs already exist in your cluster, then Linkerd must be installed with the --set enableHttpRoutes=false flag during the linkerd install --crds step or with the enableHttpRoutes=false Helm value when installing the linkerd-crds Helm chart. This avoid conflicts by instructing Linkerd to not install the Gateway API CRDs and instead rely on the Gateway CRDs which already exist.\nAn HTTPRoute is a Kubernetes resource which attaches to a parent resource, such as a Service. The HTTPRoute defines a set of rules which match HTTP requests to that resource, based on parameters such as the request’s path, method, and headers, and can configure how requests matching that rule are routed by the Linkerd service mesh.\nInbound and Outbound HTTPRoutes Two types of HTTPRoute are used for configuring the behavior of Linkerd’s proxies:\nHTTPRoutes with a Service as their parent resource configure policies for outbound proxies in pods which are clients of that Service. Outbound policy includes dynamic request routing, adding request headers, modifying a request’s path, and reliability features such as timeouts. HTTPRoutes with a Server as their parent resource configure policy for inbound proxies in pods which recieve traffic to that Server. Inbound HTTPRoutes are used to configure fine-grained per-route authorization and authentication policies. Warning Outbound HTTPRoutes and ServiceProfiles provide overlapping configuration. For backwards-compatibility reasons, a ServiceProfile will take precedence over HTTPRoutes which configure the same Service. If a ServiceProfile is defined for the parent Service of an HTTPRoute, proxies will use the ServiceProfile configuration, rather than the HTTPRoute configuration, as long as the ServiceProfile exists. Learn More To get started with HTTPRoutes, you can:\nConfigure fault injection using an outbound HTTPRoute. Configure timeouts using an outbound HTTPRoute. Configure dynamic request routing using an outbound HTTPRoute. Configure per-route authorization policy using an inbound HTTPRoute. See the reference documentation for a complete description of the HTTPRoute resource. ","description":"Linkerd can use the HTTPRoute resource to configure per-route policies.","keywords":null,"section":"2.16","title":"HTTPRoutes","uri":"https://travisbeckham.github.io/2.16/features/httproute/"},{"ancestors":["Docs","Reference","CLI"],"content":" Display the certificate(s) of one or more selected pod(s).\nThis command initiates a port-forward to a given pod or a set of pods and fetches the TLS certificate.\nExamples # Get certificate from pod foo-bar in the default namespace. linkerd identity foo-bar # Get certificate from all pods with the label name=nginx linkerd identity -l name=nginx Flags Flag Usage --namespace -n Namespace of the pod --selector -l Selector (label query) to filter on, supports ‘=’, ‘==’, and ‘!=’ ","description":"","keywords":null,"section":"2.16","title":"identity","uri":"https://travisbeckham.github.io/2.16/reference/cli/identity/"},{"ancestors":["Docs","Reference","CLI"],"content":"The inject command is a text transform that modifies Kubernetes manifests passed to it either as a file or as a stream (-) to adds a linkerd.io/inject: enabled annotation to eligible resources in the manifest. When the resulting annotated manifest is applied to the Kubernetes cluster, Linkerd’s proxy autoinjector automatically adds the Linkerd data plane proxies to the corresponding pods.\nNote that there is no a priori reason to use this command. In production, these annotations may be instead set by a CI/CD system, or any other deploy-time mechanism.\nManual injection Alternatively, this command can also perform the full injection purely on the client side, by enabling with the --manual flag. (Prior to Linkerd 2.4, this was the default behavior.)\nExamples # Inject all the deployments in the default namespace. kubectl get deploy -o yaml | linkerd inject - | kubectl apply -f - # Injecting a file from a remote URL linkerd inject https://url.to/yml | kubectl apply -f - # Inject all the resources inside a folder and its sub-folders. linkerd inject \u0026lt;folder\u0026gt; | kubectl apply -f - Flags Flag Usage --admin-port Proxy port to serve metrics on --close-wait-timeout Sets nf_conntrack_tcp_timeout_close_wait --control-port Proxy port to use for control --default-inbound-policy Inbound policy to use to control inbound access to the proxy --disable-identity Disables resources from participating in TLS identity --enable-debug-sidecar Inject a debug sidecar for data plane debugging --enable-external-profiles Enable service profiles for non-Kubernetes services --ignore-cluster Ignore the current Kubernetes cluster when checking for existing cluster configuration (default false) --image-pull-policy Docker image pull policy --inbound-port Proxy port to use for inbound traffic --ingress Enable ingress mode in the linkerd proxy --init-image Linkerd init container image name --init-image-version Linkerd init container image version --manual Include the proxy sidecar container spec in the YAML output (the auto-injector won’t pick it up, so config annotations aren’t supported) (default false) --native-sidecar Enable native sidecar --opaque-ports Set opaque ports on the proxy --outbound-port Proxy port to use for outbound traffic --output -o Output format, one of: json|yaml --proxy-cpu Amount of CPU units that the proxy sidecar requests --proxy-cpu-limit Maximum amount of CPU units that the proxy sidecar can use --proxy-cpu-request Amount of CPU units that the proxy sidecar requests --proxy-gid Run the proxy under this group ID --proxy-image Linkerd proxy container image name --proxy-log-level Log level for the proxy --proxy-memory Amount of Memory that the proxy sidecar requests --proxy-memory-limit Maximum amount of Memory that the proxy sidecar can use --proxy-memory-request Amount of Memory that the proxy sidecar requests --proxy-uid Run the proxy under this user ID --proxy-version -v Tag to be used for the Linkerd proxy images --registry Docker registry to pull images …","description":"","keywords":null,"section":"2.16","title":"inject","uri":"https://travisbeckham.github.io/2.16/reference/cli/inject/"},{"ancestors":["Docs","Tasks"],"content":"It is easy to inject failures into applications by using the HTTPRoute resource to redirect a percentage of traffic to a specific backend. This backend is completely flexible and can return whatever responses you want - 500s, timeouts or even crazy payloads.\nThe books demo is a great way to show off this behavior. The overall topology looks like:\nTopology In this guide, you will split some of the requests from webapp to books. Most requests will end up at the correct books destination, however some of them will be redirected to a faulty backend. This backend will return 500s for every request and inject faults into the webapp service. No code changes are required and as this method is configuration driven, it is a process that can be added to integration tests and CI pipelines. If you are really living the chaos engineering lifestyle, fault injection could even be used in production.\nPrerequisites To use this guide, you’ll need a Kubernetes cluster running:\nLinkerd and Linkerd-Viz. If you haven’t installed these yet, follow the Installing Linkerd Guide. Setup the service First, add the books sample application to your cluster:\nkubectl create ns booksapp \u0026amp;\u0026amp; \\ linkerd inject https://run.linkerd.io/booksapp.yml | \\ kubectl -n booksapp apply -f - As this manifest is used as a demo elsewhere, it has been configured with an error rate. To show how fault injection works, the error rate needs to be removed so that there is a reliable baseline. To increase success rate for booksapp to 100%, run:\nkubectl -n booksapp patch deploy authors \\ --type=\u0026#39;json\u0026#39; \\ -p=\u0026#39;[{\u0026#34;op\u0026#34;:\u0026#34;remove\u0026#34;, \u0026#34;path\u0026#34;:\u0026#34;/spec/template/spec/containers/0/env/2\u0026#34;}]\u0026#39; After a little while, the stats will show 100% success rate. You can verify this by running:\nlinkerd viz -n booksapp stat deploy The output will end up looking at little like:\nNAME MESHED SUCCESS RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 TCP_CONN authors 1/1 100.00% 7.1rps 4ms 26ms 33ms 6 books 1/1 100.00% 8.6rps 6ms 73ms 95ms 6 traffic 1/1 - - - - - - webapp 3/3 100.00% 7.9rps 20ms 76ms 95ms 9 Create the faulty backend Injecting faults into booksapp requires a service that is configured to return errors. To do this, you can start NGINX and configure it to return 500s by running:\ncat \u0026lt;\u0026lt;EOF | linkerd inject - | kubectl apply -f - apiVersion: v1 kind: ConfigMap metadata: name: error-injector namespace: booksapp data: nginx.conf: |- events {} http { server { listen 8080; location / { return 500; } } } --- apiVersion: apps/v1 kind: Deployment metadata: name: error-injector namespace: booksapp labels: app: error-injector spec: selector: matchLabels: app: error-injector replicas: 1 template: metadata: labels: app: error-injector spec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: nginx-config mountPath: /etc/nginx/nginx.conf subPath: nginx.conf volumes: - name: nginx-config configMap: name: error-injector --- apiVersion: v1 kind: Service metadata: name: error-injector namespace: booksapp spec: ports: - name: service port: …","description":"Practice chaos engineering by injecting faults into services with Linkerd.","keywords":null,"section":"2.16","title":"Injecting Faults","uri":"https://travisbeckham.github.io/2.16/tasks/fault-injection/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output Kubernetes configs to install Linkerd.\nThis command provides all Kubernetes configs necessary to install the Linkerd control plane.\nFor further details on how to install Linkerd onto your own cluster, check out the install documentation.\nExamples # Install CRDs first. linkerd install --crds | kubectl apply -f - # Install the core control plane. linkerd install | kubectl apply -f - The installation can be configured by using the --set, --values, --set-string and --set-file flags. A full list of configurable values can be found at https://artifacthub.io/packages/helm/linkerd2/linkerd-control-plane#values Flags Flag Usage --admin-port Proxy port to serve metrics on --cluster-domain Set custom cluster domain --control-plane-tracing Enables Control Plane Tracing with the defaults --control-plane-tracing-namespace Send control plane traces to Linkerd-Jaeger extension in this namespace --control-port Proxy port to use for control --controller-gid Run the control plane components under this group ID --controller-log-level Log level for the controller and web components --controller-replicas Replicas of the controller to deploy --controller-uid Run the control plane components under this user ID --crds Install Linkerd CRDs --default-inbound-policy Inbound policy to use to control inbound access to the proxy --disable-h2-upgrade Prevents the controller from instructing proxies to perform transparent HTTP/2 upgrading (default false) --disable-heartbeat Disables the heartbeat cronjob (default false) --enable-endpoint-slices Enables the usage of EndpointSlice informers and resources for destination service --enable-external-profiles Enable service profiles for non-Kubernetes services --ha Enable HA deployment config for the control plane (default false) --identity-clock-skew-allowance The amount of time to allow for clock skew within a Linkerd cluster --identity-external-ca Whether to use an external CA provider (default false) --identity-external-issuer Whether to use an external identity issuer (default false) --identity-issuance-lifetime The amount of time for which the Identity issuer should certify identity --identity-issuer-certificate-file A path to a PEM-encoded file containing the Linkerd Identity issuer certificate (generated by default) --identity-issuer-key-file A path to a PEM-encoded file containing the Linkerd Identity issuer private key (generated by default) --identity-trust-anchors-file A path to a PEM-encoded file containing Linkerd Identity trust anchors (generated by default) --identity-trust-domain Configures the name suffix used for identities. --ignore-cluster Ignore the current Kubernetes cluster when checking for existing cluster configuration (default false) --image-pull-policy Docker image pull policy --inbound-port Proxy port to use for inbound traffic --init-image Linkerd init container image name --init-image-version Linkerd init container image version --linkerd-cni-enabled Omit the NET_ADMIN capability in the PSP and …","description":"","keywords":null,"section":"2.16","title":"install","uri":"https://travisbeckham.github.io/2.16/reference/cli/install/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output Kubernetes configs to install Linkerd CNI.\nThis command installs a DaemonSet into the Linkerd control plane. The DaemonSet copies the necessary linkerd-cni plugin binaries and configs onto the host. It assumes that the ’linkerd install’ command will be executed with the ‘–linkerd-cni-enabled’ flag. This command needs to be executed before the ’linkerd install –linkerd-cni-enabled’ command.\nThe installation can be configured by using the –set, –values, –set-string and –set-file flags. A full list of configurable values can be found at https://artifacthub.io/packages/helm/linkerd2/linkerd2-cni#values\nFlags Flag Usage --admin-port Proxy port to serve metrics on --cni-image Image for the cni-plugin --cni-image-version Image Version for the cni-plugin --cni-log-level Log level for the cni-plugin --control-port Proxy port to use for control --dest-cni-bin-dir Directory on the host where the CNI binary will be placed --dest-cni-net-dir Directory on the host where the CNI configuration will be placed --inbound-port Proxy port to use for inbound traffic --linkerd-version -v Tag to be used for Linkerd images --outbound-port Proxy port to use for outbound traffic --priority-class-name Pod priorityClassName for CNI daemonset’s pods --proxy-gid Run the proxy under this group ID --proxy-uid Run the proxy under this user ID --redirect-ports Ports to redirect to proxy, if no port is specified then ALL ports are redirected --registry Docker registry to pull images from ($LINKERD_DOCKER_REGISTRY) --set set values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --set-file set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --skip-inbound-ports Ports and/or port ranges (inclusive) that should skip the proxy and send directly to the application --skip-outbound-ports Outbound ports and/or port ranges (inclusive) that should skip the proxy --use-wait-flag Configures the CNI plugin to use the “-w” flag for the iptables command. (default false) --values -f specify values in a YAML file or a URL (can specify multiple) ","description":"","keywords":null,"section":"2.16","title":"install-cni","uri":"https://travisbeckham.github.io/2.16/reference/cli/install-cni/"},{"ancestors":["Docs","Tasks"],"content":"Before you can use Linkerd, you’ll need to install the control plane. This page covers how to accomplish that.\nNote The Linkerd project itself only produces edge release artifacts. (For more information about the different kinds of Linkerd releases, see the Releases and Versions page.)\nAs such, this page contains instructions for installing the latest edge release of Linkerd. If you are using a stable distribution of Linkerd, the vendor should provide additional guidance on installing Linkerd.\nLinkerd’s control plane can be installed in two ways: with the CLI and with Helm. The CLI is convenient and easy, but for production use cases we recommend Helm which allows for repeatability.\nIn either case, we recommend installing the CLI itself so that you can validate the success of the installation. See the Getting Started Guide for how to install the CLI if you haven’t done this already.\nRequirements Linkerd requires a Kubernetes cluster on which to run. Where this cluster lives is not important: it might be hosted on a cloud provider, may be running on your local machine, or even somewhere else.\nMake sure that your Linkerd version and Kubernetes version are compatible by checking Linkerd’s supported Kubernetes versions.\nBefore installing the control plane, validate that this Kubernetes cluster is configured appropriately for Linkerd by running:\nlinkerd check --pre Be sure to address any issues that the checks identify before proceeding.\nNote If installing Linkerd on GKE, there are some extra steps required depending on how your cluster has been configured. If you are using any of these features, check out the additional instructions on GKE private clusters Note If installing Linkerd in a cluster that uses Cilium in kube-proxy replacement mode, additional steps may be needed to ensure service discovery works as intended. Instrunctions are on the Cilium cluster configuration page. Installing with the CLI Once you have a cluster ready, installing Linkerd is as easy as running linkerd install --crds, which installs the Linkerd CRDs, followed by linkerd install, which installs the Linkerd control plane. Both of these commands generate Kubernetes manifests, which can be applied to your cluster to install Linkerd.\nFor example:\n# install the CRDs first linkerd install --crds | kubectl apply -f - # install the Linkerd control plane once the CRDs have been installed linkerd install | kubectl apply -f - This basic installation should work for most cases. However, there are some configuration options are provided as flags for install. See the CLI reference documentation for a complete list of options. You can also use tools like Kustomize to programmatically alter this manifest.\nInstalling via Helm To install Linkerd with Helm (recommended for production installations), see the Installing Linkerd with Helm.\nVerification After installation (whether CLI or Helm) you can validate that Linkerd is in a good state running:\nlinkerd check Next steps Once you’ve …","description":"Install Linkerd onto your Kubernetes cluster.","keywords":null,"section":"2.16","title":"Installing Linkerd","uri":"https://travisbeckham.github.io/2.16/tasks/install/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd can be installed via Helm rather than with the linkerd install command. This is recommended for production, since it allows for repeatability.\nEdge release content This page contains instructions using the latest edge release of Linkerd. For other types of releases, including stable releases, please see Linkerd Releases. Prerequisite: generate mTLS certificates To do automatic mutual TLS, Linkerd requires trust anchor certificate and an issuer certificate and key pair. When you’re using linkerd install, we can generate these for you. However, for Helm, you will need to generate these yourself.\nPlease follow the instructions in Generating your own mTLS root certificates to generate these.\nHelm install procedure # Add the Helm repo for Linkerd edge releases: helm repo add linkerd-edge https://helm.linkerd.io/edge You need to install two separate charts in succession: first linkerd-crds and then linkerd-control-plane.\nNote If installing Linkerd in a cluster that uses Cilium in kube-proxy replacement mode, additional steps may be needed to ensure service discovery works as intended. Instrunctions are on the Cilium cluster configuration page. linkerd-crds The linkerd-crds chart sets up the CRDs linkerd requires:\nhelm install linkerd-crds linkerd-edge/linkerd-crds \\ -n linkerd --create-namespace Note This will create the linkerd namespace. If it already exists or you’re creating it beforehand elsewhere in your pipeline, just omit the --create-namespace flag. Note If you are using Linkerd’s CNI plugin, you must also add the --set cniEnabled=true flag to your helm install command. linkerd-control-plane The linkerd-control-plane chart sets up all the control plane components:\nhelm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM=ca.crt \\ --set-file identity.issuer.tls.crtPEM=issuer.crt \\ --set-file identity.issuer.tls.keyPEM=issuer.key \\ linkerd-edge/linkerd-control-plane Note If you are using Linkerd’s CNI plugin, you must also add the --set cniEnabled=true flag to your helm install command. Enabling high availability mode The linkerd-control-plane chart contains a file values-ha.yaml that overrides some default values to set things up under a high-availability scenario, analogous to the --ha option in linkerd install. Values such as higher number of replicas, higher memory/cpu limits, and affinities are specified in those files.\nYou can get values-ha.yaml by fetching the chart file:\nhelm fetch --untar linkerd-edge/linkerd-control-plane Then use the -f flag to provide this override file. For example:\nhelm install linkerd-control-plane \\ -n linkerd \\ --set-file identityTrustAnchorsPEM=ca.crt \\ --set-file identity.issuer.tls.crtPEM=issuer.crt \\ --set-file identity.issuer.tls.keyPEM=issuer.key \\ -f linkerd-control-plane/values-ha.yaml \\ linkerd-edge/linkerd-control-plane Upgrading with Helm First, make sure your local Helm repos are updated:\nhelm repo update helm search repo linkerd NAME CHART VERSION APP VERSION …","description":"Install Linkerd onto your Kubernetes cluster using Helm.","keywords":null,"section":"2.16","title":"Installing Linkerd with Helm","uri":"https://travisbeckham.github.io/2.16/tasks/install-helm/"},{"ancestors":["Docs","Tasks"],"content":"Multicluster support in Linkerd requires extra installation and configuration on top of the default control plane installation. This guide walks through this installation and configuration as well as common problems that you may encounter. For a detailed walkthrough and explanation of what’s going on, check out getting started.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Requirements Two clusters. A control plane installation in each cluster that shares a common trust anchor. If you have an existing installation, see the trust anchor bundle documentation to understand what is required. Each of these clusters should be configured as kubectl contexts. Elevated privileges on both clusters. We’ll be creating service accounts and granting extended privileges, so you’ll need to be able to do that on your test clusters. Support for services of type LoadBalancer in the east cluster. Check out the documentation for your cluster provider or take a look at inlets. This is what the west cluster will use to communicate with east via the gateway. Step 1: Install the multicluster control plane On each cluster, run:\nlinkerd multicluster install | \\ kubectl apply -f - To verify that everything has started up successfully, run:\nlinkerd multicluster check For a deep dive into what components are being added to your cluster and how all the pieces fit together, check out the getting started documentation.\nStep 2: Link the clusters Each cluster must be linked. This consists of installing several resources in the source cluster including a secret containing a kubeconfig that allows access to the target cluster Kubernetes API, a service mirror control for mirroring services, and a Link custom resource for holding configuration. To link cluster west to cluster east, you would run:\nlinkerd --context=east multicluster link --cluster-name east | kubectl --context=west apply -f - To verify that the credentials were created successfully and the clusters are able to reach each other, run:\nlinkerd --context=west multicluster check You should also see the list of gateways show up by running. Note that you’ll need Linkerd’s Viz extension to be installed in the source cluster to get the list of gateways:\nlinkerd --context=west multicluster gateways For a detailed explanation of what this step does, check out the linking the clusters section.\nStep 3: Export services Services are not automatically mirrored in linked clusters. By default, only services with the mirror.linkerd.io/exported label will be mirrored. For each service you would like mirrored to linked clusters, run:\nkubectl label svc foobar mirror.linkerd.io/exported=true Note You can configure a different label selector by using the --selector flag on the linkerd multicluster link command or by editing the …","description":"Allow Linkerd to manage cross-cluster communication.","keywords":null,"section":"2.16","title":"Installing Multi-cluster Components","uri":"https://travisbeckham.github.io/2.16/tasks/installing-multicluster/"},{"ancestors":["Docs","Reference"],"content":"In order to route TCP traffic in a pod to and from the proxy, an init container is used to set up iptables rules at the start of an injected pod’s lifecycle.\nAt first, linkerd-init will create two chains in the nat table: PROXY_INIT_REDIRECT, and PROXY_INIT_OUTPUT. These chains are used to route inbound and outbound packets through the proxy. Each chain has a set of rules attached to it, these rules are traversed by a packet in order.\nInbound connections When a packet arrives in a pod, it will typically be processed by the PREROUTING chain, a default chain attached to the nat table. The sidecar container will create a new chain to process inbound packets, called PROXY_INIT_REDIRECT. The sidecar container creates a rule (install-proxy-init-prerouting) to send packets from the PREROUTING chain to our redirect chain. This is the first rule traversed by an inbound packet.\nThe redirect chain will be configured with two more rules:\nignore-port: will ignore processing packets whose destination ports are included in the skip-inbound-ports install option. proxy-init-redirect-all: will redirect all incoming TCP packets through the proxy, on port 4143. Based on these two rules, there are two possible paths that an inbound packet can take, both of which are outlined below.\nInbound iptables chain traversal The packet will arrive on the PREROUTING chain and will be immediately routed to the redirect chain. If its destination port matches any of the inbound ports to skip, then it will be forwarded directly to the application process, bypassing the proxy. The list of destination ports to check against can be configured when installing Linkerd. If the packet does not match any of the ports in the list, it will be redirected through the proxy. Redirection is done by changing the incoming packet’s destination header, the target port will be replaced with 4143, which is the proxy’s inbound port. The proxy will process the packet and produce a new one that will be forwarded to the service; it will be able to get the original target (IP:PORT) of the inbound packet by using a special socket option SO_ORIGINAL_DST. The new packet will be routed through the OUTPUT chain, from there it will be sent to the application. The OUTPUT chain rules are covered in more detail below.\nOutbound connections When a packet leaves a pod, it will first traverse the OUTPUT chain, the first default chain an outgoing packet traverses in the nat table. To redirect outgoing packets through the outbound side of the proxy, the sidecar container will again create a new chain. The first outgoing rule is similar to the inbound counterpart: any packet that traverses the OUTPUT chain should be forwarded to our PROXY_INIT_OUTPUT chain to be processed.\nThe output redirect chain is slightly harder to understand but follows the same logical flow as the inbound redirect chain, in total there are 4 rules configured:\nignore-proxy-uid: any packets owned by the proxy (whose user id is 2102), will skip …","description":"A table with all of the chains and associated rules","keywords":null,"section":"2.16","title":"IPTables Reference","uri":"https://travisbeckham.github.io/2.16/reference/iptables/"},{"ancestors":["Docs","Features"],"content":"To transparently route TCP traffic through the proxy, without any awareness from the application, Linkerd will configure a set of firewall rules in each injected pod. Configuration can be done either through an init container or through a CNI plugin.\nLinkerd’s init container can be run in two separate modes: legacy or nft. The difference between the two modes is what variant of iptables they will use to configure firewall rules.\nDetails Modes for the init container can be changed either at upgrade time, or during installation. Once configured, all injected workloads (including the control plane) will use the same mode in the init container. Both modes will use the iptables utility to configure firewall rules; the main difference between the two, is which binary they will call into:\nlegacy mode will call into iptables-legacy for firewall configuration. This is the default mode that linkerd-init runs in, and is supported by most operating systems and distributions. nft mode will call into iptables-nft, which uses the newer nf_tables kernel API. The nftables utilities are used by newer operating systems to configure firewalls by default. Conceptually, iptables-nft is a bridge between the legacy and the newer nftables utilities. Under the hood, it uses a different backend, where rules additions and deletions are atomic. The nft version of iptables uses the same packet matching syntax (xtables) as its legacy counterpart.\nBecause both utilities use the same syntax, it is recommended to run in whatever mode your Kubernetes hosts support best. Certain operating systems (e.g Google Container Optimized OS) do not offer support out-of-the-box for nftables modules. Others (e.g RHEL family of operating systems) do not support the legacy version of iptables. Linkerd’s init container should be run in nft mode only if the nodes support it and contain the relevant nftables modules.\nNote Linkerd supports a -w (wait) option for its init container. Because operations are atomic, and rulesets are not reloaded when modified (only appended),this option is a no-op when running linkerd-init in nft mode. Installation The mode for linkerd-init can be overridden through the configuration option proxyInit.iptablesMode=iptables|nft. The configuration option can be used for both Helm and CLI installations (or upgrades). For example, the following line will install Linkerd and set the init container mode to nft:\nlinkerd install --set \u0026#34;proxyInit.iptablesMode=nft\u0026#34; | kubectl apply -f - ","description":"Linkerd's init container can use iptables-nft on systems that require it.","keywords":null,"section":"2.16","title":"Iptables-nft Support","uri":"https://travisbeckham.github.io/2.16/features/nft/"},{"ancestors":["Docs","Features"],"content":"As of version 2.16 (and edge-24.8.2) Linkerd fully supports Kubernetes clusters configured for IPv6-only or dual-stack networking.\nThis is disabled by default; to enable just set proxy.disableIPv6=false when installing the control plane and, if you use it, the linkerd-cni plugin.\nEnabling IPv6 support does not generally change how Linkerd operates, except in one way: when enabled on a dual-stack cluster, Linkerd will only use the IPv6 endpoints of destinations and will not use the IPv4 endpoints.\n","description":"Linkerd is compatible with both IPv6-only and dual-stack clusters.","keywords":null,"section":"2.16","title":"IPv6 Support","uri":"https://travisbeckham.github.io/2.16/features/ipv6/"},{"ancestors":["Docs","Reference","CLI"],"content":" jaeger manages the jaeger extension of Linkerd service mesh.\nFlags Flag Usage --api-addr Override kubeconfig and communicate directly with the control plane at host:port (mostly for testing) --as Username to impersonate for Kubernetes operations --as-group Group to impersonate for Kubernetes operations --context Name of the kubeconfig context to use --kubeconfig Path to the kubeconfig file to use for CLI requests --linkerd-namespace -L Namespace in which Linkerd is installed --verbose Turn on debug logging Subcommands check Check the Jaeger extension for potential problems.\nThe check command will perform a series of checks to validate that the Jaeger extension is configured correctly. If the command encounters a failure it will print additional information about the failure and exit with a non-zero exit code.\nExamples # Check that the Jaeger extension is up and running linkerd jaeger check Flags Flag Usage --namespace -n Namespace to use for –proxy checks (default: all namespaces) --output -o Output format. One of: table, json, short --proxy Also run data-plane checks, to determine if the data plane is healthy --wait Maximum allowed time for all tests to pass dashboard Open the Jaeger extension dashboard in a web browser\nFlags Flag Usage --address The address at which to serve requests --port -p The local port on which to serve requests (when set to 0, a random port will be used) --show-url show only URL in the CLI, and do not open the browser --wait Wait for dashboard to become available if it’s not available when the command is run install Output Kubernetes resources to install jaeger extension.\nExamples # Default install. linkerd jaeger install | kubectl apply -f - # Install Jaeger extension into a non-default namespace. linkerd jaeger install --namespace custom | kubectl apply -f - The installation can be configured by using the --set, --values, --set-string and --set-file flags. A full list of configurable values can be found at https://www.github.com/linkerd/linkerd2/tree/main/jaeger/charts/linkerd-jaeger/README.md Flags Flag Usage --ignore-cluster Ignore the current Kubernetes cluster when checking for existing cluster configuration (default false) --output -o Output format. One of: json|yaml --registry Docker registry to pull jaeger-webhook image from ($LINKERD_DOCKER_REGISTRY) --set set values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --set-file set values from respective files specified via the command line (can specify multiple or separate values with commas: key1=path1,key2=path2) --set-string set STRING values on the command line (can specify multiple or separate values with commas: key1=val1,key2=val2) --skip-checks Skip checks for linkerd core control-plane existence --values -f specify values in a YAML file or a URL (can specify multiple) --wait Wait for core control-plane components to be available list Lists which pods have tracing enabled\nFlags Flag Usage --all-namespaces -A …","description":"","keywords":null,"section":"2.16","title":"jaeger","uri":"https://travisbeckham.github.io/2.16/reference/cli/jaeger/"},{"ancestors":["Docs","Tasks"],"content":"Pod Security Policies have been deprecated in Kuberenetes v1.21 and removed in v1.25. However, for users who still want them, the Linkerd control plane comes with its own minimally privileged Pod Security Policy and the associated RBAC resources which can be optionally created by setting the --set enablePSP=true flag during Linkerd install or upgrade, or by using the enablePSP Helm value.\n","description":"Using Linkerd with a pod security policies enabled.","keywords":null,"section":"2.16","title":"Linkerd and Pod Security Policies (PSP)","uri":"https://travisbeckham.github.io/2.16/tasks/using-psp/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd’s automatic mTLS feature uses a set of TLS credentials to generate TLS certificates for proxies: a trust anchor, and an issuer certificate and private key. The trust anchor has a limited period of validity: 365 days if generated by linkerd install, or a customized value if generated manually.\nThus, for clusters that are expected to outlive this lifetime, you must manually rotate the trust anchor. In this document, we describe how to accomplish this without downtime.\nIndependent of the trust anchor, the issuer certificate and key pair can also expire (though it is possible to use cert-manager to set up automatic rotation. This document also covers how to rotate the issuer certificate and key pair without downtime.\nPrerequisites These instructions use the following CLI tools:\nstep to manipulate certificates and keys; Understanding the current state of your system Begin by running:\nlinkerd check --proxy If your configuration is valid and your credentials are not expiring soon, you should see output similar to:\nlinkerd-identity ---------------- √ certificate config is valid √ trust roots are using supported crypto algorithm √ trust roots are within their validity period √ trust roots are valid for at least 60 days √ issuer cert is using supported crypto algorithm √ issuer cert is within its validity period √ issuer cert is valid for at least 60 days √ issuer cert is issued by the trust root linkerd-identity-data-plane --------------------------- √ data plane proxies certificate match CA However, if you see a message warning you that your trust anchor (“trust root”) or issuer certificates are expiring soon, then you must rotate them.\nNote that this document only applies if the trust anchor is currently valid. If your trust anchor has expired, follow the Replacing Expired Certificates Guide instead. (If your issuer certificate has expired but your trust anchor is still valid, continue on with this document.)\nFor example, if your issuer certificate has expired, you will see a message similar to:\nlinkerd-identity ---------------- √ certificate config is valid √ trust roots are using supported crypto algorithm √ trust roots are within their validity period √ trust roots are valid for at least 60 days √ issuer cert is using supported crypto algorithm × issuer cert is within its validity period issuer certificate is not valid anymore. Expired on 2019-12-19T09:02:01Z see https://linkerd.io/checks/#l5d-identity-issuer-cert-is-time-valid for hints If your trust anchor has expired, you will see a message similar to:\nlinkerd-identity ---------------- √ certificate config is valid √ trust roots are using supported crypto algorithm × trust roots are within their validity period Invalid roots: * 79461543992952791393769540277800684467 identity.linkerd.cluster.local not valid anymore. Expired on 2019-12-19T09:11:30Z see https://linkerd.io/checks/#l5d-identity-roots-are-time-valid for hints Rotating the trust anchor Rotating the trust anchor without downtime …","description":"Update Linkerd's TLS trust anchor and issuer certificate.","keywords":null,"section":"2.16","title":"Manually Rotating Control Plane TLS Credentials","uri":"https://travisbeckham.github.io/2.16/tasks/manually-rotating-control-plane-tls-credentials/"},{"ancestors":["Docs","Tasks"],"content":"Emitting logs is an expensive operation for a network proxy, and by default, the Linkerd data plane proxies are configured to only log exceptional events. However, sometimes it is useful to increase the verbosity of proxy logs to assist with diagnosing proxy behavior. Happily, Linkerd allows you to modify these logs dynamically.\nNote The proxy’s proxy debug logging is distinct from the proxy HTTP access log, which is configured separately. See the documentation on enabling access logging for details on configuring Linkerd proxies to emit an HTTP access log. The log level of a Linkerd proxy can be modified on the fly by using the proxy’s /proxy-log-level endpoint on the admin-port.\nFor example, to change the proxy log-level of a pod to debug, run (replace ${POD:?} or set the environment-variable POD with the pod name):\nkubectl port-forward ${POD:?} linkerd-admin curl -v --data \u0026#39;linkerd=debug\u0026#39; -X PUT localhost:4191/proxy-log-level whereby linkerd-admin is the name of the admin-port (4191 by default) of the injected sidecar-proxy.\nThe resulting logs can be viewed with kubectl logs ${POD:?}.\nIf changes to the proxy log level should be retained beyond the lifetime of a pod, add the config.linkerd.io/proxy-log-level annotation to the pod template (or other options, see reference).\nThe syntax of the proxy log level can be found in the proxy log level reference.\nNote that logging has a noticeable, negative impact on proxy throughput. If the pod will continue to serve production traffic, you may wish to reset the log level once you are done.\n","description":"Linkerd proxy log levels can be modified dynamically to assist with debugging.","keywords":null,"section":"2.16","title":"Modifying the Proxy Log Level","uri":"https://travisbeckham.github.io/2.16/tasks/modifying-proxy-log-level/"},{"ancestors":["Docs","Features"],"content":"Linkerd can connect Kubernetes services across cluster boundaries in a way that is secure, fully transparent to the application, and independent of network topology. This multi-cluster capability is designed to provide:\nA unified trust domain. The identity of source and destination workloads are validated at every step, both in and across cluster boundaries. Separate failure domains. Failure of a cluster allows the remaining clusters to function. Support for any type of network. Linkerd does not require any specific network topology between clusters, and can function both with hierarchical networks as well as when clusters share the same flat network. A unified model alongside in-cluster communication. The same observability, reliability, and security features that Linkerd provides for in-cluster communication extend to cross-cluster communication. Just as with in-cluster connections, Linkerd’s cross-cluster connections are transparent to the application code. Regardless of whether that communication happens within a cluster, across clusters within a datacenter or VPC, or across the public Internet, Linkerd will establish a connection between clusters that’s reliable, encrypted, and authenticated on both sides with mTLS.\nHow it works Linkerd’s multi-cluster support works by “mirroring” service information between clusters, using a service mirror component that watches a target cluster for updates to services and applies those updates locally on the source cluster.\nThese mirrored services are suffixed with the name of the remote cluster, e.g. the Foo service on the west cluster would be mirrored as Foo-west on the local cluster. This approach is typically combined with traffic splitting or dynamic request routing to allow local services to access the Foo service as if it were on the local cluster.\nLinkerd supports two basic forms of multi-cluster communication: hierarchical and flat.\nHierarchical networks In hierarchical mode, Linkerd deploys a gateway component on the target cluster that allows it to receive requests from source clusters. This approach works on almost any network topology, as it only requires that the gateway IP of the destination cluster be reachable by pods on the source cluster.\nFlat networks As of Linkerd 2.14, Linkerd supports pod-to-pod communication for clusters that share a flat network, where pods can establish TCP connections and send traffic directly to each other across cluster boundaries. In these environments, Linkerd does not use a gateway intermediary for data plane traffic, which provides several advantages:\nImproved latency by avoiding an additional network hop Reduced operational costs in cloud environments that require a LoadBalancer-type service for the gateway Better multi-cluster authorization policies, as workload identity is preserved across cluster boundaries. Hierarchical (gateway-based) and flat (direct pod-to-pod) modes can be combined, and pod-to-pod mode can be enabled for specific services by using …","description":"Linkerd can transparently and securely connect services that are running in different clusters.","keywords":null,"section":"2.16","title":"Multi-cluster communication","uri":"https://travisbeckham.github.io/2.16/features/multicluster/"},{"ancestors":["Docs","Reference"],"content":"Linkerd’s multi-cluster functionality allows pods to connect to Kubernetes services across cluster boundaries in a way that is secure and fully transparent to the application. As of Linkerd 2.14, this feature supports two modes: hierarchical (using an gateway) and flat (without a gateway):\nFlat mode requires that all pods on the source cluster be able to directly connect to pods on the destination cluster. Hierarchical mode only requires that the gateway IP of the destination cluster be reachable by pods on the source cluster. These modes can be mixed and matched.\nHierarchical mode places a bare minimum of requirements on the underlying network, as it only requires that the gateway IP be reachable. However, flat mode has a few advantages over the gateway approach used in hierarchical mode, including reducing latency and preserving client identity.\nService mirroring Linkerd’s multi-cluster functionality uses a service mirror component that watches a target cluster for updates to services and mirrors those service updates locally to a source cluster.\nMulti-cluster support is underpinned by a concept known as service mirroring. Mirroring refers to importing a service definition from another cluster, and it allows applications to address and consume multi-cluster services. The service mirror component runs on the source cluster; it watches a target cluster for updates to services and mirrors those updates locally in the source cluster. Only Kubernetes service objects that match a label selector are exported.\nThe label selector also controls the mode a service is exported in. For example, by default, services labeled with mirror.linkerd.io/exported=true will be exported in hierarchical (gateway) mode, whereas services labeled with mirror.linkerd.io/exported=remote-discovery will be exported in flat (pod-to-pod) mode. Since the configuration is service-centric, switching from gateway to pod-to-pod mode is trivial and does not require the extension to be re-installed.\nNote In flat mode, the namespace of the Linkerd control plane should be the same across all clusters. We recommend leaving this at the default value of linkerd. The term “remote-discovery” refers to how the imported services should be interpreted by Linkerd’s control plane. Service discovery is performed by the destination service. Whenever traffic is sent to a target imported in “remote-discovery” mode, the destination service knows to look for all relevant information in the cluster the service has been exported from, not locally. In contrast, service discovery for a hierarchical (gateway mode) import will be performed locally; instead of routing directly to a pod, traffic will be sent to the gateway address on the target cluster.\nLinkerd’s destination service performs remote discovery by connecting directly to multiple Kubernetes API servers. Whenever two clusters are connected together, a Kubernetes Secret is created in the control plane’s namespace with a kubeconfig file that allows …","description":"Multi-cluster communication","keywords":null,"section":"2.16","title":"Multi-cluster communication","uri":"https://travisbeckham.github.io/2.16/reference/multicluster/"},{"ancestors":["Docs","Tasks"],"content":"This guide will walk you through installing and configuring Linkerd so that two clusters can talk to services hosted on both. There are a lot of moving parts and concepts here, so it is valuable to read through our introduction that explains how this works beneath the hood. By the end of this guide, you will understand how to split traffic between services that live on different clusters.\nAt a high level, you will:\nInstall Linkerd and Linkerd Viz on two clusters with a shared trust anchor. Prepare the clusters. Link the clusters. Install the demo. Export the demo services, to control visibility. Gain visibility in your linked clusters. Verify the security of your clusters. Split traffic from pods on the source cluster (west) to the target cluster (east) Prerequisites Two clusters. We will refer to them as east and west in this guide. Follow along with the blog post as you walk through this guide! The easiest way to do this for development is running a kind or k3d cluster locally on your laptop and one remotely on a cloud provider, such as AKS. Each of these clusters should be configured as kubectl contexts. We’d recommend you use the names east and west so that you can follow along with this guide. It is easy to rename contexts with kubectl, so don’t feel like you need to keep it all named this way forever. Elevated privileges on both clusters. We’ll be creating service accounts and granting extended privileges, so you’ll need to be able to do that on your test clusters. Support for services of type LoadBalancer in the east cluster. Check out the documentation for your cluster provider or take a look at inlets. This is what the west cluster will use to communicate with east via the gateway. Install Linkerd and Linkerd Viz Two Clusters Linkerd requires a shared trust anchor to exist between the installations in all clusters that communicate with each other. This is used to encrypt the traffic between clusters and authorize requests that reach the gateway so that your cluster is not open to the public internet. Instead of letting linkerd generate everything, we’ll need to generate the credentials and use them as configuration for the install command.\nWe like to use the step CLI to generate these certificates. If you prefer openssl instead, feel free to use that! To generate the trust anchor with step, you can run:\nstep certificate create root.linkerd.cluster.local root.crt root.key \\ --profile root-ca --no-password --insecure This certificate will form the common base of trust between all your clusters. Each proxy will get a copy of this certificate and use it to validate the certificates that it receives from peers as part of the mTLS handshake. With a common base of trust, we now need to generate a certificate that can be used in each cluster to issue certificates to the proxies. If you’d like to get a deeper picture into how this all works, check out the deep dive.\nThe trust anchor that we’ve generated is a self-signed certificate which can be …","description":"Allow Linkerd to manage cross-cluster communication.","keywords":null,"section":"2.16","title":"Multi-cluster communication","uri":"https://travisbeckham.github.io/2.16/tasks/multicluster/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd’s multi-cluster extension works by “mirroring” service information between clusters. Exported services in a target cluster will be mirrored as clusterIP replicas. By default, every exported service will be mirrored as clusterIP. When running workloads that require a headless service, such as StatefulSets, Linkerd’s multi-cluster extension can be configured with support for headless services to preserve the service type. Exported services that are headless will be mirrored in a source cluster as headless, preserving functionality such as DNS record creation and the ability to address an individual pod.\nThis guide will walk you through installing and configuring Linkerd and the multi-cluster extension with support for headless services and will exemplify how a StatefulSet can be deployed in a target cluster. After deploying, we will also look at how to communicate with an arbitrary pod from the target cluster’s StatefulSet from a client in the source cluster. For a more detailed overview on how multi-cluster support for headless services work, check out multi-cluster communication.\nPrerequisites Two Kubernetes clusters. They will be referred to as east and west with east being the “source” cluster and “west” the target cluster respectively. These can be in any cloud or local environment, this guide will make use of k3d to configure two local clusters. smallstep/CLI to generate certificates for Linkerd installation. A recent linkerd release (2.11 or older). To help with cluster creation and installation, there is a demo repository available. Throughout the guide, we will be using the scripts from the repository, but you can follow along without cloning or using the scripts.\nInstall Linkerd multi-cluster with headless support To start our demo and see everything in practice, we will go through a multi-cluster scenario where a pod in an east cluster will try to communicate to an arbitrary pod from a west cluster.\nThe first step is to clone the demo repository on your local machine.\n# clone example repository $ git clone git@github.com:mateiidavid/l2d-k3d-statefulset.git $ cd l2d-k3d-statefulset The second step consists of creating two k3d clusters named east and west, where the east cluster is the source and the west cluster is the target. When creating our clusters, we need a shared trust root. Luckily, the repository you have just cloned includes a handful of scripts that will greatly simplify everything.\n# create k3d clusters $ ./create.sh # list the clusters $ k3d cluster list NAME SERVERS AGENTS LOADBALANCER east 1/1 0/0 true west 1/1 0/0 true Once our clusters are created, we will install Linkerd and the multi-cluster extension. Finally, once both are installed, we need to link the two clusters together so their services may be mirrored. To enable support for headless services, we will pass an additional --set \u0026#34;enableHeadlessServices=true flag to linkerd multicluster link. As before, these steps are automated through the provided …","description":"cross-cluster communication to and from headless services.","keywords":null,"section":"2.16","title":"Multi-cluster communication with StatefulSets","uri":"https://travisbeckham.github.io/2.16/tasks/multicluster-using-statefulsets/"},{"ancestors":["Docs","Reference","CLI"],"content":" Manages the multicluster setup for Linkerd.\nThis command provides subcommands to manage the multicluster support functionality of Linkerd. You can use it to install the service mirror components on a cluster, manage credentials and link clusters together.\nExamples # Install multicluster addons. linkerd --context=cluster-a multicluster install | kubectl --context=cluster-a apply -f - # Extract mirroring cluster credentials from cluster A and install them on cluster B linkerd --context=cluster-a multicluster link --cluster-name=target | kubectl apply --context=cluster-b -f - Flags Flag Usage --api-addr Override kubeconfig and communicate directly with the control plane at host:port (mostly for testing) --as Username to impersonate for Kubernetes operations --as-group Group to impersonate for Kubernetes operations --context Name of the kubeconfig context to use --kubeconfig Path to the kubeconfig file to use for CLI requests --linkerd-namespace -L Namespace in which Linkerd is installed --verbose Turn on debug logging Subcommands allow Outputs credential resources that allow service-mirror controllers to connect to this cluster\nFlags Flag Usage --ignore-cluster Ignore cluster configuration --namespace The destination namespace for the service account. --output -o Output format. One of: json|yaml --service-account-name The name of the multicluster access service account check Check the multicluster extension for potential problems.\nThe check command will perform a series of checks to validate that the multicluster extension is configured correctly. If the command encounters a failure it will print additional information about the failure and exit with a non-zero exit code.\nExamples # Check that the multicluster extension is configured correctly linkerd multicluster check Flags Flag Usage --namespace -n --output -o Output format. One of: table, json, short --proxy --timeout Timeout for calls to the Kubernetes API --wait Maximum allowed time for all tests to pass gateways Display stats information about the gateways in target clusters\nFlags Flag Usage --cluster-name the name of the target cluster --output -o used to print output in different format --wait -w time allowed to fetch diagnostics install Output Kubernetes configs to install the Linkerd multicluster add-on\nExamples # Default install. linkerd multicluster install | kubectl apply -f - The installation can be configured by using the --set, --values, --set-string and --set-file flags. A full list of configurable values can be found at https://github.com/linkerd/linkerd2/blob/main/multicluster/charts/linkerd-multicluster/README.md Flags Flag Usage --gateway If the gateway component should be installed --gateway-port The port on the gateway used for all incoming traffic --gateway-probe-port The liveness check port of the gateway --gateway-probe-seconds The interval at which the gateway will be checked for being alive in seconds --gateway-service-type Overwrite Service type for gateway service …","description":"","keywords":null,"section":"2.16","title":"multicluster","uri":"https://travisbeckham.github.io/2.16/reference/cli/multicluster/"},{"ancestors":["Docs","Features"],"content":"Linkerd features mesh expansion, or the ability to add non-Kubernetes workloads to your service mesh by deploying the Linkerd proxy to the remote machine and connecting it back to the Linkerd control plane within the mesh. This allows you to use Linkerd to establish communication to and from the workload that is secure, reliable, and observable, just like communication to and from your Kubernetes workloads.\nRelated content:\nGuide: Adding non-Kubernetes workloads to your mesh ExternalWorkload Reference ","description":"","keywords":null,"section":"2.16","title":"Non-Kubernetes workloads (mesh expansion)","uri":"https://travisbeckham.github.io/2.16/features/non-kubernetes-workloads/"},{"ancestors":["Docs","Tasks"],"content":"Retries and timeouts can be configured by annotating Service, HTTPRoute, or GRPCRoute resources. This will apply the retry or timeout policy to all requests that are sent to that service/route.\nAdditionally, retry and timeout policy can be configured for individual HTTP requests by adding special HTTP headers to those requests.\nEnabling Per-Request Policy In order to enable per-request policy, Linkerd must be installed with the --set policyController.additionalArgs=\u0026#34;--allow-l5d-request-headers\u0026#34; flag or the corresponding Helm value. Enabling per-request policy is not recommended if your application accepts requests from untrusted sources (e.g. if it is an ingress) since this allows untrusted clients to specify Linkerd policy.\nPer-Request Policy Headers Once per-request policy is enabled, the following HTTP headers can be added to a request to set or override retry and/or timeout policy for that request:\nl5d-retry-http: Overrides the retry.linkerd.io/http annotation l5d-retry-grpc: Overrides the retry.linkerd.io/grpc annotation l5d-retry-limit: Overrides the retry.linkerd.io/limit annotation l5d-retry-timeout: Overrides the retry.linkerd.io/timeout annotation l5d-timeout: Overrides the timeout.linkerd.io/request annotation l5d-response-timeout: Overrides the timeout.linkerd.io/response annotation ","description":"Using HTTP headers to specify per-request policy","keywords":null,"section":"2.16","title":"Per-Request Policy","uri":"https://travisbeckham.github.io/2.16/tasks/per-request-policy/"},{"ancestors":["Docs","Tasks"],"content":"By default, Linkerd’s multicluster extension works by sending all cross-cluster traffic through a gateway on the target cluster. However, when multiple Kubernetes clusters are deployed on a flat network where pods from one cluster can communicate directly with pods on another, Linkerd can export multicluster services in pod-to-pod mode where cross-cluster traffic does not go through the gateway, but instead goes directly to the target pods.\nThis guide will walk you through exporting multicluster services in pod-to-pod mode, setting up authorization policies, and monitoring the traffic.\nPrerequisites Two clusters. We will refer to them as east and west in this guide. The clusters must be on a flat network. In other words, pods from one cluster must be able to address and connect to pods in the other cluster. Each of these clusters should be configured as kubectl contexts. We’d recommend you use the names east and west so that you can follow along with this guide. It is easy to rename contexts with kubectl, so don’t feel like you need to keep it all named this way forever. Step 1: Installing Linkerd and Linkerd-Viz First, install Linkerd and Linkerd-Viz into both clusters, as described in the multicluster guide. Make sure to take care that both clusters share a common trust anchor.\nStep 2: Installing Linkerd-Multicluster We will install the multicluster extension into both clusters. We can install without the gateway because we will be using direct pod-to-pod communication.\n\u0026gt; linkerd --context east multicluster install --gateway=false | kubectl --context east apply -f - \u0026gt; linkerd --context east check \u0026gt; linkerd --context west multicluster install --gateway=false | kubectl --context west apply -f - \u0026gt; linkerd --context west check Step 3: Linking the Clusters We use the linkerd multilcuster link command to link our two clusters together. This is exactly the same as in the regular Multicluster guide except that we pass the --gateway=false flag to create a Link which doesn’t require a gateway.\n\u0026gt; linkerd --context east multicluster link --cluster-name=target --gateway=false | kubectl --context west apply -f - Step 4: Deploy and Exporting a Service For our guide, we’ll deploy the bb service, which is a simple server that just returns a static response. We deploy it into the target cluster:\n\u0026gt; cat \u0026lt;\u0026lt;EOF | linkerd --context east inject - | kubectl --context east apply -f - --- apiVersion: v1 kind: Namespace metadata: name: mc-demo --- apiVersion: apps/v1 kind: Deployment metadata: name: bb namespace: mc-demo spec: replicas: 1 selector: matchLabels: app: bb template: metadata: labels: app: bb spec: containers: - name: terminus image: buoyantio/bb:v0.0.6 args: - terminus - \u0026#34;--h1-server-port=8080\u0026#34; - \u0026#34;--response-text=hello\\n\u0026#34; ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: bb namespace: mc-demo spec: ports: - name: http port: 8080 targetPort: 8080 selector: app: bb EOF We then create the corresponding namespace on the source cluster …","description":"Multi-Cluster Communication for Flat Networks","keywords":null,"section":"2.16","title":"Pod-to-Pod Multi-cluster communication","uri":"https://travisbeckham.github.io/2.16/tasks/pod-to-pod-multicluster/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output service profile config for Kubernetes.\nCheck out the service profile documentation for more details on what this command does and what you can do with service profiles.\nExamples # Output a basic template to apply after modification. linkerd profile -n emojivoto --template web-svc # Generate a profile from an OpenAPI specification. linkerd profile -n emojivoto --open-api web-svc.swagger web-svc # Generate a profile from a protobuf definition. linkerd profile -n emojivoto --proto Voting.proto vote-svc Flags Flag Usage --ignore-cluster Output a service profile through offline generation --namespace -n Namespace of the service --open-api Output a service profile based on the given OpenAPI spec file --output -o Output format. One of: yaml, json --proto Output a service profile based on the given Protobuf spec file --template Output a service profile template ","description":"","keywords":null,"section":"2.16","title":"profile","uri":"https://travisbeckham.github.io/2.16/reference/cli/profile/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd’s dynamic request routing allows you to dynamically shift traffic between services. This can be used to implement lower-risk deployment strategies like blue-green deploys and canaries.\nBut simply shifting traffic from one version of a service to the next is just the beginning. We can combine traffic splitting with Linkerd’s automatic golden metrics telemetry and drive traffic decisions based on the observed metrics. For example, we can gradually shift traffic from an old deployment to a new one while continually monitoring its success rate. If at any point the success rate drops, we can shift traffic back to the original deployment and back out of the release. Ideally, our users remain happy throughout, not noticing a thing!\nIn this tutorial, we’ll show you how to use two different progressive delivery tools: Flagger and Argo Rollouts and how to tie Linkerd’s metrics and request routing together in a control loop, allowing for fully-automated, metrics-aware canary deployments.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Prerequisites To use this guide, you’ll need a Kubernetes cluster running:\nLinkerd and Linkerd-Viz. If you haven’t installed these yet, follow the Installing Linkerd Guide. Flagger Install Flagger While Linkerd will be managing the actual traffic routing, Flagger automates the process of creating new Kubernetes resources, watching metrics and incrementally sending users over to the new version. To add Flagger to your cluster and have it configured to work with Linkerd, run:\nkubectl apply -k github.com/fluxcd/flagger/kustomize/linkerd This command adds:\nThe canary CRD that enables configuring how a rollout should occur. RBAC which grants Flagger permissions to modify all the resources that it needs to, such as deployments and services. A Flagger controller configured to interact with the Linkerd control plane. To watch until everything is up and running, you can use kubectl:\nkubectl -n flagger-system rollout status deploy/flagger Set up the demo This demo consists of three components: a load generator, a deployment and a frontend. The deployment creates a pod that returns some information such as name. You can use the responses to watch the incremental rollout as Flagger orchestrates it. A load generator simply makes it easier to execute the rollout as there needs to be some kind of active traffic to complete the operation. Together, these components have a topology that looks like:\nTopology To add these components to your cluster and include them in the Linkerd data plane, run:\nkubectl create ns test \u0026amp;\u0026amp; \\ kubectl apply -f https://run.linkerd.io/flagger.yml Verify that everything has started up successfully by running:\nkubectl -n test rollout status deploy podinfo Check it out by forwarding the frontend …","description":"Reduce deployment risk by automating canary releases based on service metrics.","keywords":null,"section":"2.16","title":"Progressive Delivery","uri":"https://travisbeckham.github.io/2.16/tasks/flagger/"},{"ancestors":["Docs","Common Errors"],"content":"Linkerd is capable of proxying all TCP traffic, including TLS connections, WebSockets, and HTTP tunneling. In most cases where the client speaks first when a new connection is made, Linkerd can detect the protocol in use, allowing it to perform per-request routing and metrics.\nIf your proxy logs contain messages like protocol detection timed out after 10s, or you’re experiencing 10-second delays when establishing connections, you’re probably running a situation where Linkerd cannot detect the protocol. This is most common for protocols where the server speaks first, and the client is waiting for information from the server. It may also occur with non-HTTP protocols for which Linkerd doesn’t yet understand the wire format of a request.\nYou’ll need to understand exactly what the situation is to fix this:\nA server-speaks-first protocol will probably need to be configured as a skip or opaque port, as described in the protocol detection documentation.\nIf you’re seeing transient protocol detection timeouts, this is more likely to indicate a misbehaving workload.\nIf you know the protocol is client-speaks-first but you’re getting consistent protocol detection timeouts, you’ll probably need to fall back on a skip or opaque port.\nNote that marking ports as skip or opaque has ramifications beyond protocol detection timeouts; see the protocol detection documentation for more information.\n","description":"Protocol detection errors indicate that Linkerd doesn't understand the protocol in use.","keywords":null,"section":"2.16","title":"Protocol Detection Errors","uri":"https://travisbeckham.github.io/2.16/common-errors/protocol-detection/"},{"ancestors":["Docs","Reference"],"content":"Linkerd provides a set of annotations that can be used to override the data plane proxy’s configuration. This is useful for overriding the default configurations of auto-injected proxies.\nThe following is the list of supported annotations:\nAnnotation Description config.alpha.linkerd.io/proxy-enable-native-sidecar Enable KEP-753 native sidecars. This is an experimental feature. It requires Kubernetes \u0026gt;= 1.29. If enabled, .proxy.waitBeforeExitSeconds should not be used. config.alpha.linkerd.io/proxy-wait-before-exit-seconds The proxy sidecar will stay alive for at least the given period after receiving SIGTERM signal from Kubernetes but no longer than pod’s terminationGracePeriodSeconds. Defaults to 0 config.linkerd.io/access-log Enables HTTP access logging in the proxy. Accepted values are apache, to output the access log in the Appache Common Log Format, and json, to output the access log in JSON. config.linkerd.io/admin-port Proxy port to serve metrics on config.linkerd.io/close-wait-timeout Sets nf_conntrack_tcp_timeout_close_wait. Accepts a duration string, e.g. 1m or 3600s config.linkerd.io/control-port Proxy port to use for control config.linkerd.io/debug-image Linkerd debug container image name config.linkerd.io/debug-image-pull-policy Docker image pull policy for debug image config.linkerd.io/debug-image-version Linkerd debug container image version config.linkerd.io/default-inbound-policy Proxy’s default inbound policy config.linkerd.io/enable-debug-sidecar Inject a debug sidecar for data plane debugging config.linkerd.io/enable-external-profiles Enable service profiles for non-Kubernetes services config.linkerd.io/image-pull-policy Docker image pull policy config.linkerd.io/inbound-port Proxy port to use for inbound traffic config.linkerd.io/init-image Linkerd init container image name config.linkerd.io/init-image-version Linkerd init container image version config.linkerd.io/opaque-ports Ports that skip the proxy’s protocol detection mechanism and are proxied opaquely. Comma-separated list of values, where each value can be a port number or a range a-b. config.linkerd.io/outbound-port Proxy port to use for outbound traffic config.linkerd.io/pod-inbound-ports Comma-separated list of (non-proxy) container ports exposed by the pod spec. Useful when other mutating webhooks inject sidecar containers after the proxy injector has run config.linkerd.io/proxy-await The application container will not start until the proxy is ready; accepted values are enabled and disabled config.linkerd.io/proxy-cpu-limit Maximum amount of CPU units that the proxy sidecar can use config.linkerd.io/proxy-cpu-request Amount of CPU units that the proxy sidecar requests config.linkerd.io/proxy-disable-inbound-protocol-detect-timeout When set to true, disables the protocol detection timeout on the inbound side of the proxy by setting it to a very high value config.linkerd.io/proxy-disable-outbound-protocol-detect-timeout When set to true, disables the protocol …","description":"Linkerd provides a set of annotations that can be used to override the data plane proxy's configuration.","keywords":null,"section":"2.16","title":"Proxy Configuration","uri":"https://travisbeckham.github.io/2.16/reference/proxy-configuration/"},{"ancestors":["Docs","Reference"],"content":"The Linkerd proxy’s log level can be configured via the:\nLINKERD_PROXY_LOG environment variable --proxy-log-level CLI flag of the install, inject and upgrade commands config.linkerd.io/proxy-log-level annotation (see Proxy Configuration) which sets LINKERD_PROXY_LOG environment-variable on the injected sidecar an endpoint on the admin port of a running proxy. The log level is a comma-separated list of log directives, which is based on the logging syntax of the env_logger crate.\nA log directive consists of either:\nA level (e.g. info), which sets the global log level, or A module path (e.g. foo or foo::bar::baz), or A module path followed by an equals sign and a level (e.g. foo=warn or foo::bar::baz=debug), which sets the log level for that module A level is one of:\ntrace debug info warn error A module path represents the path to a Rust module. It consists of one or more module names, separated by ::.\nA module name starts with a letter, and consists of alphanumeric characters and _.\nThe proxy’s default log level is set to warn,linkerd2_proxy=info.\n","description":"Syntax of the proxy log level.","keywords":null,"section":"2.16","title":"Proxy Log Level","uri":"https://travisbeckham.github.io/2.16/reference/proxy-log-level/"},{"ancestors":["Docs","Reference"],"content":"The Linkerd proxy exposes metrics that describe the traffic flowing through the proxy. The following metrics are available at /metrics on the proxy’s metrics port (default: :4191) in the Prometheus format.\nProtocol-Level Metrics request_total: A counter of the number of requests the proxy has received. This is incremented when the request stream begins.\nresponse_total: A counter of the number of responses the proxy has received. This is incremented when the response stream ends.\nresponse_latency_ms: A histogram of response latencies. This measurement reflects the time-to-first-byte (TTFB) by recording the elapsed time between the proxy processing a request’s headers and the first data frame of the response. If a response does not include any data, the end-of-stream event is used. The TTFB measurement is used so that Linkerd accurately reflects application behavior when a server provides response headers immediately but is slow to begin serving the response body.\nroute_request_total, route_response_latency_ms, and route_response_total: These metrics are analogous to request_total, response_latency_ms, and response_total except that they are collected at the route level. This means that they do not have authority, tls, grpc_status_code or any outbound labels but instead they have:\ndst: The authority of this request. rt_route: The name of the route for this request. control_request_total, control_response_latency_ms, and control_response_total: These metrics are analogous to request_total, response_latency_ms, and response_total but for requests that the proxy makes to the Linkerd control plane. Instead of authority, direction, or any outbound labels, instead they have:\naddr: The address used to connect to the control plane. inbound_http_authz_allow_total: A counter of the total number of inbound HTTP requests that were authorized.\nauthz_name: The name of the authorization policy used to allow the request. inbound_http_authz_deny_total: A counter of the total number of inbound HTTP requests that could not be processed due to being denied by the authorization policy.\ninbound_http_route_not_found_total: A counter of the total number of inbound HTTP requests that could not be associated with a route.\nNote that latency measurements are not exported to Prometheus until the stream completes. This is necessary so that latencies can be labeled with the appropriate response classification.\nLabels Each of these metrics has the following labels:\nauthority: The value of the :authority (HTTP/2) or Host (HTTP/1.1) header of the request. direction: inbound if the request originated from outside of the pod, outbound if the request originated from inside of the pod. tls: true if the request’s connection was secured with TLS. Response Labels The following labels are only applicable on response_* metrics.\nstatus_code: The HTTP status code of the response. Response Total Labels In addition to the labels applied to all response_* metrics, the response_total, …","description":"The Linkerd proxy natively exports Prometheus metrics for all incoming and outgoing traffic.","keywords":null,"section":"2.16","title":"Proxy Metrics","uri":"https://travisbeckham.github.io/2.16/reference/proxy-metrics/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output extraneous Kubernetes resources in the linkerd control plane.\nExamples # Prune extraneous resources. linkerd prune | kubectl delete -f - Flags Flag Usage --output -o Output format. One of: json|yaml ","description":"","keywords":null,"section":"2.16","title":"prune","uri":"https://travisbeckham.github.io/2.16/reference/cli/prune/"},{"ancestors":["Docs","Tasks"],"content":"If any of your TLS certs are approaching expiry and you are not relying on an external certificate management solution such as cert-manager, you can follow Manually Rotating Control Plane TLS Credentials to update them without incurring downtime. However, if any of your certificates have already expired, your mesh is already in an invalid state and any measures to avoid downtime are not guaranteed to give good results. Instead, you need to replace the expired certificates with valid certificates.\nReplacing only the issuer certificate It might be the case that your issuer certificate is expired. If this it true running linkerd check --proxy will produce output similar to:\nlinkerd-identity ---------------- √ certificate config is valid √ trust roots are using supported crypto algorithm √ trust roots are within their validity period √ trust roots are valid for at least 60 days √ issuer cert is using supported crypto algorithm × issuer cert is within its validity period issuer certificate is not valid anymore. Expired on 2019-12-19T09:21:08Z see https://linkerd.io/checks/#l5d-identity-issuer-cert-is-time-valid for hints In this situation, if you have installed Linkerd with a manually supplied trust root and you have its key, you can follow the instructions to rotate your identity issuer certificate to update your expired certificate.\nReplacing the root and issuer certificates If your root certificate is expired or you do not have its key, you need to replace both your root and issuer certificates at the same time. If your root has expired linkerd check will indicate that by outputting an error similar to:\nlinkerd-identity ---------------- √ certificate config is valid √ trust roots are using supported crypto algorithm × trust roots are within their validity period Invalid roots: * 272080721524060688352608293567629376512 identity.linkerd.cluster.local not valid anymore. Expired on 2019-12-19T10:05:31Z see https://linkerd.io/checks/#l5d-identity-roots-are-time-valid for hints You can follow Generating your own mTLS root certificates to create new root and issuer certificates. Then use the linkerd upgrade command:\nlinkerd upgrade \\ --identity-issuer-certificate-file=./issuer-new.crt \\ --identity-issuer-key-file=./issuer-new.key \\ --identity-trust-anchors-file=./ca-new.crt \\ --force \\ | kubectl apply -f - Usually upgrade will prevent you from using an issuer certificate that will not work with the roots your meshed pods are using. At that point we do not need this check as we are updating both the root and issuer certs at the same time. Therefore we use the --force flag to ignore this error.\nIf you run linkerd check --proxy while pods are restarting after the trust bundle is updated, you will probably see warnings about pods not having the current trust bundle:\nlinkerd-identity ---------------- √ certificate config is valid √ trust roots are using supported crypto algorithm √ trust roots are within their validity period √ trust roots are valid for at …","description":"Follow this workflow if any of your TLS certs have expired.","keywords":null,"section":"2.16","title":"Replacing expired certificates","uri":"https://travisbeckham.github.io/2.16/tasks/replacing_expired_certificates/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd policy resources can be used to restrict which clients may access a service. In this example, we’ll use Emojivoto to show how to restrict access to the Voting service so that it may only be called from the Web service.\nFor a more comprehensive description of the policy resources, see the Policy reference docs.\nPrerequisites To use this guide, you’ll need to have Linkerd installed on your cluster, along with its Viz extension. Follow the Installing Linkerd Guide if you haven’t already done this.\nSetup Inject and install the Emojivoto application:\n$ linkerd inject https://run.linkerd.io/emojivoto.yml | kubectl apply -f - ... $ linkerd check -n emojivoto --proxy -o short ... Creating a Server resource We start by creating a Server resource for the Voting service. A Server is a Linkerd custom resource which describes a specific port of a workload. Once the Server resource has been created, only clients which have been authorized may access it (we’ll see how to authorize clients in a moment).\nkubectl apply -f - \u0026lt;\u0026lt;EOF --- apiVersion: policy.linkerd.io/v1beta1 kind: Server metadata: namespace: emojivoto name: voting-grpc labels: app: voting-svc spec: podSelector: matchLabels: app: voting-svc port: grpc proxyProtocol: gRPC EOF We see that this Server uses a podSelector to select the pods that it describes: in this case the voting service pods. It also specifies the named port (grpc) that it applies to. Finally, it specifies the protocol that is served on this port. This ensures that the proxy treats traffic correctly and allows it skip protocol detection.\nAt this point, no clients have been authorized to access this service and you will likely see a drop in success rate as requests from the Web service to Voting start to get rejected.\nWe can use the linkerd viz authz command to look at the authorization status of requests coming to the voting service and see that all incoming requests to the voting-grpc server are currently unauthorized:\n\u0026gt; linkerd viz authz -n emojivoto deploy/voting ROUTE SERVER AUTHORIZATION UNAUTHORIZED SUCCESS RPS LATENCY_P50 LATENCY_P95 LATENCY_P99 default default:all-unauthenticated default/all-unauthenticated 0.0rps 100.00% 0.1rps 1ms 1ms 1ms probe default:all-unauthenticated default/probe 0.0rps 100.00% 0.2rps 1ms 1ms 1ms default voting-grpc 1.0rps 0.00% 0.0rps 0ms 0ms 0ms Creating a ServerAuthorization resource A ServerAuthorization grants a set of clients access to a set of Servers. Here we will create a ServerAuthorization which grants the Web service access to the Voting Server we created above. Note that meshed mTLS uses ServiceAccounts as the basis for identity, thus our authorization will also be based on ServiceAccounts.\nkubectl apply -f - \u0026lt;\u0026lt;EOF --- apiVersion: policy.linkerd.io/v1beta1 kind: ServerAuthorization metadata: namespace: emojivoto name: voting-grpc labels: app.kubernetes.io/part-of: emojivoto app.kubernetes.io/name: voting app.kubernetes.io/version: v11 spec: server: name: voting-grpc # The voting …","description":"Use Linkerd policy to restrict access to a service.","keywords":null,"section":"2.16","title":"Restricting Access To Services","uri":"https://travisbeckham.github.io/2.16/tasks/restricting-access/"},{"ancestors":["Docs","Reference"],"content":"Linkerd can be configured to automatically retry requests when it receives a failed response instead of immediately returning that failure to the client. This is valuable tool for improving success rate in the face of transient failures.\nRetries are a client-side behavior, and are therefore performed by the outbound side of the Linkerd proxy.1 If retries are configured on an HTTPRoute or GRPCRoute with multiple backends, each retry of a request can potentially get sent to a different backend. If a request has a body larger than 64KiB then it will not be retried.\nConfiguring Retries Retries are configured by a set of annotations which can be set on a Kubernetes Service resource or on a HTTPRoute or GRPCRoute which has a Service as a parent. Client proxies will then retry failed requests to that Service or route. If any retry configuration annotations are present on a route resource, they override all retry configuration annotations on the parent Service.\nWarning Retries configured in this way are incompatible with ServiceProfiles. If a ServiceProfile is defined for a Service, proxies will use the ServiceProfile retry configuration and ignore any retry annotations. retry.linkerd.io/http: A comma separated list of HTTP response codes which should be retried. Each element of the list may be xxx to retry a single response code (for example, \u0026#34;504\u0026#34; – remember, annotation values must be strings!); xxx-yyy to retry a range of response codes (for example, 500-504); gateway-error to retry response codes 502-504; or 5xx to retry all 5XX response codes. This annotation is not valid on GRPCRoute resources. retry.linkerd.io/grpc: A comma seperated list of gRPC status codes which should be retried. Each element of the list may be cancelled deadline-exceeded internal resource-exhausted unavailable This annotation is not valid on HTTPRoute resources. retry.linkerd.io/limit: The maximum number of times a request can be retried. If unspecified, the default is 1. retry.linkerd.io/timeout: A retry timeout after which a request is cancelled and retried (if the retry limit has not yet been reached). If unspecified, no retry timeout is applied. Units must be specified in this value e.g. 5s or 200ms. Examples kind: HTTPRoute apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: schlep-default namespace: schlep annotations: retry.linkerd.io/http: 5xx retry.linkerd.io/limit: \u0026#34;2\u0026#34; retry.linkerd.io/timeout: 300ms spec: parentRefs: - name: schlep kind: Service group: core port: 80 rules: - matches: - path: type: PathPrefix value: \u0026#34;/\u0026#34; kind: GRPCRoute apiVersion: gateway.networking.k8s.io/v1alpha2 metadata: name: schlep-default namespace: schlep annotations: retry.linkerd.io/grpc: internal retry.linkerd.io/limit: \u0026#34;2\u0026#34; retry.linkerd.io/timeout: 400ms spec: parentRefs: - name: schlep kind: Service group: core port: 8080 rules: - matches: - method: type: Exact service: schlep.Schlep method: Get The part of the proxy which handles connections from within the pod to the rest of …","description":"How Linkerd implements retries.","keywords":null,"section":"2.16","title":"Retries","uri":"https://travisbeckham.github.io/2.16/reference/retries/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd uses the Kubernetes admission webhooks and extension API server to implement some of its core features like automatic proxy injection and service profiles validation.\nAlso, the viz extension uses a webhook to make pods tappable, as does the jaeger extension to turn on tracing on pods.\nTo secure the connections between the Kubernetes API server and the webhooks, all the webhooks are TLS-enabled. The x509 certificates used by these webhooks are issued by the self-signed CA certificates embedded in the webhooks configuration.\nBy default, these certificates have a validity period of 365 days. They are stored in the following secrets:\nIn the linkerd namespace: linkerd-policy-validator-k8s-tls, linkerd-proxy-injector-k8s-tls and linkerd-sp-validator-k8s-tls In the linkerd-viz namespace: tap-injector-k8s-tls In the linkerd-jaeger namespace: jaeger-injector-k8s-tls The rest of this documentation provides instructions on how to renew these certificates.\nRenewing the webhook certificates To check the validity of all the TLS secrets (using step):\n# assuming you have viz and jaeger installed, otherwise trim down these arrays # accordingly SECRETS=(\u0026#34;linkerd-policy-validator-k8s-tls\u0026#34; \u0026#34;linkerd-proxy-injector-k8s-tls\u0026#34; \u0026#34;linkerd-sp-validator-k8s-tls\u0026#34; \u0026#34;tap-injector-k8s-tls\u0026#34; \u0026#34;jaeger-injector-k8s-tls\u0026#34;) NS=(\u0026#34;linkerd\u0026#34; \u0026#34;linkerd\u0026#34; \u0026#34;linkerd-viz\u0026#34; \u0026#34;linkerd-jaeger\u0026#34;) for idx in \u0026#34;${!SECRETS[@]}\u0026#34;; do \\ kubectl -n \u0026#34;${NS[$idx]}\u0026#34; get secret \u0026#34;${SECRETS[$idx]}\u0026#34; -ojsonpath=\u0026#39;{.data.tls\\.crt}\u0026#39; | \\ base64 --decode - | \\ step certificate inspect - | \\ grep -iA2 validity; \\ done Manually delete these secrets and use upgrade/install to recreate them:\nfor idx in \u0026#34;${!SECRETS[@]}\u0026#34;; do \\ kubectl -n \u0026#34;${NS[$idx]}\u0026#34; delete secret \u0026#34;${SECRETS[$idx]}\u0026#34;; \\ done linkerd upgrade | kubectl apply -f - linkerd viz install | kubectl apply -f - linkerd jaeger install | kubectl apply -f - The above command will recreate the secrets without restarting Linkerd.\nNote For Helm users, use the helm upgrade command to recreate the deleted secrets.\nIf you render the helm charts externally and apply them with kubectl apply (e.g. in a CI/CD pipeline), you do not need to delete the secrets manually, as they wil be overwritten by a new cert and key generated by the helm chart.\nConfirm that the secrets are recreated with new certificates:\nfor idx in \u0026#34;${!SECRETS[@]}\u0026#34;; do \\ kubectl -n \u0026#34;${NS[$idx]}\u0026#34; get secret \u0026#34;${SECRETS[$idx]}\u0026#34; -ojsonpath=\u0026#39;{.data.crt\\.pem}\u0026#39; | \\ base64 --decode - | \\ step certificate inspect - | \\ grep -iA2 validity; \\ done Ensure that Linkerd remains healthy:\nlinkerd check Restarting the pods that implement the webhooks and API services is usually not necessary. But if the cluster is large, or has a high pod churn, it may be advisable to restart the pods manually, to avoid cascading failures.\nIf you observe certificate expiry errors or mismatched CA certs, restart their pods with:\nkubectl -n linkerd rollout restart deploy \\ linkerd-proxy-injector \\ linkerd-sp-validator \\ kubectl -n linkerd-viz rollout …","description":"Follow these steps to rotate your Linkerd webhooks certificates.","keywords":null,"section":"2.16","title":"Rotating webhooks certificates","uri":"https://travisbeckham.github.io/2.16/tasks/rotating_webhooks_certificates/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd provides a powerful tool called tap which allows users to introspect live traffic in real time. While powerful, this feature can potentially expose sensitive data such as request and response headers, which may contain confidential information. To mitigate this risk, Linkerd has a configuration field called tap.ignoreHeaders which can be used to exclude specific headers from being captured by tap. Access to tap is controlled using role-based access control (RBAC). This page illustrates best practices to enable this introspection in a secure way.\nTap Linkerd’s Viz extension includes Tap support. This feature is available via the following commands:\nlinkerd viz tap linkerd viz top linkerd viz profile --tap linkerd viz dashboard Depending on your RBAC setup, you may need to perform additional steps to enable your user(s) to perform Tap actions.\nNote If you are on GKE, skip to the GKE section below. Check for Tap access Use kubectl to determine whether your user is authorized to perform tap actions. For more information, see the Kubernetes docs on authorization.\nTo determine if you can watch pods in all namespaces:\nkubectl auth can-i watch pods.tap.linkerd.io --all-namespaces To determine if you can watch deployments in the emojivoto namespace:\nkubectl auth can-i watch deployments.tap.linkerd.io -n emojivoto To determine if a specific user can watch deployments in the emojivoto namespace:\nkubectl auth can-i watch deployments.tap.linkerd.io -n emojivoto --as $(whoami) You can also use the Linkerd CLI’s --as flag to confirm:\n$ linkerd viz tap -n linkerd deploy/linkerd-controller --as $(whoami) Cannot connect to Linkerd Viz: namespaces is forbidden: User \u0026#34;XXXX\u0026#34; cannot list resource \u0026#34;namespaces\u0026#34; in API group \u0026#34;\u0026#34; at the cluster scope Validate the install with: linkerd viz check ... Enabling Tap access If the above commands indicate you need additional access, you can enable access with as much granularity as you choose.\nGranular Tap access To enable tap access to all resources in all namespaces, you may bind your user to the linkerd-linkerd-tap-admin ClusterRole, installed by default:\n$ kubectl describe clusterroles/linkerd-linkerd-viz-tap-admin Name: linkerd-linkerd-viz-tap-admin Labels: component=tap linkerd.io/extension=viz Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;rbac.authorization.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ClusterRole\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;component=tap... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- *.tap.linkerd.io [] [] [watch] Note This ClusterRole name includes the Linkerd Viz namespace, so it may vary if you installed Viz into a non-default namespace: linkerd-[LINKERD_VIZ_NAMESPACE]-tap-admin To bind the linkerd-linkerd-viz-tap-admin ClusterRole to a particular user:\nkubectl create clusterrolebinding \\ $(whoami)-tap-admin \\ --clusterrole=linkerd-linkerd-viz-tap-admin \\ --user=$(whoami) You can verify you now have tap access …","description":"Best practices for securing Linkerd's tap feature.","keywords":null,"section":"2.16","title":"Securing Linkerd Tap","uri":"https://travisbeckham.github.io/2.16/tasks/securing-linkerd-tap/"},{"ancestors":["Docs","Features"],"content":" Note HTTPRoutes are the recommended method for getting per-route metrics, specifying timeouts, and specifying retries. Service profiles continue to be supported for backwards compatibility. A service profile is a custom Kubernetes resource (CRD) that can provide Linkerd additional information about a service. In particular, it allows you to define a list of routes for the service. Each route uses a regular expression to define which paths should match that route. Defining a service profile enables Linkerd to report per-route metrics and also allows you to enable per-route features such as retries and timeouts.\nNote If working with headless services, service profiles cannot be retrieved. Linkerd reads service discovery information based off the target IP address, and if that happens to be a pod IP address then it cannot tell which service the pod belongs to. To get started with service profiles you can:\nLook into setting up service profiles for your own services. Glance at the reference documentation. ","description":"Linkerd's service profiles enable per-route metrics as well as retries and timeouts.","keywords":null,"section":"2.16","title":"Service Profiles","uri":"https://travisbeckham.github.io/2.16/features/service-profiles/"},{"ancestors":["Docs","Reference"],"content":"Service profiles provide Linkerd additional information about a service. This is a reference for everything that can be done with service profiles.\nSpec A service profile spec must contain the following top level fields:\nfield value routes a list of route objects retryBudget a retry budget object that defines the maximum retry rate to this service Route A route object must contain the following fields:\nfield value name the name of this route as it will appear in the route label condition a request match object that defines if a request matches this route responseClasses (optional) a list of response class objects isRetryable indicates that requests to this route are always safe to retry and will cause the proxy to retry failed requests on this route whenever possible timeout the maximum amount of time to wait for a response (including retries) to complete after the request is sent Request Match A request match object must contain exactly one of the following fields:\nfield value pathRegex a regular expression to match the request path against method one of GET, POST, PUT, DELETE, OPTION, HEAD, TRACE all a list of request match objects which must all match any a list of request match objects, at least one of which must match not a request match object which must not match Request Match Usage Examples The simplest condition is a path regular expression:\npathRegex: \u0026#39;/authors/\\d+\u0026#39; This is a condition that checks the request method:\nmethod: POST If more than one condition field is set, all of them must be satisfied. This is equivalent to using the ‘all’ condition:\nall: - pathRegex: \u0026#39;/authors/\\d+\u0026#39; - method: POST Conditions can be combined using ‘all’, ‘any’, and ’not’:\nany: - all: - method: POST - pathRegex: \u0026#39;/authors/\\d+\u0026#39; - all: - not: method: DELETE - pathRegex: /info.txt Response Class A response class object must contain the following fields:\nfield value condition a response match object that defines if a response matches this response class isFailure a boolean that defines if these responses should be classified as failed Response Match A response match object must contain exactly one of the following fields:\nfield value status a status range object to match the response status code against all a list of response match objects which must all match any a list of response match objects, at least one of which must match not a response match object which must not match Response Match conditions can be combined in a similar way as shown above for Request Match Usage Examples\nStatus Range A status range object must contain at least one of the following fields. Specifying only one of min or max matches just that one status code.\nfield value min the status code must be greater than or equal to this value max the status code must be less than or equal to this value Retry Budget A retry budget specifies the maximum total number of retries that should be sent to this service as a ratio of the original request volume.\nfield value retryRatio the maximum ratio …","description":"Details on the specification and what is possible with service profiles.","keywords":null,"section":"2.16","title":"Service Profiles","uri":"https://travisbeckham.github.io/2.16/reference/service-profiles/"},{"ancestors":["Docs","Tasks"],"content":"Service profiles provide Linkerd additional information about a service and how to handle requests for a service.\nWhen an HTTP (not HTTPS) request is received by a Linkerd proxy, the destination service of that request is identified. If a service profile for that destination service exists, then that service profile is used to to provide per-route metrics, retries and timeouts.\nThe destination service for a request is computed by selecting the value of the first header to exist of, l5d-dst-override, :authority, and Host. The port component, if included and including the colon, is stripped. That value is mapped to the fully qualified DNS name. When the destination service matches the name of a service profile in the namespace of the sender or the receiver, Linkerd will use that to provide per-route metrics, retries and timeouts.\nThere are times when you may need to define a service profile for a service which resides in a namespace that you do not control. To accomplish this, simply create a service profile as before, but edit the namespace of the service profile to the namespace of the pod which is calling the service. When Linkerd proxies a request to a service, a service profile in the source namespace will take priority over a service profile in the destination namespace.\nYour destination service may be a ExternalName service. In that case, use the spec.metadata.name and the `spec.metadata.namespace’ values to name your ServiceProfile. For example,\napiVersion: v1 kind: Service metadata: name: my-service namespace: prod spec: type: ExternalName externalName: my.database.example.com use the name my-service.prod.svc.cluster.local for the ServiceProfile.\nNote that at present, you cannot view statistics gathered for routes in this ServiceProfile in the web dashboard. You can get the statistics using the CLI.\nFor a complete demo walkthrough, check out the books demo.\nThere are a couple different ways to use linkerd profile to create service profiles.\nSwagger Protobuf Auto-Creation Template Requests which have been associated with a route will have a rt_route annotation. To manually verify if the requests are being associated correctly, run tap on your own deployment:\nlinkerd viz tap -o wide \u0026lt;target\u0026gt; | grep req The output will stream the requests that deploy/webapp is receiving in real time. A sample is:\nreq id=0:1 proxy=in src=10.1.3.76:57152 dst=10.1.3.74:7000 tls=disabled :method=POST :authority=webapp.default:7000 :path=/books/2878/edit src_res=deploy/traffic src_ns=foobar dst_res=deploy/webapp dst_ns=default rt_route=POST /books/{id}/edit Conversely, if rt_route is not present, a request has not been associated with any route. Try running:\nlinkerd viz tap -o wide \u0026lt;target\u0026gt; | grep req | grep -v rt_route Swagger If you have an OpenAPI (Swagger) spec for your service, you can use the --open-api flag to generate a service profile from the OpenAPI spec file.\nlinkerd profile --open-api webapp.swagger webapp This generates a service profile from the …","description":"Create a service profile that provides more details for Linkerd to build on.","keywords":null,"section":"2.16","title":"Setting Up Service Profiles","uri":"https://travisbeckham.github.io/2.16/tasks/setting-up-service-profiles/"},{"ancestors":["Docs","Reference"],"content":"Linkerd supports all versions of Kubernetes that were supported at the time that a given Linkerd version ships. For example, at the time that Linkerd 2.14 shipped, Kubernetes versions 1.26, 1.27, and 1.28 were supported, so Linkerd 2.14 supports all of those Kubernetes versions. (In many cases, as you’ll see below, Linkerd versions will also support older Kubernetes versions.)\nObviously, Linkerd 2.14 has no knowledge of what changes will come after Kubernetes 1.28. In some cases, later versions of Kubernetes end up making changes that cause older versions of Linkerd to not work: we will update the chart below as these situations arise.\nLinkerd Version Minimum Kubernetes Version Maximum Kubernetes Version 2.10 1.16 1.23 2.11 1.17 1.23 2.12 1.21 1.24 2.13 1.21 1.28 2.14 1.21 1.28 2.15 1.22 1.29 Note that Linkerd will almost never change the supported Kubernetes version in a minor release, which is why the table above only lists major versions. One known exception: Linkerd 2.11.0 supported Kubernetes 1.16, but 2.11.1 and later required Kubernetes 1.17 as shown in the table above.\nEdge Releases Linkerd Version Minimum Kubernetes Version Maximum Kubernetes Version edge-22.10.1 - edge-23.12.1 1.21 1.29 edge-23.12.2 and newer 1.22 1.29 ","description":"Reference documentation for which Linkerd version supports which Kubernetes version","keywords":null,"section":"2.16","title":"Supported Kubernetes Versions","uri":"https://travisbeckham.github.io/2.16/reference/k8s-versions/"},{"ancestors":["Docs","Reference"],"content":"Linkerd can be configured with timeouts to limit the maximum amount of time on a request before aborting.\nTimeouts are a client-side behavior, and are therefore performed by the outbound side of the Linkerd proxy.1 Note that timeouts configured in this way are not retryable – if these timeouts are reached, the request will not be retried. Retryable timeouts can be configured as part of retry configuration.\nConfiguring Timeouts Timeous are configured by a set of annotations which can be set on a Kubernetes Service resource or on a HTTPRoute or GRPCRoute which has a Service as a parent. Client proxies will then fail requests to that Service or route once they exceed the timeout. If any timeout configuration annotations are present on a route resource, they override all timeout configuration annotations on the parent Service.\nWarning Timeouts configured in this way are incompatible with ServiceProfiles. If a ServiceProfile is defined for a Service, proxies will use the ServiceProfile timeout configuration and ignore any timeout annotations. timeout.linkerd.io/request: The maximum amount of time a full request-response stream is in flight. timeout.linkerd.io/response: The maximum amount of time a backend response may be in-flight. timeout.linkerd.io/idle: The maximum amount of time a stream may be idle, regardless of its state. If the request timeout field is set on an HTTPRoute resource, it will be used as the timeout.linkerd.io/request timeout. However, if both the field and the annotation are specified, the annotation will take priority.\nExamples kind: HTTPRoute apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: schlep-default namespace: schlep annotations: timeout.linkerd.io/request: 2s timeout.linkerd.io/response: 1s spec: parentRefs: - name: schlep kind: Service group: core port: 80 rules: - matches: - path: type: PathPrefix value: \u0026#34;/\u0026#34; The part of the proxy which handles connections from within the pod to the rest of the cluster. ↩︎\n","description":"How Linkerd implements timeouts.","keywords":null,"section":"2.16","title":"Timeouts","uri":"https://travisbeckham.github.io/2.16/reference/timeouts/"},{"ancestors":["Docs","Tasks"],"content":"Traffic splitting and shifting are powerful features that enable operators to dynamically shift traffic to different backend Services. This can be used to implement A/B experiments, red/green deploys, canary rollouts, fault injection and more.\nLinkerd supports two different ways to configure traffic shifting: you can use the Linkerd SMI extension and TrafficSplit resources, or you can use HTTPRoute resources which Linkerd natively supports. While certain integrations such as Flagger rely on the SMI and TrafficSplit approach, using HTTPRoute is the preferred method going forward.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Prerequisites To use this guide, you’ll need a Kubernetes cluster running:\nLinkerd and Linkerd-Viz. If you haven’t installed these yet, follow the Installing Linkerd Guide. Set up the demo We will set up a minimal demo which involves a load generator and two backends called v1 and v2 respectively. You could imagine that these represent two different versions of a service and that we would like to test v2 on a small sample of traffic before rolling it out completely.\nFor load generation we’ll use Slow-Cooker and for the backends we’ll use BB.\nTo add these components to your cluster and include them in the Linkerd data plane, run:\ncat \u0026lt;\u0026lt;EOF | linkerd inject - | kubectl apply -f - --- apiVersion: v1 kind: Namespace metadata: name: traffic-shift-demo --- apiVersion: apps/v1 kind: Deployment metadata: name: v1 namespace: traffic-shift-demo spec: replicas: 1 selector: matchLabels: app: bb version: v1 template: metadata: labels: app: bb version: v1 spec: containers: - name: terminus image: buoyantio/bb:v0.0.6 args: - terminus - \u0026#34;--h1-server-port=8080\u0026#34; - \u0026#34;--response-text=v1\u0026#34; ports: - containerPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: v2 namespace: traffic-shift-demo spec: replicas: 1 selector: matchLabels: app: bb version: v2 template: metadata: labels: app: bb version: v2 spec: containers: - name: terminus image: buoyantio/bb:v0.0.6 args: - terminus - \u0026#34;--h1-server-port=8080\u0026#34; - \u0026#34;--response-text=v2\u0026#34; ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: bb namespace: traffic-shift-demo spec: ports: - name: http port: 8080 targetPort: 8080 selector: app: bb version: v1 --- apiVersion: v1 kind: Service metadata: name: bb-v2 namespace: traffic-shift-demo spec: ports: - name: http port: 8080 targetPort: 8080 selector: app: bb version: v2 --- apiVersion: apps/v1 kind: Deployment metadata: name: slow-cooker namespace: traffic-shift-demo spec: replicas: 1 selector: matchLabels: app: slow-cooker template: metadata: labels: app: slow-cooker spec: containers: - args: - -c - | sleep 5 # wait for pods to start /slow_cooker/slow_cooker --qps 10 http://bb:8080 command: - /bin/sh image: …","description":"Dynamically split and shift traffic between backends","keywords":null,"section":"2.16","title":"Traffic Shifting","uri":"https://travisbeckham.github.io/2.16/tasks/traffic-shifting/"},{"ancestors":["Docs","Features"],"content":"Linkerd’s traffic split functionality allows you to dynamically shift arbitrary portions of traffic destined for a Kubernetes service to a different destination service. This feature can be used to implement sophisticated rollout strategies such as canary deployments and blue/green deployments, for example, by slowly easing traffic off of an older version of a service and onto a newer version.\nNote This feature will eventually be supplanted by the newer dynamic request routing capabilities, which does not require the SMI extension. Note TrafficSplits cannot be used with headless services. Linkerd reads service discovery information based off the target IP address, and if that happens to be a pod IP address then it cannot tell which service the pod belongs to. Linkerd exposes this functionality via the Service Mesh Interface (SMI) TrafficSplit API. To use this feature, you create a Kubernetes resource as described in the TrafficSplit spec, and Linkerd takes care of the rest. You can see step by step documentation on our Getting started with Linkerd SMI extension page.\nBy combining traffic splitting with Linkerd’s metrics, it is possible to accomplish even more powerful deployment techniques that automatically take into account the success rate and latency of old and new versions. See the Flagger project for one example of this.\nCheck out some examples of what you can do with traffic splitting:\nCanary Releases Fault Injection ","description":"Linkerd can dynamically send a portion of traffic to different services.","keywords":null,"section":"2.16","title":"Traffic Split (canaries, blue/green deploys)","uri":"https://travisbeckham.github.io/2.16/features/traffic-split/"},{"ancestors":["Docs","Tasks"],"content":"This section provides resolution steps for common problems reported with the linkerd check command.\nThe “pre-kubernetes-cluster-setup” checks These checks only run when the --pre flag is set. This flag is intended for use prior to running linkerd install, to verify your cluster is prepared for installation.\n√ control plane namespace does not already exist Example failure:\n× control plane namespace does not already exist The \u0026#34;linkerd\u0026#34; namespace already exists By default linkerd install will create a linkerd namespace. Prior to installation, that namespace should not exist. To check with a different namespace, run:\nlinkerd check --pre --linkerd-namespace linkerd-test √ can create Kubernetes resources The subsequent checks in this section validate whether you have permission to create the Kubernetes resources required for Linkerd installation, specifically:\n√ can create Namespaces √ can create ClusterRoles √ can create ClusterRoleBindings √ can create CustomResourceDefinitions The “pre-kubernetes-setup” checks These checks only run when the --pre flag is set This flag is intended for use prior to running linkerd install, to verify you have the correct RBAC permissions to install Linkerd.\n√ can create Namespaces √ can create ClusterRoles √ can create ClusterRoleBindings √ can create CustomResourceDefinitions √ can create PodSecurityPolicies √ can create ServiceAccounts √ can create Services √ can create Deployments √ can create ConfigMaps √ no clock skew detected This check detects any differences between the system running the linkerd install command and the Kubernetes nodes (known as clock skew). Having a substantial clock skew can cause TLS validation problems because a node may determine that a TLS certificate is expired when it should not be, or vice versa.\nLinkerd version edge-20.3.4 and later check for a difference of at most 5 minutes and older versions of Linkerd (including stable-2.7) check for a difference of at most 1 minute. If your Kubernetes node heartbeat interval is longer than this difference, you may experience false positives of this check. The default node heartbeat interval was increased to 5 minutes in Kubernetes 1.17 meaning that users running Linkerd versions prior to edge-20.3.4 on Kubernetes 1.17 or later are likely to experience these false positives. If this is the case, you can upgrade to Linkerd edge-20.3.4 or later. If you choose to ignore this error, we strongly recommend that you verify that your system clocks are consistent.\nThe “pre-kubernetes-capability” checks These checks only run when the --pre flag is set. This flag is intended for use prior to running linkerd install, to verify you have the correct Kubernetes capability permissions to install Linkerd.\nThe “pre-linkerd-global-resources” checks These checks only run when the --pre flag is set. This flag is intended for use prior to running linkerd install, to verify you have not already installed the Linkerd control plane.\n√ no ClusterRoles exist √ no …","description":"Troubleshoot issues with your Linkerd installation.","keywords":null,"section":"2.16","title":"Troubleshooting","uri":"https://travisbeckham.github.io/2.16/tasks/troubleshooting/"},{"ancestors":["Docs","Reference","CLI"],"content":" Remove the Linkerd proxy from a Kubernetes config.\nYou can uninject resources contained in a single file, inside a folder and its sub-folders, or coming from stdin.\nExamples # Uninject all the deployments in the default namespace. kubectl get deploy -o yaml | linkerd uninject - | kubectl apply -f - # Download a resource and uninject it through stdin. curl http://url.to/yml | linkerd uninject - | kubectl apply -f - # Uninject all the resources inside a folder and its sub-folders. linkerd uninject \u0026lt;folder\u0026gt; | kubectl apply -f - Flags Flag Usage --output -o Output format, one of: json|yaml ","description":"","keywords":null,"section":"2.16","title":"uninject","uri":"https://travisbeckham.github.io/2.16/reference/cli/uninject/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output Kubernetes resources to uninstall Linkerd control plane.\nThis command provides all Kubernetes namespace-scoped and cluster-scoped resources (e.g services, deployments, RBACs, etc.) necessary to uninstall Linkerd control plane.\nExamples linkerd uninstall | kubectl delete -f - Flags Flag Usage --force -f Force uninstall even if there exist non-control-plane injected pods --output -o Output format. One of: json|yaml ","description":"","keywords":null,"section":"2.16","title":"uninstall","uri":"https://travisbeckham.github.io/2.16/reference/cli/uninstall/"},{"ancestors":["Docs","Tasks"],"content":"Removing Linkerd from a Kubernetes cluster requires a few steps: removing any data plane proxies, removing all the extensions and then removing the core control plane.\nRemoving Linkerd data plane proxies To remove the Linkerd data plane proxies, you should remove any Linkerd proxy injection annotations and roll the deployments. When Kubernetes recreates the pods, they will not have the Linkerd data plane attached.\nRemoving extensions To remove any extension, call its uninstall subcommand and pipe it to kubectl delete -f -. For the bundled extensions that means:\n# To remove Linkerd Viz linkerd viz uninstall | kubectl delete -f - # To remove Linkerd Jaeger linkerd jaeger uninstall | kubectl delete -f - # To remove Linkerd Multicluster linkerd multicluster uninstall | kubectl delete -f - Removing the control plane Note Uninstallating the control plane requires cluster-wide permissions. To remove the control plane, run:\nlinkerd uninstall | kubectl delete -f - The linkerd uninstall command outputs the manifest for all of the Kubernetes resources necessary for the control plane, including namespaces, service accounts, CRDs, and more; kubectl delete then deletes those resources.\nThis command can also be used to remove control planes that have been partially installed. Note that kubectl delete will complain about any resources that it was asked to delete that hadn’t been created, but these errors can be safely ignored.\n","description":"Linkerd can be easily removed from a Kubernetes cluster.","keywords":null,"section":"2.16","title":"Uninstalling Linkerd","uri":"https://travisbeckham.github.io/2.16/tasks/uninstall/"},{"ancestors":["Docs","Tasks"],"content":"The Linkerd multicluster components allow for sending traffic from one cluster to another. For more information on how to set this up, see installing multicluster.\nUnlinking Unlinking a cluster will delete all resources associated with that link including:\nthe service mirror controller the Link resource the credentials secret mirror services It is recommended that you use the unlink command rather than deleting any of these resources individually to help ensure that all mirror services get cleaned up correctly and are not left orphaned.\nTo unlink, run the linkerd multicluster unlink command and pipe the output to kubectl delete:\nlinkerd multicluster unlink --cluster-name=target | kubectl delete -f - Uninstalling Uninstalling the multicluster components will remove all components associated with Linkerd’s multicluster functionality including the gateway and service account. Before you can uninstall, you must remove all existing links as described above. Once all links have been removed, run:\nlinkerd multicluster uninstall | kubectl delete -f - Attempting to uninstall while at least one link remains will result in an error.\n","description":"Unlink and uninstall Linkerd multicluster.","keywords":null,"section":"2.16","title":"Uninstalling Multicluster","uri":"https://travisbeckham.github.io/2.16/tasks/uninstall-multicluster/"},{"ancestors":["Docs","Reference","CLI"],"content":" Output Kubernetes configs to upgrade an existing Linkerd control plane.\nNote that the default flag values for this command come from the Linkerd control plane. The default values displayed in the Flags section below only apply to the install command.\nThe upgrade can be configured by using the –set, –values, –set-string and –set-file flags. A full list of configurable values can be found at https://www.github.com/linkerd/linkerd2/tree/main/charts/linkerd2/README.md\nExamples # Upgrade CRDs first linkerd upgrade --crds | kubectl apply -f - # Then upgrade the control plane linkerd upgrade | kubectl apply -f - # And lastly, remove linkerd resources that no longer exist in the current version linkerd prune | kubectl delete -f - Flags Flag Usage --admin-port Proxy port to serve metrics on --control-plane-tracing Enables Control Plane Tracing with the defaults --control-plane-tracing-namespace Send control plane traces to Linkerd-Jaeger extension in this namespace --control-port Proxy port to use for control --controller-gid Run the control plane components under this group ID --controller-log-level Log level for the controller and web components --controller-replicas Replicas of the controller to deploy --controller-uid Run the control plane components under this user ID --crds Upgrade Linkerd CRDs --default-inbound-policy Inbound policy to use to control inbound access to the proxy --disable-h2-upgrade Prevents the controller from instructing proxies to perform transparent HTTP/2 upgrading (default false) --disable-heartbeat Disables the heartbeat cronjob (default false) --enable-endpoint-slices Enables the usage of EndpointSlice informers and resources for destination service --enable-external-profiles Enable service profiles for non-Kubernetes services --force Force upgrade operation even when issuer certificate does not work with the trust anchors of all proxies --from-manifests Read config from a Linkerd install YAML rather than from Kubernetes --ha Enable HA deployment config for the control plane (default false) --identity-clock-skew-allowance The amount of time to allow for clock skew within a Linkerd cluster --identity-issuance-lifetime The amount of time for which the Identity issuer should certify identity --identity-issuer-certificate-file A path to a PEM-encoded file containing the Linkerd Identity issuer certificate (generated by default) --identity-issuer-key-file A path to a PEM-encoded file containing the Linkerd Identity issuer private key (generated by default) --identity-trust-anchors-file A path to a PEM-encoded file containing Linkerd Identity trust anchors (generated by default) --image-pull-policy Docker image pull policy --inbound-port Proxy port to use for inbound traffic --init-image Linkerd init container image name --init-image-version Linkerd init container image version --linkerd-cni-enabled Omit the NET_ADMIN capability in the PSP and the proxy-init container when injecting the proxy; requires the linkerd-cni plugin to …","description":"","keywords":null,"section":"2.16","title":"upgrade","uri":"https://travisbeckham.github.io/2.16/reference/cli/upgrade/"},{"ancestors":["Docs","Tasks"],"content":"In this guide, we’ll walk you through how to perform zero-downtime upgrades for Linkerd.\nNote This page contains instructions for upgrading to the latest edge release of Linkerd. If you have installed a stable distribution of Linkerd, the vendor may have alternative guidance on how to upgrade. You can find more information about the different kinds of Linkerd releases on the Releases and Versions page. Read through this guide carefully. Additionally, before starting a specific upgrade, please read through the version-specific upgrade notices below, which may contain important information about your version.\nUpgrade notice: 2.15 and beyond Upgrade notice: stable-2.14.0 Upgrade notice: stable-2.13.0 Upgrade notice: stable-2.12.0 Upgrade notice: stable-2.11.0 Upgrade notice: stable-2.10.0 Version numbering Stable releases For stable releases, Linkerd follows a version numbering scheme of the form 2.\u0026lt;major\u0026gt;.\u0026lt;minor\u0026gt;. In other words, “2” is a static prefix, followed by the major version, then the minor.\nChanges in minor versions are intended to be backwards compatible with the previous version. Changes in major version may introduce breaking changes, although we try to avoid that whenever possible.\nEdge releases For edge releases, Linkerd issues explicit guidance about each release. Be sure to consult this guidance before installing any release artifact.\nNote Edge releases are not semantically versioned; the edge release number itself does not give you any assurance about breaking changes, incompatibilities, etc. Instead, this information is available in the release notes. Upgrade paths The following upgrade paths are generally safe. However, before starting a deploy, it is important to check the upgrade notes before proceeding—occasionally, specific minor releases may have additional restrictions.\nStable within the same major version. It is usually safe to upgrade to the latest minor version within the same major version. In other words, if you are currently running version 2.x.y, upgrading to 2.x.z, where z is the latest minor version for major version x, is safe. This is true even if you would skip intermediate intermediate minor versions, i.e. it is still safe even if z \u0026gt; y + 1.\nStable to the next major version. It is usually safe to upgrade to the latest minor version of the next major version. In other words, if you are currently running version 2.x.y, upgrading to 2.x + 1.w will be safe, where w is the latest minor version available for major version x + 1.\nStable to a later major version. Upgrades that skip one or more major versions are not supported. Instead, you should upgrade major versions incrementally.\nEdge release to a later edge release. This is generally safe unless the Cautions for the later edge release indicate otherwise.\nAgain, please check the upgrade notes or release guidance for the specific version you are upgrading to for any version-specific caveats.\nData plane vs control plane version skew Since a Linkerd upgrade always …","description":"Perform zero-downtime upgrades for Linkerd.","keywords":null,"section":"2.16","title":"Upgrading Linkerd","uri":"https://travisbeckham.github.io/2.16/tasks/upgrade/"},{"ancestors":["Docs","Tasks"],"content":"For Kubernetes clusters that use custom cluster domain, Linkerd must be installed using the --cluster-domain option:\n# first, install the Linkerd CRDs: linkerd install --crds | kubectl apply -f - # next, install the Linkerd control plane, using the custom cluster domain: linkerd install --cluster-domain=example.org \\ --identity-trust-domain=example.org \\ | kubectl apply -f - # The Linkerd Viz extension also requires a similar setting: linkerd viz install --set clusterDomain=example.org | kubectl apply -f - # And so does the Multicluster extension: linkerd multicluster install --set identityTrustDomain=example.org | kubectl apply -f - This ensures that all Linkerd handles all service discovery, routing, service profiles and traffic split resources using the example.org domain.\nNote Note that the identity trust domain must match the cluster domain for mTLS to work. Note Changing the cluster domain while upgrading Linkerd isn’t supported. ","description":"Use Linkerd with a custom cluster domain.","keywords":null,"section":"2.16","title":"Using a Custom Cluster Domain","uri":"https://travisbeckham.github.io/2.16/tasks/using-custom-domain/"},{"ancestors":["Docs","Tasks"],"content":"Linkerd extensions are components which can be added to a Linkerd installation to enable additional functionality. By default, the following extensions are available:\nviz: Metrics and visibility features jaeger: Distributed tracing multicluster: Cross-cluster routing But other extensions are also possible. Read on for more!\nInstalling extensions Before installing any extensions, make sure that you have already installed Linkerd and validated your cluster with linkerd check.\nThen, you can install the extension with the extension’s install command. For example, to install the viz extension, you can use:\nlinkerd viz install | kubectl apply -f - For built-in extensions, such as viz, jaeger, and multicluster, that’s all you need to do. Of course, these extensions can also be installed by with Helm by installing that extension’s Helm chart.\nOnce an extension has been installed, it will be included as part of the standard linkerd check command.\nWarning Installing extensions in the linkerd namespace is not supported. In general, Linkerd extensions will each create their own namespace – this is the recommended best practice. Installing third-party extensions Third-party extensions require one additional step: you must download the extension’s CLI and put it in your path. This will allow you to invoke the extension CLI through the Linkerd CLI. (E.g. any call to linkerd foo will automatically call the linkerd-foo binary, if it is found on your path.)\nListing extensions Every extension creates a Kubernetes namespace with the linkerd.io/extension label. Thus, you can list all extensions installed on your cluster by running:\nkubectl get ns -l linkerd.io/extension Upgrading extensions Unless otherwise stated, extensions do not persist any configuration in the cluster. To upgrade an extension, run the install again with a newer version of the extension CLI or with a different set of configuration flags.\nUninstalling extensions All extensions have an uninstall command that should be used to gracefully clean up all resources owned by an extension. For example, to uninstall the foo extension, run:\nlinkerd foo uninstall | kubectl delete -f - ","description":"Add functionality to Linkerd with optional extensions.","keywords":null,"section":"2.16","title":"Using extensions","uri":"https://travisbeckham.github.io/2.16/tasks/extensions/"},{"ancestors":["Docs","Tasks"],"content":"GitOps is an approach to automate the management and delivery of your Kubernetes infrastructure and applications using Git as a single source of truth. It usually utilizes some software agents to detect and reconcile any divergence between version-controlled artifacts in Git with what’s running in a cluster.\nThis guide will show you how to set up Argo CD to manage the installation and upgrade of Linkerd using a GitOps workflow.\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. Specifically, this guide provides instructions on how to securely generate and manage Linkerd’s mTLS private keys and certificates using Sealed Secrets and cert-manager. It will also show you how to integrate the auto proxy injection feature into your workflow. Finally, this guide conclude with steps to upgrade Linkerd to a newer version following a GitOps workflow.\nLinkerd GitOps workflow The software and tools used in this guide are selected for demonstration purposes only. Feel free to choose others that are most suited for your requirements.\nYou will need to clone this example repository to your local machine and replicate it in your Kubernetes cluster following the steps defined in the next section.\nThis guide uses the step cli to create certificates used by the Linkerd clusters to enforce mTLS, so make sure you have installed step for your environment.\nSet up the repositories Clone the example repository to your local machine:\ngit clone https://github.com/linkerd/linkerd-examples.git This repository will be used to demonstrate Git operations like add, commit and push later in this guide.\nAdd a new remote endpoint to the repository to point to the in-cluster Git server, which will be set up in the next section:\ncd linkerd-examples git remote add git-server git://localhost/linkerd-examples.git Note To simplify the steps in this guide, we will be interacting with the in-cluster Git server via port-forwarding. Hence, the remote endpoint that we just created targets your localhost. Deploy the Git server to the scm namespace in your cluster:\nkubectl apply -f gitops/resources/git-server.yaml Later in this guide, Argo CD will be configured to watch the repositories hosted by this Git server.\nNote This Git server is configured to run as a daemon over the git protocol, with unauthenticated access to the Git data. This setup is not recommended for production use. Confirm that the Git server is healthy:\nkubectl -n scm rollout status deploy/git-server Clone the example repository to your in-cluster Git server:\ngit_server=`kubectl -n scm get po -l app=git-server -oname | awk -F/ \u0026#39;{ print $2 }\u0026#39;` kubectl -n scm exec \u0026#34;${git_server}\u0026#34; -- \\ git clone --bare https://github.com/linkerd/linkerd-examples.git Confirm that the remote repository is successfully cloned:\nkubectl -n scm …","description":"Use Argo CD to manage Linkerd installation and upgrade lifecycle.","keywords":null,"section":"2.16","title":"Using GitOps with Linkerd with Argo CD","uri":"https://travisbeckham.github.io/2.16/tasks/gitops/"},{"ancestors":["Docs","Tasks"],"content":"Debugging a service mesh can be hard. When something just isn’t working, is the problem with the proxy? With the application? With the client? With the underlying network? Sometimes, nothing beats looking at raw network data.\nIn cases where you need network-level visibility into packets entering and leaving your application, Linkerd provides a debug sidecar with some helpful tooling. Similar to how proxy sidecar injection works, you add a debug sidecar to a pod by setting the config.linkerd.io/enable-debug-sidecar: \u0026#34;true\u0026#34; annotation at pod creation time. For convenience, the linkerd inject command provides an --enable-debug-sidecar option that does this annotation for you.\n(Note that the set of containers in a Kubernetes pod is not mutable, so simply adding this annotation to a pre-existing pod will not work. It must be present at pod creation time.)\nLinkerd Production Tip This page contains best-effort instructions by the open source community. Production users with mission-critical applications should familiarize themselves with Linkerd production resources and/or connect with a commercial Linkerd provider. The debug sidecar image contains tshark, tcpdump, lsof, and iproute2. Once installed, it starts automatically logging all incoming and outgoing traffic with tshark, which can then be viewed with kubectl logs. Alternatively, you can use kubectl exec to access the container and run commands directly.\nFor instance, if you’ve gone through the Linkerd Getting Started guide and installed the emojivoto application, and wish to debug traffic to the voting service, you could run:\nkubectl -n emojivoto get deploy/voting -o yaml \\ | linkerd inject --enable-debug-sidecar - \\ | kubectl apply -f - to deploy the debug sidecar container to all pods in the voting service. (Note that there’s only one pod in this deployment, which will be recreated to do this–see the note about pod mutability above.)\nYou can confirm that the debug container is running by listing all the containers in pods with the voting-svc label:\nkubectl get pods -n emojivoto -l app=voting-svc \\ -o jsonpath=\u0026#39;{.items[*].spec.containers[*].name}\u0026#39; Then, you can watch live tshark output from the logs by simply running:\nkubectl -n emojivoto logs deploy/voting linkerd-debug -f If that’s not enough, you can exec to the container and run your own commands in the context of the network. For example, if you want to inspect the HTTP headers of the requests, you could run something like this:\nkubectl -n emojivoto exec -it \\ $(kubectl -n emojivoto get pod -l app=voting-svc \\ -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) \\ -c linkerd-debug -- tshark -i any -f \u0026#34;tcp\u0026#34; -V -Y \u0026#34;http.request\u0026#34; A real-world error message written by the proxy that the debug sidecar is effective in troubleshooting is a Connection Refused error like this one:\nERR! [\u0026lt;time\u0026gt;] proxy={server=in listen=0.0.0.0:4143 remote=some.svc:50416} linkerd2_proxy::app::errors unexpected error: error trying to connect: Connection refused (os error 111) …","description":"Inject the debug container to capture network packets.","keywords":null,"section":"2.16","title":"Using the Debug Sidecar","uri":"https://travisbeckham.github.io/2.16/tasks/using-the-debug-container/"},{"ancestors":["Docs","Tasks"],"content":"By default, Linkerd automatically enables mutual Transport Layer Security (mTLS) for TCP traffic between meshed pods, by establishing and authenticating secure, private TLS connections between Linkerd proxies. Simply add your services to Linkerd, and Linkerd will take care of the rest.\nLinkerd’s automatic mTLS is done in a way that’s completely transparent to the application. Of course, sometimes it’s helpful to be able to validate whether mTLS is in effect!\nValidating mTLS with linkerd viz edges To validate that mTLS is working, you can view a summary of the TCP connections between services that are managed by Linkerd using the linkerd viz edges command. For example:\nlinkerd viz -n linkerd edges deployment The output will look like:\nSRC DST SRC_NS DST_NS SECURED prometheus linkerd-controller linkerd-viz linkerd √ prometheus linkerd-destination linkerd-viz linkerd √ prometheus linkerd-identity linkerd-viz linkerd √ prometheus linkerd-proxy-injector linkerd-viz linkerd √ prometheus linkerd-sp-validator linkerd-viz linkerd √ In this example, everything is successfully mTLS’d, and the CLIENT and SERVER columns denote the identities used, in the form service-account-name.namespace. (See Linkerd’s automatic mTLS documentation for more on what these identities mean.) If there were a problem automatically upgrading the connection with mTLS, the MSG field would contain the reason why.\nValidating mTLS with linkerd viz tap Instead of relying on an aggregate, it is also possible to watch the requests and responses in real time to understand what is getting mTLS’d. We can use the linkerd viz tap command to sample real time request data.\nlinkerd viz -n linkerd tap deploy Note By default, the control plane resources are not tappable. After having installed the Viz extension (through linkerd viz install), you can enable tap on the control plane components simply by restarting them, which can be done with no downtime with kubectl -n linkerd rollout restart deploy. To enable tap on the Viz extension itself, issue kubectl -n linkerd-viz rollout restart deploy. Looking at the control plane specifically, there will be two main types of output.\nreq id=0:0 proxy=in src=10.42.0.1:60318 dst=10.42.0.23:9995 tls=no_tls_from_remote :method=GET :authority=10.42.0.23:9995 :path=/ready rsp id=0:0 proxy=in src=10.42.0.1:60318 dst=10.42.0.23:9995 tls=no_tls_from_remote :status=200 latency=267µs end id=0:0 proxy=in src=10.42.0.1:60318 dst=10.42.0.23:9995 tls=no_tls_from_remote duration=20µs response-length=3B These are calls by the Kubernetes readiness probe. As probes are initiated from the kubelet, which is not in the mesh, there is no identity and these requests are not mTLS’d, as denoted by the tls=no_tls_from_remote message.\nOther requests to the control plane are TLS’d:\nireq id=2:1 proxy=in src=10.42.0.31:55428 dst=10.42.0.22:9995 tls=true :method=GET :authority=10.42.0.22:9995 :path=/metrics rsp id=2:1 proxy=in src=10.42.0.31:55428 dst=10.42.0.22:9995 tls=true :status=200 …","description":"You can validate whether or not your traffic is being mTLS'd by Linkerd.","keywords":null,"section":"2.16","title":"Validating your mTLS traffic","uri":"https://travisbeckham.github.io/2.16/tasks/validating-your-traffic/"},{"ancestors":["Docs","Reference","CLI"],"content":" Print the client and server version information\nFlags Flag Usage --client Print the client version only --namespace -n Namespace to use for –proxy versions (default: all namespaces) --proxy Print data-plane versions --short Print the version number(s) only, with no additional output ","description":"","keywords":null,"section":"2.16","title":"version","uri":"https://travisbeckham.github.io/2.16/reference/cli/version/"},{"ancestors":["Docs","Reference","CLI"],"content":" viz manages the linkerd-viz extension of Linkerd service mesh.\nFlags Flag Usage --api-addr Override kubeconfig and communicate directly with the control plane at host:port (mostly for testing) --as Username to impersonate for Kubernetes operations --as-group Group to impersonate for Kubernetes operations --context Name of the kubeconfig context to use --kubeconfig Path to the kubeconfig file to use for CLI requests --linkerd-namespace -L Namespace in which Linkerd is installed --verbose Turn on debug logging --viz-namespace Name of the linkerd-viz namespace. If not set, it’s automatically detected Subcommands allow-scrapes Output Kubernetes resources to authorize Prometheus scrapes in a namespace or cluster with config.linkerd.io/default-inbound-policy: deny.\nExamples # Allow scrapes in the \u0026#39;emojivoto\u0026#39; namespace linkerd viz allow-scrapes --namespace emojivoto | kubectl apply -f - Flags Flag Usage --namespace -n The namespace in which to authorize Prometheus scrapes. --output -o Output format. One of: json|yaml authz Display stats for authorizations for a resource.\nFlags Flag Usage --namespace -n Namespace of the specified resource --output -o Output format; one of: “table” or “json” or “wide” --selector -l Selector (label query) to filter on, supports ‘=’, ‘==’, and ‘!=’ --time-window -t Stat window (for example: “15s”, “1m”, “10m”, “1h”). Needs to be at least 15s. check Check the Linkerd Viz extension for potential problems.\nThe check command will perform a series of checks to validate that the Linkerd Viz extension is configured correctly. If the command encounters a failure it will print additional information about the failure and exit with a non-zero exit code.\nExamples # Check that the viz extension is up and running linkerd viz check Flags Flag Usage --namespace -n Namespace to use for –proxy checks (default: all namespaces) --output -o Output format. One of: table, json, short --proxy Also run data-plane checks, to determine if the data plane is healthy --wait Maximum allowed time for all tests to pass dashboard Open the Linkerd dashboard in a web browser\nCheck out the architecture docs for a more thorough explanation of what this command does.\nFlags Flag Usage --address The address at which to serve requests --port -p The local port on which to serve requests (when set to 0, a random port will be used) --show Open a dashboard in a browser or show URLs in the CLI (one of: linkerd, grafana, url) --wait Wait for dashboard to become available if it’s not available when the command is run (*) You’ll need to tweak the dashboard’s enforced-host parameter with this value, as explained in the DNS-rebinding protection docs\nedges Display connections between resources, and Linkerd proxy identities.\nThe RESOURCETYPE argument specifies the type of resource to display edges within.\nExamples:\ncronjob deploy ds job po rc rs sts Valid resource types include:\ncronjobs daemonsets deployments jobs pods replicasets replicationcontrollers statefulsets …","description":"","keywords":null,"section":"2.16","title":"viz","uri":"https://travisbeckham.github.io/2.16/reference/cli/viz/"}]